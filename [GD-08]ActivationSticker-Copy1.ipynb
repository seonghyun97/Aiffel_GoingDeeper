{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901da4b5",
   "metadata": {},
   "source": [
    "# [GD-08] 행동 스티커 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c782b17",
   "metadata": {},
   "source": [
    "## 00. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d94006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ea22d",
   "metadata": {},
   "source": [
    "## 01. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b555f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4555bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324350f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566b7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34358173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546d8e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83ed55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de53f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fbea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 05:05:01,271\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.84gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1321)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1320)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2349b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ef848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=1322)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c467c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52a04609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583df5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4d6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a819b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1fc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaabf9c",
   "metadata": {},
   "source": [
    "## 02. 학습 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae17dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc74baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d413b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41871c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.77165461 epoch total loss 2.77165461\n",
      "Trained batch 2 batch loss 2.50471544 epoch total loss 2.63818502\n",
      "Trained batch 3 batch loss 2.37863302 epoch total loss 2.55166769\n",
      "Trained batch 4 batch loss 2.34412074 epoch total loss 2.49978089\n",
      "Trained batch 5 batch loss 2.27875948 epoch total loss 2.45557666\n",
      "Trained batch 6 batch loss 2.31265831 epoch total loss 2.43175697\n",
      "Trained batch 7 batch loss 2.22616768 epoch total loss 2.40238714\n",
      "Trained batch 8 batch loss 2.08677864 epoch total loss 2.36293602\n",
      "Trained batch 9 batch loss 2.23337674 epoch total loss 2.34854054\n",
      "Trained batch 10 batch loss 1.99471211 epoch total loss 2.3131578\n",
      "Trained batch 11 batch loss 2.09828472 epoch total loss 2.29362392\n",
      "Trained batch 12 batch loss 2.07045102 epoch total loss 2.27502608\n",
      "Trained batch 13 batch loss 2.0819149 epoch total loss 2.26017141\n",
      "Trained batch 14 batch loss 2.02057076 epoch total loss 2.24305701\n",
      "Trained batch 15 batch loss 2.09430814 epoch total loss 2.23314047\n",
      "Trained batch 16 batch loss 2.002 epoch total loss 2.21869421\n",
      "Trained batch 17 batch loss 2.03185606 epoch total loss 2.20770383\n",
      "Trained batch 18 batch loss 2.01007581 epoch total loss 2.19672441\n",
      "Trained batch 19 batch loss 1.96568775 epoch total loss 2.18456459\n",
      "Trained batch 20 batch loss 1.86440563 epoch total loss 2.16855669\n",
      "Trained batch 21 batch loss 1.80873799 epoch total loss 2.1514225\n",
      "Trained batch 22 batch loss 1.7413342 epoch total loss 2.13278198\n",
      "Trained batch 23 batch loss 1.62336242 epoch total loss 2.11063337\n",
      "Trained batch 24 batch loss 1.75252557 epoch total loss 2.09571218\n",
      "Trained batch 25 batch loss 1.92436683 epoch total loss 2.08885837\n",
      "Trained batch 26 batch loss 1.84477723 epoch total loss 2.07947063\n",
      "Trained batch 27 batch loss 1.90649271 epoch total loss 2.07306409\n",
      "Trained batch 28 batch loss 1.91534448 epoch total loss 2.06743121\n",
      "Trained batch 29 batch loss 1.93904674 epoch total loss 2.06300402\n",
      "Trained batch 30 batch loss 1.89584339 epoch total loss 2.05743194\n",
      "Trained batch 31 batch loss 1.87835181 epoch total loss 2.05165529\n",
      "Trained batch 32 batch loss 1.91008139 epoch total loss 2.0472312\n",
      "Trained batch 33 batch loss 1.89963233 epoch total loss 2.0427587\n",
      "Trained batch 34 batch loss 1.81536305 epoch total loss 2.03607035\n",
      "Trained batch 35 batch loss 1.90936244 epoch total loss 2.0324502\n",
      "Trained batch 36 batch loss 1.67644548 epoch total loss 2.02256107\n",
      "Trained batch 37 batch loss 1.64433885 epoch total loss 2.01233912\n",
      "Trained batch 38 batch loss 1.68911767 epoch total loss 2.00383306\n",
      "Trained batch 39 batch loss 1.79862583 epoch total loss 1.9985714\n",
      "Trained batch 40 batch loss 1.78124607 epoch total loss 1.99313807\n",
      "Trained batch 41 batch loss 1.70430291 epoch total loss 1.98609328\n",
      "Trained batch 42 batch loss 1.74223876 epoch total loss 1.98028731\n",
      "Trained batch 43 batch loss 1.75562048 epoch total loss 1.97506249\n",
      "Trained batch 44 batch loss 1.72572911 epoch total loss 1.96939588\n",
      "Trained batch 45 batch loss 1.60116184 epoch total loss 1.96121287\n",
      "Trained batch 46 batch loss 1.75857 epoch total loss 1.95680749\n",
      "Trained batch 47 batch loss 1.67825651 epoch total loss 1.95088077\n",
      "Trained batch 48 batch loss 1.72604799 epoch total loss 1.94619691\n",
      "Trained batch 49 batch loss 1.84768271 epoch total loss 1.94418633\n",
      "Trained batch 50 batch loss 1.74720013 epoch total loss 1.94024658\n",
      "Trained batch 51 batch loss 1.67991376 epoch total loss 1.93514204\n",
      "Trained batch 52 batch loss 1.69901288 epoch total loss 1.93060112\n",
      "Trained batch 53 batch loss 1.68129337 epoch total loss 1.92589712\n",
      "Trained batch 54 batch loss 1.73092782 epoch total loss 1.92228651\n",
      "Trained batch 55 batch loss 1.80867505 epoch total loss 1.92022097\n",
      "Trained batch 56 batch loss 1.74821126 epoch total loss 1.91714942\n",
      "Trained batch 57 batch loss 1.76162815 epoch total loss 1.91442096\n",
      "Trained batch 58 batch loss 1.8227489 epoch total loss 1.91284037\n",
      "Trained batch 59 batch loss 1.79260921 epoch total loss 1.9108026\n",
      "Trained batch 60 batch loss 1.83193684 epoch total loss 1.9094882\n",
      "Trained batch 61 batch loss 1.75000596 epoch total loss 1.9068737\n",
      "Trained batch 62 batch loss 1.72221267 epoch total loss 1.90389538\n",
      "Trained batch 63 batch loss 1.74558342 epoch total loss 1.90138245\n",
      "Trained batch 64 batch loss 1.72947598 epoch total loss 1.89869642\n",
      "Trained batch 65 batch loss 1.71792734 epoch total loss 1.89591539\n",
      "Trained batch 66 batch loss 1.81514513 epoch total loss 1.89469159\n",
      "Trained batch 67 batch loss 1.79654968 epoch total loss 1.89322674\n",
      "Trained batch 68 batch loss 1.81110072 epoch total loss 1.89201891\n",
      "Trained batch 69 batch loss 1.70257056 epoch total loss 1.88927341\n",
      "Trained batch 70 batch loss 1.74751878 epoch total loss 1.88724828\n",
      "Trained batch 71 batch loss 1.60762608 epoch total loss 1.88330984\n",
      "Trained batch 72 batch loss 1.76602459 epoch total loss 1.88168085\n",
      "Trained batch 73 batch loss 1.69693804 epoch total loss 1.87915015\n",
      "Trained batch 74 batch loss 1.69725144 epoch total loss 1.87669206\n",
      "Trained batch 75 batch loss 1.73667097 epoch total loss 1.874825\n",
      "Trained batch 76 batch loss 1.6145699 epoch total loss 1.87140048\n",
      "Trained batch 77 batch loss 1.69900692 epoch total loss 1.86916161\n",
      "Trained batch 78 batch loss 1.79576707 epoch total loss 1.86822057\n",
      "Trained batch 79 batch loss 1.75235367 epoch total loss 1.86675394\n",
      "Trained batch 80 batch loss 1.79603124 epoch total loss 1.86586988\n",
      "Trained batch 81 batch loss 1.76285899 epoch total loss 1.86459827\n",
      "Trained batch 82 batch loss 1.76117933 epoch total loss 1.86333704\n",
      "Trained batch 83 batch loss 1.71681857 epoch total loss 1.86157167\n",
      "Trained batch 84 batch loss 1.73309469 epoch total loss 1.86004221\n",
      "Trained batch 85 batch loss 1.72712994 epoch total loss 1.85847855\n",
      "Trained batch 86 batch loss 1.78369975 epoch total loss 1.85760903\n",
      "Trained batch 87 batch loss 1.61557937 epoch total loss 1.85482717\n",
      "Trained batch 88 batch loss 1.68587387 epoch total loss 1.85290718\n",
      "Trained batch 89 batch loss 1.86399889 epoch total loss 1.85303175\n",
      "Trained batch 90 batch loss 1.84289265 epoch total loss 1.85291922\n",
      "Trained batch 91 batch loss 1.80348194 epoch total loss 1.85237586\n",
      "Trained batch 92 batch loss 1.77083921 epoch total loss 1.85148966\n",
      "Trained batch 93 batch loss 1.80436993 epoch total loss 1.85098302\n",
      "Trained batch 94 batch loss 1.71804249 epoch total loss 1.84956884\n",
      "Trained batch 95 batch loss 1.62298512 epoch total loss 1.8471837\n",
      "Trained batch 96 batch loss 1.67119265 epoch total loss 1.84535038\n",
      "Trained batch 97 batch loss 1.67851555 epoch total loss 1.84363043\n",
      "Trained batch 98 batch loss 1.69853318 epoch total loss 1.84214985\n",
      "Trained batch 99 batch loss 1.68719876 epoch total loss 1.84058464\n",
      "Trained batch 100 batch loss 1.75330698 epoch total loss 1.8397119\n",
      "Trained batch 101 batch loss 1.67940545 epoch total loss 1.83812475\n",
      "Trained batch 102 batch loss 1.64315736 epoch total loss 1.83621335\n",
      "Trained batch 103 batch loss 1.70594263 epoch total loss 1.83494866\n",
      "Trained batch 104 batch loss 1.7212702 epoch total loss 1.83385551\n",
      "Trained batch 105 batch loss 1.78610504 epoch total loss 1.83340073\n",
      "Trained batch 106 batch loss 1.7803998 epoch total loss 1.83290076\n",
      "Trained batch 107 batch loss 1.76738119 epoch total loss 1.83228838\n",
      "Trained batch 108 batch loss 1.77918768 epoch total loss 1.83179677\n",
      "Trained batch 109 batch loss 1.76778042 epoch total loss 1.83120942\n",
      "Trained batch 110 batch loss 1.72380364 epoch total loss 1.83023298\n",
      "Trained batch 111 batch loss 1.57685924 epoch total loss 1.82795024\n",
      "Trained batch 112 batch loss 1.61792672 epoch total loss 1.82607496\n",
      "Trained batch 113 batch loss 1.5325489 epoch total loss 1.82347739\n",
      "Trained batch 114 batch loss 1.58245564 epoch total loss 1.82136321\n",
      "Trained batch 115 batch loss 1.65889394 epoch total loss 1.81995046\n",
      "Trained batch 116 batch loss 1.45161438 epoch total loss 1.81677508\n",
      "Trained batch 117 batch loss 1.45322824 epoch total loss 1.81366789\n",
      "Trained batch 118 batch loss 1.45443571 epoch total loss 1.81062353\n",
      "Trained batch 119 batch loss 1.59145105 epoch total loss 1.80878174\n",
      "Trained batch 120 batch loss 1.52851713 epoch total loss 1.80644619\n",
      "Trained batch 121 batch loss 1.47391784 epoch total loss 1.80369806\n",
      "Trained batch 122 batch loss 1.63337302 epoch total loss 1.802302\n",
      "Trained batch 123 batch loss 1.62422466 epoch total loss 1.80085421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 124 batch loss 1.589329 epoch total loss 1.79914832\n",
      "Trained batch 125 batch loss 1.54288292 epoch total loss 1.79709816\n",
      "Trained batch 126 batch loss 1.59661269 epoch total loss 1.79550707\n",
      "Trained batch 127 batch loss 1.64122093 epoch total loss 1.79429221\n",
      "Trained batch 128 batch loss 1.66720259 epoch total loss 1.79329932\n",
      "Trained batch 129 batch loss 1.75205469 epoch total loss 1.7929796\n",
      "Trained batch 130 batch loss 1.77278292 epoch total loss 1.79282427\n",
      "Trained batch 131 batch loss 1.77904367 epoch total loss 1.79271901\n",
      "Trained batch 132 batch loss 1.71575201 epoch total loss 1.79213595\n",
      "Trained batch 133 batch loss 1.77138078 epoch total loss 1.79197991\n",
      "Trained batch 134 batch loss 1.75688934 epoch total loss 1.79171801\n",
      "Trained batch 135 batch loss 1.68074799 epoch total loss 1.79089606\n",
      "Trained batch 136 batch loss 1.68312776 epoch total loss 1.79010355\n",
      "Trained batch 137 batch loss 1.64312828 epoch total loss 1.78903079\n",
      "Trained batch 138 batch loss 1.6526711 epoch total loss 1.78804255\n",
      "Trained batch 139 batch loss 1.64913499 epoch total loss 1.78704333\n",
      "Trained batch 140 batch loss 1.6182363 epoch total loss 1.78583753\n",
      "Trained batch 141 batch loss 1.72458196 epoch total loss 1.78540313\n",
      "Trained batch 142 batch loss 1.61937582 epoch total loss 1.78423381\n",
      "Trained batch 143 batch loss 1.55942035 epoch total loss 1.78266168\n",
      "Trained batch 144 batch loss 1.59688044 epoch total loss 1.78137159\n",
      "Trained batch 145 batch loss 1.64444888 epoch total loss 1.78042734\n",
      "Trained batch 146 batch loss 1.61794198 epoch total loss 1.7793144\n",
      "Trained batch 147 batch loss 1.60513139 epoch total loss 1.77812958\n",
      "Trained batch 148 batch loss 1.5712496 epoch total loss 1.77673173\n",
      "Trained batch 149 batch loss 1.59689879 epoch total loss 1.77552474\n",
      "Trained batch 150 batch loss 1.48715913 epoch total loss 1.77360225\n",
      "Trained batch 151 batch loss 1.56063724 epoch total loss 1.77219188\n",
      "Trained batch 152 batch loss 1.52367699 epoch total loss 1.77055705\n",
      "Trained batch 153 batch loss 1.65330219 epoch total loss 1.76979053\n",
      "Trained batch 154 batch loss 1.61669719 epoch total loss 1.76879644\n",
      "Trained batch 155 batch loss 1.68471551 epoch total loss 1.76825404\n",
      "Trained batch 156 batch loss 1.7349937 epoch total loss 1.76804078\n",
      "Trained batch 157 batch loss 1.7160697 epoch total loss 1.76770973\n",
      "Trained batch 158 batch loss 1.70960474 epoch total loss 1.76734185\n",
      "Trained batch 159 batch loss 1.68858624 epoch total loss 1.76684666\n",
      "Trained batch 160 batch loss 1.62264216 epoch total loss 1.76594543\n",
      "Trained batch 161 batch loss 1.64706981 epoch total loss 1.76520705\n",
      "Trained batch 162 batch loss 1.72569311 epoch total loss 1.76496327\n",
      "Trained batch 163 batch loss 1.69753814 epoch total loss 1.76454961\n",
      "Trained batch 164 batch loss 1.6631397 epoch total loss 1.76393127\n",
      "Trained batch 165 batch loss 1.63230944 epoch total loss 1.76313365\n",
      "Trained batch 166 batch loss 1.60576546 epoch total loss 1.76218569\n",
      "Trained batch 167 batch loss 1.49329805 epoch total loss 1.76057553\n",
      "Trained batch 168 batch loss 1.52033114 epoch total loss 1.7591455\n",
      "Trained batch 169 batch loss 1.66134 epoch total loss 1.75856674\n",
      "Trained batch 170 batch loss 1.7142005 epoch total loss 1.75830579\n",
      "Trained batch 171 batch loss 1.71373844 epoch total loss 1.7580452\n",
      "Trained batch 172 batch loss 1.55455899 epoch total loss 1.75686216\n",
      "Trained batch 173 batch loss 1.5178802 epoch total loss 1.75548077\n",
      "Trained batch 174 batch loss 1.56073856 epoch total loss 1.75436151\n",
      "Trained batch 175 batch loss 1.58601463 epoch total loss 1.75339961\n",
      "Trained batch 176 batch loss 1.6555 epoch total loss 1.75284338\n",
      "Trained batch 177 batch loss 1.63581908 epoch total loss 1.75218225\n",
      "Trained batch 178 batch loss 1.73996615 epoch total loss 1.75211358\n",
      "Trained batch 179 batch loss 1.69274902 epoch total loss 1.75178194\n",
      "Trained batch 180 batch loss 1.73417354 epoch total loss 1.75168407\n",
      "Trained batch 181 batch loss 1.58014631 epoch total loss 1.75073636\n",
      "Trained batch 182 batch loss 1.60105991 epoch total loss 1.74991393\n",
      "Trained batch 183 batch loss 1.56627536 epoch total loss 1.74891055\n",
      "Trained batch 184 batch loss 1.6647861 epoch total loss 1.74845338\n",
      "Trained batch 185 batch loss 1.68486667 epoch total loss 1.7481097\n",
      "Trained batch 186 batch loss 1.71656334 epoch total loss 1.74794006\n",
      "Trained batch 187 batch loss 1.78972816 epoch total loss 1.74816358\n",
      "Trained batch 188 batch loss 1.7151798 epoch total loss 1.7479881\n",
      "Trained batch 189 batch loss 1.80900204 epoch total loss 1.74831092\n",
      "Trained batch 190 batch loss 1.72212315 epoch total loss 1.74817312\n",
      "Trained batch 191 batch loss 1.76437056 epoch total loss 1.74825799\n",
      "Trained batch 192 batch loss 1.78003442 epoch total loss 1.74842346\n",
      "Trained batch 193 batch loss 1.75425386 epoch total loss 1.74845362\n",
      "Trained batch 194 batch loss 1.6769172 epoch total loss 1.74808478\n",
      "Trained batch 195 batch loss 1.58891487 epoch total loss 1.74726856\n",
      "Trained batch 196 batch loss 1.63670075 epoch total loss 1.74670446\n",
      "Trained batch 197 batch loss 1.63792682 epoch total loss 1.74615228\n",
      "Trained batch 198 batch loss 1.6441524 epoch total loss 1.74563718\n",
      "Trained batch 199 batch loss 1.63326883 epoch total loss 1.7450726\n",
      "Trained batch 200 batch loss 1.59146369 epoch total loss 1.74430454\n",
      "Trained batch 201 batch loss 1.70092297 epoch total loss 1.74408865\n",
      "Trained batch 202 batch loss 1.67740059 epoch total loss 1.74375856\n",
      "Trained batch 203 batch loss 1.49484038 epoch total loss 1.74253237\n",
      "Trained batch 204 batch loss 1.49392653 epoch total loss 1.7413137\n",
      "Trained batch 205 batch loss 1.55125856 epoch total loss 1.74038672\n",
      "Trained batch 206 batch loss 1.62018454 epoch total loss 1.73980308\n",
      "Trained batch 207 batch loss 1.67228508 epoch total loss 1.73947692\n",
      "Trained batch 208 batch loss 1.69711065 epoch total loss 1.73927319\n",
      "Trained batch 209 batch loss 1.63569665 epoch total loss 1.73877776\n",
      "Trained batch 210 batch loss 1.64824259 epoch total loss 1.7383467\n",
      "Trained batch 211 batch loss 1.68558395 epoch total loss 1.73809659\n",
      "Trained batch 212 batch loss 1.60754597 epoch total loss 1.73748076\n",
      "Trained batch 213 batch loss 1.60232782 epoch total loss 1.73684621\n",
      "Trained batch 214 batch loss 1.57567966 epoch total loss 1.73609316\n",
      "Trained batch 215 batch loss 1.51099241 epoch total loss 1.73504615\n",
      "Trained batch 216 batch loss 1.64336872 epoch total loss 1.73462164\n",
      "Trained batch 217 batch loss 1.72799551 epoch total loss 1.73459113\n",
      "Trained batch 218 batch loss 1.64969349 epoch total loss 1.73420167\n",
      "Trained batch 219 batch loss 1.56246924 epoch total loss 1.73341751\n",
      "Trained batch 220 batch loss 1.59547877 epoch total loss 1.73279059\n",
      "Trained batch 221 batch loss 1.64447176 epoch total loss 1.732391\n",
      "Trained batch 222 batch loss 1.59617519 epoch total loss 1.73177731\n",
      "Trained batch 223 batch loss 1.62556195 epoch total loss 1.73130095\n",
      "Trained batch 224 batch loss 1.54617846 epoch total loss 1.73047447\n",
      "Trained batch 225 batch loss 1.71484041 epoch total loss 1.73040497\n",
      "Trained batch 226 batch loss 1.63762367 epoch total loss 1.72999454\n",
      "Trained batch 227 batch loss 1.69650221 epoch total loss 1.72984695\n",
      "Trained batch 228 batch loss 1.70169222 epoch total loss 1.72972345\n",
      "Trained batch 229 batch loss 1.60271454 epoch total loss 1.72916889\n",
      "Trained batch 230 batch loss 1.57526541 epoch total loss 1.72849965\n",
      "Trained batch 231 batch loss 1.55869174 epoch total loss 1.72776461\n",
      "Trained batch 232 batch loss 1.57683337 epoch total loss 1.72711408\n",
      "Trained batch 233 batch loss 1.56495488 epoch total loss 1.72641802\n",
      "Trained batch 234 batch loss 1.53484237 epoch total loss 1.72559941\n",
      "Trained batch 235 batch loss 1.55928159 epoch total loss 1.72489166\n",
      "Trained batch 236 batch loss 1.62430096 epoch total loss 1.72446549\n",
      "Trained batch 237 batch loss 1.61425924 epoch total loss 1.72400045\n",
      "Trained batch 238 batch loss 1.6366694 epoch total loss 1.72363341\n",
      "Trained batch 239 batch loss 1.61509657 epoch total loss 1.72317922\n",
      "Trained batch 240 batch loss 1.51271093 epoch total loss 1.72230232\n",
      "Trained batch 241 batch loss 1.61810732 epoch total loss 1.72187006\n",
      "Trained batch 242 batch loss 1.58243728 epoch total loss 1.72129381\n",
      "Trained batch 243 batch loss 1.61841631 epoch total loss 1.72087038\n",
      "Trained batch 244 batch loss 1.72363448 epoch total loss 1.7208817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 245 batch loss 1.67375517 epoch total loss 1.72068942\n",
      "Trained batch 246 batch loss 1.62448394 epoch total loss 1.72029829\n",
      "Trained batch 247 batch loss 1.72235632 epoch total loss 1.72030663\n",
      "Trained batch 248 batch loss 1.56466436 epoch total loss 1.71967912\n",
      "Trained batch 249 batch loss 1.58549702 epoch total loss 1.71914029\n",
      "Trained batch 250 batch loss 1.5072695 epoch total loss 1.71829271\n",
      "Trained batch 251 batch loss 1.47520709 epoch total loss 1.71732426\n",
      "Trained batch 252 batch loss 1.55037928 epoch total loss 1.71666181\n",
      "Trained batch 253 batch loss 1.5551151 epoch total loss 1.71602333\n",
      "Trained batch 254 batch loss 1.54852974 epoch total loss 1.71536386\n",
      "Trained batch 255 batch loss 1.49199891 epoch total loss 1.71448791\n",
      "Trained batch 256 batch loss 1.72647941 epoch total loss 1.71453476\n",
      "Trained batch 257 batch loss 1.57359648 epoch total loss 1.7139864\n",
      "Trained batch 258 batch loss 1.44578052 epoch total loss 1.71294677\n",
      "Trained batch 259 batch loss 1.56966591 epoch total loss 1.71239364\n",
      "Trained batch 260 batch loss 1.66557884 epoch total loss 1.71221364\n",
      "Trained batch 261 batch loss 1.59704566 epoch total loss 1.71177232\n",
      "Trained batch 262 batch loss 1.62199593 epoch total loss 1.71142972\n",
      "Trained batch 263 batch loss 1.65221715 epoch total loss 1.71120465\n",
      "Trained batch 264 batch loss 1.64218926 epoch total loss 1.71094322\n",
      "Trained batch 265 batch loss 1.65371108 epoch total loss 1.71072721\n",
      "Trained batch 266 batch loss 1.70211387 epoch total loss 1.71069491\n",
      "Trained batch 267 batch loss 1.48270082 epoch total loss 1.70984089\n",
      "Trained batch 268 batch loss 1.64216697 epoch total loss 1.70958841\n",
      "Trained batch 269 batch loss 1.65751147 epoch total loss 1.70939481\n",
      "Trained batch 270 batch loss 1.58589971 epoch total loss 1.70893753\n",
      "Trained batch 271 batch loss 1.61914217 epoch total loss 1.70860612\n",
      "Trained batch 272 batch loss 1.60986602 epoch total loss 1.70824313\n",
      "Trained batch 273 batch loss 1.51142395 epoch total loss 1.70752215\n",
      "Trained batch 274 batch loss 1.53330874 epoch total loss 1.70688629\n",
      "Trained batch 275 batch loss 1.60056078 epoch total loss 1.70649958\n",
      "Trained batch 276 batch loss 1.6273551 epoch total loss 1.70621276\n",
      "Trained batch 277 batch loss 1.60940599 epoch total loss 1.70586336\n",
      "Trained batch 278 batch loss 1.50795 epoch total loss 1.70515144\n",
      "Trained batch 279 batch loss 1.61265218 epoch total loss 1.70481992\n",
      "Trained batch 280 batch loss 1.56625676 epoch total loss 1.70432496\n",
      "Trained batch 281 batch loss 1.59880197 epoch total loss 1.70394957\n",
      "Trained batch 282 batch loss 1.61059833 epoch total loss 1.70361853\n",
      "Trained batch 283 batch loss 1.59305406 epoch total loss 1.70322776\n",
      "Trained batch 284 batch loss 1.65859079 epoch total loss 1.70307064\n",
      "Trained batch 285 batch loss 1.55703521 epoch total loss 1.70255828\n",
      "Trained batch 286 batch loss 1.58646703 epoch total loss 1.70215225\n",
      "Trained batch 287 batch loss 1.62259889 epoch total loss 1.70187509\n",
      "Trained batch 288 batch loss 1.58131778 epoch total loss 1.70145655\n",
      "Trained batch 289 batch loss 1.54240012 epoch total loss 1.70090604\n",
      "Trained batch 290 batch loss 1.59195542 epoch total loss 1.70053041\n",
      "Trained batch 291 batch loss 1.56212223 epoch total loss 1.70005476\n",
      "Trained batch 292 batch loss 1.64414966 epoch total loss 1.69986331\n",
      "Trained batch 293 batch loss 1.60906911 epoch total loss 1.69955337\n",
      "Trained batch 294 batch loss 1.53691721 epoch total loss 1.69900024\n",
      "Trained batch 295 batch loss 1.5928421 epoch total loss 1.69864035\n",
      "Trained batch 296 batch loss 1.65152156 epoch total loss 1.6984812\n",
      "Trained batch 297 batch loss 1.66402054 epoch total loss 1.69836521\n",
      "Trained batch 298 batch loss 1.71376729 epoch total loss 1.69841695\n",
      "Trained batch 299 batch loss 1.69104993 epoch total loss 1.69839227\n",
      "Trained batch 300 batch loss 1.65395212 epoch total loss 1.69824409\n",
      "Trained batch 301 batch loss 1.58143473 epoch total loss 1.69785595\n",
      "Trained batch 302 batch loss 1.63437486 epoch total loss 1.69764578\n",
      "Trained batch 303 batch loss 1.64690709 epoch total loss 1.69747829\n",
      "Trained batch 304 batch loss 1.56060624 epoch total loss 1.69702816\n",
      "Trained batch 305 batch loss 1.63354802 epoch total loss 1.69682\n",
      "Trained batch 306 batch loss 1.53526926 epoch total loss 1.69629204\n",
      "Trained batch 307 batch loss 1.66788471 epoch total loss 1.69619966\n",
      "Trained batch 308 batch loss 1.57983303 epoch total loss 1.69582176\n",
      "Trained batch 309 batch loss 1.4627142 epoch total loss 1.69506741\n",
      "Trained batch 310 batch loss 1.47461963 epoch total loss 1.6943562\n",
      "Trained batch 311 batch loss 1.51445413 epoch total loss 1.6937778\n",
      "Trained batch 312 batch loss 1.65154421 epoch total loss 1.6936425\n",
      "Trained batch 313 batch loss 1.65002835 epoch total loss 1.69350314\n",
      "Trained batch 314 batch loss 1.61331511 epoch total loss 1.6932478\n",
      "Trained batch 315 batch loss 1.60848546 epoch total loss 1.69297862\n",
      "Trained batch 316 batch loss 1.57473195 epoch total loss 1.6926043\n",
      "Trained batch 317 batch loss 1.59145474 epoch total loss 1.69228518\n",
      "Trained batch 318 batch loss 1.49084377 epoch total loss 1.6916517\n",
      "Trained batch 319 batch loss 1.53177154 epoch total loss 1.69115067\n",
      "Trained batch 320 batch loss 1.60060692 epoch total loss 1.69086766\n",
      "Trained batch 321 batch loss 1.58322084 epoch total loss 1.69053221\n",
      "Trained batch 322 batch loss 1.63925719 epoch total loss 1.69037306\n",
      "Trained batch 323 batch loss 1.59602273 epoch total loss 1.69008088\n",
      "Trained batch 324 batch loss 1.59482372 epoch total loss 1.68978691\n",
      "Trained batch 325 batch loss 1.48901117 epoch total loss 1.68916917\n",
      "Trained batch 326 batch loss 1.59573936 epoch total loss 1.68888271\n",
      "Trained batch 327 batch loss 1.58257747 epoch total loss 1.68855762\n",
      "Trained batch 328 batch loss 1.59402847 epoch total loss 1.6882695\n",
      "Trained batch 329 batch loss 1.61593688 epoch total loss 1.68804967\n",
      "Trained batch 330 batch loss 1.53439081 epoch total loss 1.68758392\n",
      "Trained batch 331 batch loss 1.54764605 epoch total loss 1.68716121\n",
      "Trained batch 332 batch loss 1.53664589 epoch total loss 1.68670785\n",
      "Trained batch 333 batch loss 1.49235785 epoch total loss 1.68612421\n",
      "Trained batch 334 batch loss 1.47187006 epoch total loss 1.68548274\n",
      "Trained batch 335 batch loss 1.49726832 epoch total loss 1.68492091\n",
      "Trained batch 336 batch loss 1.50653756 epoch total loss 1.68439\n",
      "Trained batch 337 batch loss 1.62032938 epoch total loss 1.68419981\n",
      "Trained batch 338 batch loss 1.70628405 epoch total loss 1.68426514\n",
      "Trained batch 339 batch loss 1.57020199 epoch total loss 1.68392861\n",
      "Trained batch 340 batch loss 1.58934808 epoch total loss 1.68365049\n",
      "Trained batch 341 batch loss 1.51151729 epoch total loss 1.68314576\n",
      "Trained batch 342 batch loss 1.59376633 epoch total loss 1.68288434\n",
      "Trained batch 343 batch loss 1.56890225 epoch total loss 1.6825521\n",
      "Trained batch 344 batch loss 1.54860818 epoch total loss 1.68216264\n",
      "Trained batch 345 batch loss 1.53938317 epoch total loss 1.68174875\n",
      "Trained batch 346 batch loss 1.50436056 epoch total loss 1.68123591\n",
      "Trained batch 347 batch loss 1.62230766 epoch total loss 1.68106616\n",
      "Trained batch 348 batch loss 1.55294228 epoch total loss 1.68069792\n",
      "Trained batch 349 batch loss 1.49248648 epoch total loss 1.68015862\n",
      "Trained batch 350 batch loss 1.53879523 epoch total loss 1.67975485\n",
      "Trained batch 351 batch loss 1.61419916 epoch total loss 1.67956805\n",
      "Trained batch 352 batch loss 1.46633315 epoch total loss 1.67896223\n",
      "Trained batch 353 batch loss 1.4311794 epoch total loss 1.67826021\n",
      "Trained batch 354 batch loss 1.45608568 epoch total loss 1.67763269\n",
      "Trained batch 355 batch loss 1.46255088 epoch total loss 1.67702675\n",
      "Trained batch 356 batch loss 1.49011159 epoch total loss 1.67650163\n",
      "Trained batch 357 batch loss 1.62406301 epoch total loss 1.67635489\n",
      "Trained batch 358 batch loss 1.61598825 epoch total loss 1.6761862\n",
      "Trained batch 359 batch loss 1.60851252 epoch total loss 1.67599773\n",
      "Trained batch 360 batch loss 1.43654132 epoch total loss 1.67533243\n",
      "Trained batch 361 batch loss 1.51439285 epoch total loss 1.6748867\n",
      "Trained batch 362 batch loss 1.57142234 epoch total loss 1.67460084\n",
      "Trained batch 363 batch loss 1.60839045 epoch total loss 1.67441845\n",
      "Trained batch 364 batch loss 1.52743042 epoch total loss 1.67401457\n",
      "Trained batch 365 batch loss 1.55176139 epoch total loss 1.67367959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 366 batch loss 1.50531077 epoch total loss 1.67321956\n",
      "Trained batch 367 batch loss 1.53819895 epoch total loss 1.67285168\n",
      "Trained batch 368 batch loss 1.55171847 epoch total loss 1.67252254\n",
      "Trained batch 369 batch loss 1.43339181 epoch total loss 1.67187452\n",
      "Trained batch 370 batch loss 1.45614111 epoch total loss 1.67129135\n",
      "Trained batch 371 batch loss 1.57763851 epoch total loss 1.67103899\n",
      "Trained batch 372 batch loss 1.58244157 epoch total loss 1.67080081\n",
      "Trained batch 373 batch loss 1.60492563 epoch total loss 1.67062414\n",
      "Trained batch 374 batch loss 1.55644739 epoch total loss 1.67031896\n",
      "Trained batch 375 batch loss 1.54860842 epoch total loss 1.66999435\n",
      "Trained batch 376 batch loss 1.58523893 epoch total loss 1.66976893\n",
      "Trained batch 377 batch loss 1.62343431 epoch total loss 1.66964602\n",
      "Trained batch 378 batch loss 1.70822537 epoch total loss 1.66974819\n",
      "Trained batch 379 batch loss 1.63663769 epoch total loss 1.66966081\n",
      "Trained batch 380 batch loss 1.5571909 epoch total loss 1.66936481\n",
      "Trained batch 381 batch loss 1.59975052 epoch total loss 1.66918206\n",
      "Trained batch 382 batch loss 1.52887046 epoch total loss 1.66881478\n",
      "Trained batch 383 batch loss 1.53517318 epoch total loss 1.66846585\n",
      "Trained batch 384 batch loss 1.61324072 epoch total loss 1.66832197\n",
      "Trained batch 385 batch loss 1.73670053 epoch total loss 1.66849947\n",
      "Trained batch 386 batch loss 1.64363766 epoch total loss 1.6684351\n",
      "Trained batch 387 batch loss 1.54797626 epoch total loss 1.66812384\n",
      "Trained batch 388 batch loss 1.45177555 epoch total loss 1.66756618\n",
      "Trained batch 389 batch loss 1.47739506 epoch total loss 1.66707742\n",
      "Trained batch 390 batch loss 1.65534139 epoch total loss 1.66704726\n",
      "Trained batch 391 batch loss 1.57295763 epoch total loss 1.66680658\n",
      "Trained batch 392 batch loss 1.50210273 epoch total loss 1.66638637\n",
      "Trained batch 393 batch loss 1.49097037 epoch total loss 1.66594\n",
      "Trained batch 394 batch loss 1.6255784 epoch total loss 1.66583753\n",
      "Trained batch 395 batch loss 1.41871738 epoch total loss 1.6652118\n",
      "Trained batch 396 batch loss 1.5193671 epoch total loss 1.66484344\n",
      "Trained batch 397 batch loss 1.47898865 epoch total loss 1.66437531\n",
      "Trained batch 398 batch loss 1.55573404 epoch total loss 1.66410232\n",
      "Trained batch 399 batch loss 1.51394856 epoch total loss 1.66372609\n",
      "Trained batch 400 batch loss 1.58848751 epoch total loss 1.6635381\n",
      "Trained batch 401 batch loss 1.64311123 epoch total loss 1.6634872\n",
      "Trained batch 402 batch loss 1.5077914 epoch total loss 1.66309988\n",
      "Trained batch 403 batch loss 1.49336863 epoch total loss 1.66267872\n",
      "Trained batch 404 batch loss 1.58164108 epoch total loss 1.66247821\n",
      "Trained batch 405 batch loss 1.61822248 epoch total loss 1.66236889\n",
      "Trained batch 406 batch loss 1.60674691 epoch total loss 1.66223192\n",
      "Trained batch 407 batch loss 1.63190651 epoch total loss 1.66215742\n",
      "Trained batch 408 batch loss 1.52027953 epoch total loss 1.66180956\n",
      "Trained batch 409 batch loss 1.53729546 epoch total loss 1.6615051\n",
      "Trained batch 410 batch loss 1.54056358 epoch total loss 1.66121018\n",
      "Trained batch 411 batch loss 1.51420116 epoch total loss 1.66085255\n",
      "Trained batch 412 batch loss 1.42787766 epoch total loss 1.66028702\n",
      "Trained batch 413 batch loss 1.6023351 epoch total loss 1.66014683\n",
      "Trained batch 414 batch loss 1.58531857 epoch total loss 1.65996611\n",
      "Trained batch 415 batch loss 1.57665515 epoch total loss 1.65976536\n",
      "Trained batch 416 batch loss 1.7049799 epoch total loss 1.65987396\n",
      "Trained batch 417 batch loss 1.57750559 epoch total loss 1.65967643\n",
      "Trained batch 418 batch loss 1.63759136 epoch total loss 1.65962362\n",
      "Trained batch 419 batch loss 1.46162581 epoch total loss 1.65915096\n",
      "Trained batch 420 batch loss 1.64124334 epoch total loss 1.65910828\n",
      "Trained batch 421 batch loss 1.54202485 epoch total loss 1.65883029\n",
      "Trained batch 422 batch loss 1.61363828 epoch total loss 1.65872324\n",
      "Trained batch 423 batch loss 1.67958426 epoch total loss 1.65877247\n",
      "Trained batch 424 batch loss 1.67354727 epoch total loss 1.65880728\n",
      "Trained batch 425 batch loss 1.6596626 epoch total loss 1.6588093\n",
      "Trained batch 426 batch loss 1.62841368 epoch total loss 1.65873802\n",
      "Trained batch 427 batch loss 1.65069175 epoch total loss 1.65871918\n",
      "Trained batch 428 batch loss 1.67533517 epoch total loss 1.65875804\n",
      "Trained batch 429 batch loss 1.58233404 epoch total loss 1.65857983\n",
      "Trained batch 430 batch loss 1.62499249 epoch total loss 1.65850174\n",
      "Trained batch 431 batch loss 1.6711247 epoch total loss 1.65853107\n",
      "Trained batch 432 batch loss 1.54575396 epoch total loss 1.65827012\n",
      "Trained batch 433 batch loss 1.56824899 epoch total loss 1.65806222\n",
      "Trained batch 434 batch loss 1.55608141 epoch total loss 1.65782726\n",
      "Trained batch 435 batch loss 1.59132767 epoch total loss 1.65767431\n",
      "Trained batch 436 batch loss 1.51124191 epoch total loss 1.65733838\n",
      "Trained batch 437 batch loss 1.6087774 epoch total loss 1.65722728\n",
      "Trained batch 438 batch loss 1.60320544 epoch total loss 1.6571039\n",
      "Trained batch 439 batch loss 1.63135934 epoch total loss 1.65704525\n",
      "Trained batch 440 batch loss 1.65709507 epoch total loss 1.65704536\n",
      "Trained batch 441 batch loss 1.61136174 epoch total loss 1.65694189\n",
      "Trained batch 442 batch loss 1.49673784 epoch total loss 1.65657949\n",
      "Trained batch 443 batch loss 1.4815923 epoch total loss 1.65618443\n",
      "Trained batch 444 batch loss 1.59545422 epoch total loss 1.6560477\n",
      "Trained batch 445 batch loss 1.57606733 epoch total loss 1.65586793\n",
      "Trained batch 446 batch loss 1.51825428 epoch total loss 1.6555593\n",
      "Trained batch 447 batch loss 1.53289223 epoch total loss 1.65528488\n",
      "Trained batch 448 batch loss 1.63806272 epoch total loss 1.6552465\n",
      "Trained batch 449 batch loss 1.56755471 epoch total loss 1.65505123\n",
      "Trained batch 450 batch loss 1.6584878 epoch total loss 1.65505886\n",
      "Trained batch 451 batch loss 1.35526061 epoch total loss 1.65439415\n",
      "Trained batch 452 batch loss 1.48099351 epoch total loss 1.65401065\n",
      "Trained batch 453 batch loss 1.60362387 epoch total loss 1.65389943\n",
      "Trained batch 454 batch loss 1.64243388 epoch total loss 1.65387416\n",
      "Trained batch 455 batch loss 1.58880103 epoch total loss 1.65373123\n",
      "Trained batch 456 batch loss 1.57581925 epoch total loss 1.65356028\n",
      "Trained batch 457 batch loss 1.53831744 epoch total loss 1.65330815\n",
      "Trained batch 458 batch loss 1.53738475 epoch total loss 1.65305507\n",
      "Trained batch 459 batch loss 1.44530153 epoch total loss 1.65260255\n",
      "Trained batch 460 batch loss 1.39322424 epoch total loss 1.65203869\n",
      "Trained batch 461 batch loss 1.50283504 epoch total loss 1.65171504\n",
      "Trained batch 462 batch loss 1.66851258 epoch total loss 1.6517514\n",
      "Trained batch 463 batch loss 1.70756269 epoch total loss 1.65187192\n",
      "Trained batch 464 batch loss 1.65622914 epoch total loss 1.65188134\n",
      "Trained batch 465 batch loss 1.60500467 epoch total loss 1.65178049\n",
      "Trained batch 466 batch loss 1.64797401 epoch total loss 1.65177226\n",
      "Trained batch 467 batch loss 1.56855917 epoch total loss 1.65159404\n",
      "Trained batch 468 batch loss 1.53585947 epoch total loss 1.6513468\n",
      "Trained batch 469 batch loss 1.55735278 epoch total loss 1.65114653\n",
      "Trained batch 470 batch loss 1.57268596 epoch total loss 1.65097952\n",
      "Trained batch 471 batch loss 1.58411145 epoch total loss 1.65083754\n",
      "Trained batch 472 batch loss 1.54265428 epoch total loss 1.65060842\n",
      "Trained batch 473 batch loss 1.60548759 epoch total loss 1.65051293\n",
      "Trained batch 474 batch loss 1.49520564 epoch total loss 1.65018523\n",
      "Trained batch 475 batch loss 1.57402444 epoch total loss 1.65002489\n",
      "Trained batch 476 batch loss 1.56343639 epoch total loss 1.64984298\n",
      "Trained batch 477 batch loss 1.52186215 epoch total loss 1.64957464\n",
      "Trained batch 478 batch loss 1.48244882 epoch total loss 1.649225\n",
      "Trained batch 479 batch loss 1.54117894 epoch total loss 1.64899945\n",
      "Trained batch 480 batch loss 1.52486897 epoch total loss 1.64874077\n",
      "Trained batch 481 batch loss 1.52335596 epoch total loss 1.64848018\n",
      "Trained batch 482 batch loss 1.4799459 epoch total loss 1.64813042\n",
      "Trained batch 483 batch loss 1.49641383 epoch total loss 1.6478163\n",
      "Trained batch 484 batch loss 1.51682806 epoch total loss 1.6475457\n",
      "Trained batch 485 batch loss 1.60319257 epoch total loss 1.64745426\n",
      "Trained batch 486 batch loss 1.51127851 epoch total loss 1.64717412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 487 batch loss 1.34753716 epoch total loss 1.64655876\n",
      "Trained batch 488 batch loss 1.28209627 epoch total loss 1.64581203\n",
      "Trained batch 489 batch loss 1.46563661 epoch total loss 1.64544356\n",
      "Trained batch 490 batch loss 1.52072859 epoch total loss 1.64518905\n",
      "Trained batch 491 batch loss 1.45804822 epoch total loss 1.64480793\n",
      "Trained batch 492 batch loss 1.54649806 epoch total loss 1.64460814\n",
      "Trained batch 493 batch loss 1.51401567 epoch total loss 1.64434326\n",
      "Trained batch 494 batch loss 1.48184383 epoch total loss 1.64401448\n",
      "Trained batch 495 batch loss 1.63495779 epoch total loss 1.64399612\n",
      "Trained batch 496 batch loss 1.46575022 epoch total loss 1.64363682\n",
      "Trained batch 497 batch loss 1.68829012 epoch total loss 1.64372659\n",
      "Trained batch 498 batch loss 1.81790376 epoch total loss 1.64407647\n",
      "Trained batch 499 batch loss 1.66889405 epoch total loss 1.64412618\n",
      "Trained batch 500 batch loss 1.64282143 epoch total loss 1.64412355\n",
      "Trained batch 501 batch loss 1.6217823 epoch total loss 1.64407885\n",
      "Trained batch 502 batch loss 1.55186355 epoch total loss 1.64389527\n",
      "Trained batch 503 batch loss 1.5739578 epoch total loss 1.64375627\n",
      "Trained batch 504 batch loss 1.58766878 epoch total loss 1.64364493\n",
      "Trained batch 505 batch loss 1.57913423 epoch total loss 1.64351726\n",
      "Trained batch 506 batch loss 1.59445751 epoch total loss 1.64342034\n",
      "Trained batch 507 batch loss 1.58904815 epoch total loss 1.64331305\n",
      "Trained batch 508 batch loss 1.56967688 epoch total loss 1.64316821\n",
      "Trained batch 509 batch loss 1.60845506 epoch total loss 1.6431\n",
      "Trained batch 510 batch loss 1.5828588 epoch total loss 1.64298189\n",
      "Trained batch 511 batch loss 1.53367424 epoch total loss 1.64276803\n",
      "Trained batch 512 batch loss 1.61560857 epoch total loss 1.64271498\n",
      "Trained batch 513 batch loss 1.60445023 epoch total loss 1.64264035\n",
      "Trained batch 514 batch loss 1.58315814 epoch total loss 1.6425246\n",
      "Trained batch 515 batch loss 1.63705707 epoch total loss 1.64251399\n",
      "Trained batch 516 batch loss 1.58049524 epoch total loss 1.64239383\n",
      "Trained batch 517 batch loss 1.6112051 epoch total loss 1.64233351\n",
      "Trained batch 518 batch loss 1.52065217 epoch total loss 1.64209855\n",
      "Trained batch 519 batch loss 1.56069481 epoch total loss 1.64194167\n",
      "Trained batch 520 batch loss 1.59203839 epoch total loss 1.6418457\n",
      "Trained batch 521 batch loss 1.61219835 epoch total loss 1.64178872\n",
      "Trained batch 522 batch loss 1.52838564 epoch total loss 1.64157152\n",
      "Trained batch 523 batch loss 1.50421476 epoch total loss 1.6413089\n",
      "Trained batch 524 batch loss 1.49948072 epoch total loss 1.64103818\n",
      "Trained batch 525 batch loss 1.53068531 epoch total loss 1.64082801\n",
      "Trained batch 526 batch loss 1.48112547 epoch total loss 1.64052439\n",
      "Trained batch 527 batch loss 1.5248692 epoch total loss 1.64030492\n",
      "Trained batch 528 batch loss 1.51053357 epoch total loss 1.64005911\n",
      "Trained batch 529 batch loss 1.57289386 epoch total loss 1.63993216\n",
      "Trained batch 530 batch loss 1.55266166 epoch total loss 1.63976753\n",
      "Trained batch 531 batch loss 1.55312967 epoch total loss 1.63960433\n",
      "Trained batch 532 batch loss 1.56138158 epoch total loss 1.63945735\n",
      "Trained batch 533 batch loss 1.47131538 epoch total loss 1.6391418\n",
      "Trained batch 534 batch loss 1.48817563 epoch total loss 1.63885915\n",
      "Trained batch 535 batch loss 1.58998036 epoch total loss 1.63876772\n",
      "Trained batch 536 batch loss 1.58682632 epoch total loss 1.6386708\n",
      "Trained batch 537 batch loss 1.60571611 epoch total loss 1.63860953\n",
      "Trained batch 538 batch loss 1.57604575 epoch total loss 1.63849318\n",
      "Trained batch 539 batch loss 1.53726912 epoch total loss 1.63830543\n",
      "Trained batch 540 batch loss 1.49907684 epoch total loss 1.63804758\n",
      "Trained batch 541 batch loss 1.47796082 epoch total loss 1.6377517\n",
      "Trained batch 542 batch loss 1.53115666 epoch total loss 1.637555\n",
      "Trained batch 543 batch loss 1.60625768 epoch total loss 1.63749743\n",
      "Trained batch 544 batch loss 1.49047101 epoch total loss 1.63722706\n",
      "Trained batch 545 batch loss 1.5429821 epoch total loss 1.6370542\n",
      "Trained batch 546 batch loss 1.55112374 epoch total loss 1.63689685\n",
      "Trained batch 547 batch loss 1.5462395 epoch total loss 1.63673115\n",
      "Trained batch 548 batch loss 1.56387138 epoch total loss 1.63659811\n",
      "Trained batch 549 batch loss 1.53284633 epoch total loss 1.63640916\n",
      "Trained batch 550 batch loss 1.47258 epoch total loss 1.63611126\n",
      "Trained batch 551 batch loss 1.43886495 epoch total loss 1.63575327\n",
      "Trained batch 552 batch loss 1.53955936 epoch total loss 1.63557899\n",
      "Trained batch 553 batch loss 1.57038701 epoch total loss 1.63546109\n",
      "Trained batch 554 batch loss 1.57668579 epoch total loss 1.63535488\n",
      "Trained batch 555 batch loss 1.60448503 epoch total loss 1.63529932\n",
      "Trained batch 556 batch loss 1.66635311 epoch total loss 1.63535523\n",
      "Trained batch 557 batch loss 1.6481657 epoch total loss 1.63537824\n",
      "Trained batch 558 batch loss 1.47849917 epoch total loss 1.63509715\n",
      "Trained batch 559 batch loss 1.54862928 epoch total loss 1.63494253\n",
      "Trained batch 560 batch loss 1.55526698 epoch total loss 1.6348002\n",
      "Trained batch 561 batch loss 1.49903405 epoch total loss 1.6345582\n",
      "Trained batch 562 batch loss 1.50918519 epoch total loss 1.63433504\n",
      "Trained batch 563 batch loss 1.45885456 epoch total loss 1.63402331\n",
      "Trained batch 564 batch loss 1.54857922 epoch total loss 1.63387179\n",
      "Trained batch 565 batch loss 1.57529092 epoch total loss 1.6337682\n",
      "Trained batch 566 batch loss 1.55751944 epoch total loss 1.63363349\n",
      "Trained batch 567 batch loss 1.44357264 epoch total loss 1.63329816\n",
      "Trained batch 568 batch loss 1.46961403 epoch total loss 1.63301\n",
      "Trained batch 569 batch loss 1.50503993 epoch total loss 1.6327852\n",
      "Trained batch 570 batch loss 1.52558589 epoch total loss 1.63259709\n",
      "Trained batch 571 batch loss 1.54181731 epoch total loss 1.63243806\n",
      "Trained batch 572 batch loss 1.56288815 epoch total loss 1.63231647\n",
      "Trained batch 573 batch loss 1.43333769 epoch total loss 1.63196921\n",
      "Trained batch 574 batch loss 1.48771644 epoch total loss 1.63171792\n",
      "Trained batch 575 batch loss 1.55305123 epoch total loss 1.63158107\n",
      "Trained batch 576 batch loss 1.58990145 epoch total loss 1.63150871\n",
      "Trained batch 577 batch loss 1.62076044 epoch total loss 1.63149011\n",
      "Trained batch 578 batch loss 1.65567088 epoch total loss 1.63153207\n",
      "Trained batch 579 batch loss 1.69544744 epoch total loss 1.63164234\n",
      "Trained batch 580 batch loss 1.52119374 epoch total loss 1.63145196\n",
      "Trained batch 581 batch loss 1.49141288 epoch total loss 1.63121092\n",
      "Trained batch 582 batch loss 1.46994185 epoch total loss 1.63093388\n",
      "Trained batch 583 batch loss 1.5445503 epoch total loss 1.6307857\n",
      "Trained batch 584 batch loss 1.64575326 epoch total loss 1.63081133\n",
      "Trained batch 585 batch loss 1.55951095 epoch total loss 1.63068938\n",
      "Trained batch 586 batch loss 1.57230461 epoch total loss 1.63058984\n",
      "Trained batch 587 batch loss 1.48943377 epoch total loss 1.6303494\n",
      "Trained batch 588 batch loss 1.38811684 epoch total loss 1.62993741\n",
      "Trained batch 589 batch loss 1.3558259 epoch total loss 1.62947202\n",
      "Trained batch 590 batch loss 1.4174993 epoch total loss 1.62911272\n",
      "Trained batch 591 batch loss 1.50405025 epoch total loss 1.62890112\n",
      "Trained batch 592 batch loss 1.32525373 epoch total loss 1.62838817\n",
      "Trained batch 593 batch loss 1.59363985 epoch total loss 1.62832952\n",
      "Trained batch 594 batch loss 1.49843931 epoch total loss 1.62811089\n",
      "Trained batch 595 batch loss 1.48775887 epoch total loss 1.62787485\n",
      "Trained batch 596 batch loss 1.42952549 epoch total loss 1.62754202\n",
      "Trained batch 597 batch loss 1.5397408 epoch total loss 1.62739503\n",
      "Trained batch 598 batch loss 1.44031525 epoch total loss 1.62708211\n",
      "Trained batch 599 batch loss 1.56923652 epoch total loss 1.62698555\n",
      "Trained batch 600 batch loss 1.51975358 epoch total loss 1.62680686\n",
      "Trained batch 601 batch loss 1.21503806 epoch total loss 1.62612164\n",
      "Trained batch 602 batch loss 1.15361357 epoch total loss 1.62533677\n",
      "Trained batch 603 batch loss 1.45766616 epoch total loss 1.62505865\n",
      "Trained batch 604 batch loss 1.57796335 epoch total loss 1.62498069\n",
      "Trained batch 605 batch loss 1.67963696 epoch total loss 1.62507105\n",
      "Trained batch 606 batch loss 1.67971754 epoch total loss 1.62516117\n",
      "Trained batch 607 batch loss 1.55388594 epoch total loss 1.62504375\n",
      "Trained batch 608 batch loss 1.58798075 epoch total loss 1.62498271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 1.4700644 epoch total loss 1.62472844\n",
      "Trained batch 610 batch loss 1.61446369 epoch total loss 1.62471151\n",
      "Trained batch 611 batch loss 1.48861933 epoch total loss 1.62448883\n",
      "Trained batch 612 batch loss 1.55660105 epoch total loss 1.62437785\n",
      "Trained batch 613 batch loss 1.47561812 epoch total loss 1.62413526\n",
      "Trained batch 614 batch loss 1.43967438 epoch total loss 1.62383485\n",
      "Trained batch 615 batch loss 1.57517719 epoch total loss 1.62375581\n",
      "Trained batch 616 batch loss 1.522668 epoch total loss 1.62359166\n",
      "Trained batch 617 batch loss 1.40393257 epoch total loss 1.62323558\n",
      "Trained batch 618 batch loss 1.47421384 epoch total loss 1.62299454\n",
      "Trained batch 619 batch loss 1.44879699 epoch total loss 1.62271309\n",
      "Trained batch 620 batch loss 1.432392 epoch total loss 1.62240613\n",
      "Trained batch 621 batch loss 1.38263011 epoch total loss 1.62202\n",
      "Trained batch 622 batch loss 1.45855224 epoch total loss 1.62175715\n",
      "Trained batch 623 batch loss 1.455917 epoch total loss 1.62149107\n",
      "Trained batch 624 batch loss 1.45555961 epoch total loss 1.62122512\n",
      "Trained batch 625 batch loss 1.52135861 epoch total loss 1.62106538\n",
      "Trained batch 626 batch loss 1.51481724 epoch total loss 1.62089562\n",
      "Trained batch 627 batch loss 1.55408394 epoch total loss 1.62078905\n",
      "Trained batch 628 batch loss 1.54767609 epoch total loss 1.62067258\n",
      "Trained batch 629 batch loss 1.6283226 epoch total loss 1.62068474\n",
      "Trained batch 630 batch loss 1.69377244 epoch total loss 1.62080073\n",
      "Trained batch 631 batch loss 1.64400887 epoch total loss 1.62083757\n",
      "Trained batch 632 batch loss 1.53075242 epoch total loss 1.62069488\n",
      "Trained batch 633 batch loss 1.47700453 epoch total loss 1.62046802\n",
      "Trained batch 634 batch loss 1.40886927 epoch total loss 1.62013412\n",
      "Trained batch 635 batch loss 1.49888539 epoch total loss 1.61994326\n",
      "Trained batch 636 batch loss 1.42552161 epoch total loss 1.61963749\n",
      "Trained batch 637 batch loss 1.54012442 epoch total loss 1.6195128\n",
      "Trained batch 638 batch loss 1.52313936 epoch total loss 1.61936176\n",
      "Trained batch 639 batch loss 1.57396448 epoch total loss 1.61929083\n",
      "Trained batch 640 batch loss 1.58793592 epoch total loss 1.61924171\n",
      "Trained batch 641 batch loss 1.58627558 epoch total loss 1.61919034\n",
      "Trained batch 642 batch loss 1.56730151 epoch total loss 1.61910939\n",
      "Trained batch 643 batch loss 1.53913271 epoch total loss 1.61898518\n",
      "Trained batch 644 batch loss 1.49073744 epoch total loss 1.61878598\n",
      "Trained batch 645 batch loss 1.5707072 epoch total loss 1.61871135\n",
      "Trained batch 646 batch loss 1.51455057 epoch total loss 1.61855006\n",
      "Trained batch 647 batch loss 1.48380375 epoch total loss 1.6183418\n",
      "Trained batch 648 batch loss 1.46090627 epoch total loss 1.61809886\n",
      "Trained batch 649 batch loss 1.48750067 epoch total loss 1.61789775\n",
      "Trained batch 650 batch loss 1.42334175 epoch total loss 1.61759841\n",
      "Trained batch 651 batch loss 1.47549963 epoch total loss 1.61738\n",
      "Trained batch 652 batch loss 1.48504627 epoch total loss 1.61717701\n",
      "Trained batch 653 batch loss 1.45479059 epoch total loss 1.61692846\n",
      "Trained batch 654 batch loss 1.51302111 epoch total loss 1.61676955\n",
      "Trained batch 655 batch loss 1.5132097 epoch total loss 1.61661148\n",
      "Trained batch 656 batch loss 1.54729819 epoch total loss 1.61650574\n",
      "Trained batch 657 batch loss 1.50630677 epoch total loss 1.61633801\n",
      "Trained batch 658 batch loss 1.55340242 epoch total loss 1.61624229\n",
      "Trained batch 659 batch loss 1.59737206 epoch total loss 1.61621368\n",
      "Trained batch 660 batch loss 1.53657889 epoch total loss 1.61609316\n",
      "Trained batch 661 batch loss 1.53384721 epoch total loss 1.61596859\n",
      "Trained batch 662 batch loss 1.46482015 epoch total loss 1.6157403\n",
      "Trained batch 663 batch loss 1.37044954 epoch total loss 1.61537039\n",
      "Trained batch 664 batch loss 1.41651845 epoch total loss 1.61507094\n",
      "Trained batch 665 batch loss 1.41912532 epoch total loss 1.61477625\n",
      "Trained batch 666 batch loss 1.52033257 epoch total loss 1.61463451\n",
      "Trained batch 667 batch loss 1.483091 epoch total loss 1.61443722\n",
      "Trained batch 668 batch loss 1.42525351 epoch total loss 1.61415398\n",
      "Trained batch 669 batch loss 1.46297622 epoch total loss 1.61392808\n",
      "Trained batch 670 batch loss 1.51109028 epoch total loss 1.61377466\n",
      "Trained batch 671 batch loss 1.39526653 epoch total loss 1.61344898\n",
      "Trained batch 672 batch loss 1.39343488 epoch total loss 1.61312163\n",
      "Trained batch 673 batch loss 1.46637118 epoch total loss 1.61290359\n",
      "Trained batch 674 batch loss 1.5704478 epoch total loss 1.61284065\n",
      "Trained batch 675 batch loss 1.63719344 epoch total loss 1.61287665\n",
      "Trained batch 676 batch loss 1.50755274 epoch total loss 1.61272097\n",
      "Trained batch 677 batch loss 1.52897358 epoch total loss 1.61259711\n",
      "Trained batch 678 batch loss 1.53056216 epoch total loss 1.61247611\n",
      "Trained batch 679 batch loss 1.53436685 epoch total loss 1.61236119\n",
      "Trained batch 680 batch loss 1.54152608 epoch total loss 1.61225688\n",
      "Trained batch 681 batch loss 1.47697854 epoch total loss 1.61205816\n",
      "Trained batch 682 batch loss 1.50721908 epoch total loss 1.6119045\n",
      "Trained batch 683 batch loss 1.35816562 epoch total loss 1.61153293\n",
      "Trained batch 684 batch loss 1.54754972 epoch total loss 1.61143947\n",
      "Trained batch 685 batch loss 1.52068889 epoch total loss 1.61130691\n",
      "Trained batch 686 batch loss 1.44717395 epoch total loss 1.61106765\n",
      "Trained batch 687 batch loss 1.50473094 epoch total loss 1.61091292\n",
      "Trained batch 688 batch loss 1.48053336 epoch total loss 1.6107235\n",
      "Trained batch 689 batch loss 1.40533948 epoch total loss 1.61042547\n",
      "Trained batch 690 batch loss 1.44958639 epoch total loss 1.6101923\n",
      "Trained batch 691 batch loss 1.4477371 epoch total loss 1.60995722\n",
      "Trained batch 692 batch loss 1.53372037 epoch total loss 1.60984707\n",
      "Trained batch 693 batch loss 1.51197672 epoch total loss 1.60970581\n",
      "Trained batch 694 batch loss 1.55615067 epoch total loss 1.60962868\n",
      "Trained batch 695 batch loss 1.55859137 epoch total loss 1.60955524\n",
      "Trained batch 696 batch loss 1.5127387 epoch total loss 1.60941601\n",
      "Trained batch 697 batch loss 1.65142429 epoch total loss 1.60947621\n",
      "Trained batch 698 batch loss 1.63173985 epoch total loss 1.60950804\n",
      "Trained batch 699 batch loss 1.56974947 epoch total loss 1.60945117\n",
      "Trained batch 700 batch loss 1.59847617 epoch total loss 1.60943556\n",
      "Trained batch 701 batch loss 1.54189432 epoch total loss 1.60933912\n",
      "Trained batch 702 batch loss 1.53240108 epoch total loss 1.60922945\n",
      "Trained batch 703 batch loss 1.54281735 epoch total loss 1.60913503\n",
      "Trained batch 704 batch loss 1.64004481 epoch total loss 1.6091789\n",
      "Trained batch 705 batch loss 1.63215399 epoch total loss 1.60921156\n",
      "Trained batch 706 batch loss 1.58696914 epoch total loss 1.60918\n",
      "Trained batch 707 batch loss 1.54347467 epoch total loss 1.60908699\n",
      "Trained batch 708 batch loss 1.56177914 epoch total loss 1.60902011\n",
      "Trained batch 709 batch loss 1.55785322 epoch total loss 1.60894799\n",
      "Trained batch 710 batch loss 1.56279 epoch total loss 1.6088829\n",
      "Trained batch 711 batch loss 1.57848883 epoch total loss 1.60884023\n",
      "Trained batch 712 batch loss 1.56611931 epoch total loss 1.60878026\n",
      "Trained batch 713 batch loss 1.57246518 epoch total loss 1.60872936\n",
      "Trained batch 714 batch loss 1.34236968 epoch total loss 1.60835636\n",
      "Trained batch 715 batch loss 1.36557794 epoch total loss 1.60801685\n",
      "Trained batch 716 batch loss 1.38412595 epoch total loss 1.60770416\n",
      "Trained batch 717 batch loss 1.31213403 epoch total loss 1.60729194\n",
      "Trained batch 718 batch loss 1.44192469 epoch total loss 1.60706162\n",
      "Trained batch 719 batch loss 1.58247352 epoch total loss 1.60702753\n",
      "Trained batch 720 batch loss 1.44960082 epoch total loss 1.60680878\n",
      "Trained batch 721 batch loss 1.58677232 epoch total loss 1.60678101\n",
      "Trained batch 722 batch loss 1.64057755 epoch total loss 1.60682797\n",
      "Trained batch 723 batch loss 1.55974889 epoch total loss 1.60676277\n",
      "Trained batch 724 batch loss 1.54738879 epoch total loss 1.60668063\n",
      "Trained batch 725 batch loss 1.58033013 epoch total loss 1.60664427\n",
      "Trained batch 726 batch loss 1.59384787 epoch total loss 1.60662675\n",
      "Trained batch 727 batch loss 1.50710666 epoch total loss 1.60648978\n",
      "Trained batch 728 batch loss 1.52238429 epoch total loss 1.60637426\n",
      "Trained batch 729 batch loss 1.63047636 epoch total loss 1.60640728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 730 batch loss 1.55726707 epoch total loss 1.60633993\n",
      "Trained batch 731 batch loss 1.5579071 epoch total loss 1.60627365\n",
      "Trained batch 732 batch loss 1.46749663 epoch total loss 1.60608411\n",
      "Trained batch 733 batch loss 1.48484957 epoch total loss 1.60591877\n",
      "Trained batch 734 batch loss 1.47513568 epoch total loss 1.60574055\n",
      "Trained batch 735 batch loss 1.42889285 epoch total loss 1.60549986\n",
      "Trained batch 736 batch loss 1.57392776 epoch total loss 1.60545695\n",
      "Trained batch 737 batch loss 1.49778271 epoch total loss 1.60531092\n",
      "Trained batch 738 batch loss 1.50327086 epoch total loss 1.60517263\n",
      "Trained batch 739 batch loss 1.60862136 epoch total loss 1.6051774\n",
      "Trained batch 740 batch loss 1.68197489 epoch total loss 1.60528123\n",
      "Trained batch 741 batch loss 1.71948624 epoch total loss 1.60543525\n",
      "Trained batch 742 batch loss 1.66033447 epoch total loss 1.60550916\n",
      "Trained batch 743 batch loss 1.56795275 epoch total loss 1.60545874\n",
      "Trained batch 744 batch loss 1.60499632 epoch total loss 1.60545814\n",
      "Trained batch 745 batch loss 1.51331425 epoch total loss 1.6053344\n",
      "Trained batch 746 batch loss 1.49161851 epoch total loss 1.60518193\n",
      "Trained batch 747 batch loss 1.54574275 epoch total loss 1.60510242\n",
      "Trained batch 748 batch loss 1.60060585 epoch total loss 1.60509634\n",
      "Trained batch 749 batch loss 1.50206208 epoch total loss 1.60495877\n",
      "Trained batch 750 batch loss 1.44119501 epoch total loss 1.60474038\n",
      "Trained batch 751 batch loss 1.468907 epoch total loss 1.60455942\n",
      "Trained batch 752 batch loss 1.49791205 epoch total loss 1.60441768\n",
      "Trained batch 753 batch loss 1.59132266 epoch total loss 1.60440028\n",
      "Trained batch 754 batch loss 1.69404042 epoch total loss 1.60451925\n",
      "Trained batch 755 batch loss 1.66456258 epoch total loss 1.60459876\n",
      "Trained batch 756 batch loss 1.64693654 epoch total loss 1.60465479\n",
      "Trained batch 757 batch loss 1.58899426 epoch total loss 1.60463405\n",
      "Trained batch 758 batch loss 1.64688087 epoch total loss 1.60468984\n",
      "Trained batch 759 batch loss 1.5979836 epoch total loss 1.60468102\n",
      "Trained batch 760 batch loss 1.56764293 epoch total loss 1.60463226\n",
      "Trained batch 761 batch loss 1.53591537 epoch total loss 1.6045419\n",
      "Trained batch 762 batch loss 1.61995983 epoch total loss 1.60456216\n",
      "Trained batch 763 batch loss 1.72929513 epoch total loss 1.6047256\n",
      "Trained batch 764 batch loss 1.62057114 epoch total loss 1.60474634\n",
      "Trained batch 765 batch loss 1.69018149 epoch total loss 1.60485804\n",
      "Trained batch 766 batch loss 1.60892844 epoch total loss 1.60486329\n",
      "Trained batch 767 batch loss 1.56122601 epoch total loss 1.60480654\n",
      "Trained batch 768 batch loss 1.43033671 epoch total loss 1.60457933\n",
      "Trained batch 769 batch loss 1.4266088 epoch total loss 1.60434794\n",
      "Trained batch 770 batch loss 1.53569818 epoch total loss 1.60425866\n",
      "Trained batch 771 batch loss 1.5924077 epoch total loss 1.60424328\n",
      "Trained batch 772 batch loss 1.49784362 epoch total loss 1.60410547\n",
      "Trained batch 773 batch loss 1.47856259 epoch total loss 1.60394299\n",
      "Trained batch 774 batch loss 1.43438959 epoch total loss 1.603724\n",
      "Trained batch 775 batch loss 1.47367394 epoch total loss 1.60355616\n",
      "Trained batch 776 batch loss 1.54722297 epoch total loss 1.60348356\n",
      "Trained batch 777 batch loss 1.37244749 epoch total loss 1.60318613\n",
      "Trained batch 778 batch loss 1.48763728 epoch total loss 1.60303771\n",
      "Trained batch 779 batch loss 1.50487816 epoch total loss 1.60291171\n",
      "Trained batch 780 batch loss 1.50781083 epoch total loss 1.60278976\n",
      "Trained batch 781 batch loss 1.51112103 epoch total loss 1.60267234\n",
      "Trained batch 782 batch loss 1.62412322 epoch total loss 1.60269988\n",
      "Trained batch 783 batch loss 1.60148072 epoch total loss 1.60269821\n",
      "Trained batch 784 batch loss 1.68769741 epoch total loss 1.60280669\n",
      "Trained batch 785 batch loss 1.65599251 epoch total loss 1.60287452\n",
      "Trained batch 786 batch loss 1.70257235 epoch total loss 1.60300124\n",
      "Trained batch 787 batch loss 1.62133098 epoch total loss 1.6030246\n",
      "Trained batch 788 batch loss 1.53225732 epoch total loss 1.60293472\n",
      "Trained batch 789 batch loss 1.45268071 epoch total loss 1.60274422\n",
      "Trained batch 790 batch loss 1.3156898 epoch total loss 1.60238087\n",
      "Trained batch 791 batch loss 1.33765471 epoch total loss 1.60204613\n",
      "Trained batch 792 batch loss 1.46644449 epoch total loss 1.60187495\n",
      "Trained batch 793 batch loss 1.39014602 epoch total loss 1.60160792\n",
      "Trained batch 794 batch loss 1.26446831 epoch total loss 1.60118341\n",
      "Trained batch 795 batch loss 1.27126789 epoch total loss 1.60076833\n",
      "Trained batch 796 batch loss 1.27893174 epoch total loss 1.60036409\n",
      "Trained batch 797 batch loss 1.41005588 epoch total loss 1.60012519\n",
      "Trained batch 798 batch loss 1.47787237 epoch total loss 1.59997201\n",
      "Trained batch 799 batch loss 1.55849957 epoch total loss 1.59992015\n",
      "Trained batch 800 batch loss 1.58439493 epoch total loss 1.59990072\n",
      "Trained batch 801 batch loss 1.5785147 epoch total loss 1.5998739\n",
      "Trained batch 802 batch loss 1.61976039 epoch total loss 1.5998987\n",
      "Trained batch 803 batch loss 1.58292329 epoch total loss 1.59987748\n",
      "Trained batch 804 batch loss 1.41995144 epoch total loss 1.59965372\n",
      "Trained batch 805 batch loss 1.60351837 epoch total loss 1.59965849\n",
      "Trained batch 806 batch loss 1.38003266 epoch total loss 1.59938598\n",
      "Trained batch 807 batch loss 1.29639792 epoch total loss 1.59901047\n",
      "Trained batch 808 batch loss 1.43780446 epoch total loss 1.59881091\n",
      "Trained batch 809 batch loss 1.41508102 epoch total loss 1.59858382\n",
      "Trained batch 810 batch loss 1.53322029 epoch total loss 1.59850311\n",
      "Trained batch 811 batch loss 1.48190606 epoch total loss 1.59835935\n",
      "Trained batch 812 batch loss 1.56282926 epoch total loss 1.5983156\n",
      "Trained batch 813 batch loss 1.54491985 epoch total loss 1.59824991\n",
      "Trained batch 814 batch loss 1.4781456 epoch total loss 1.59810245\n",
      "Trained batch 815 batch loss 1.47525454 epoch total loss 1.59795165\n",
      "Trained batch 816 batch loss 1.45327544 epoch total loss 1.59777427\n",
      "Trained batch 817 batch loss 1.40123498 epoch total loss 1.5975337\n",
      "Trained batch 818 batch loss 1.36409843 epoch total loss 1.59724844\n",
      "Trained batch 819 batch loss 1.5956068 epoch total loss 1.59724641\n",
      "Trained batch 820 batch loss 1.49067879 epoch total loss 1.59711647\n",
      "Trained batch 821 batch loss 1.51084518 epoch total loss 1.59701145\n",
      "Trained batch 822 batch loss 1.49054289 epoch total loss 1.59688199\n",
      "Trained batch 823 batch loss 1.46252668 epoch total loss 1.59671867\n",
      "Trained batch 824 batch loss 1.38214242 epoch total loss 1.59645832\n",
      "Trained batch 825 batch loss 1.33490586 epoch total loss 1.59614134\n",
      "Trained batch 826 batch loss 1.62657 epoch total loss 1.59617829\n",
      "Trained batch 827 batch loss 1.50443649 epoch total loss 1.59606731\n",
      "Trained batch 828 batch loss 1.59274793 epoch total loss 1.59606326\n",
      "Trained batch 829 batch loss 1.56683373 epoch total loss 1.59602809\n",
      "Trained batch 830 batch loss 1.54951131 epoch total loss 1.59597218\n",
      "Trained batch 831 batch loss 1.484097 epoch total loss 1.59583759\n",
      "Trained batch 832 batch loss 1.49147403 epoch total loss 1.59571207\n",
      "Trained batch 833 batch loss 1.57238078 epoch total loss 1.59568405\n",
      "Trained batch 834 batch loss 1.53284633 epoch total loss 1.59560871\n",
      "Trained batch 835 batch loss 1.43585253 epoch total loss 1.5954175\n",
      "Trained batch 836 batch loss 1.55318606 epoch total loss 1.59536695\n",
      "Trained batch 837 batch loss 1.46592069 epoch total loss 1.59521234\n",
      "Trained batch 838 batch loss 1.39956915 epoch total loss 1.59497881\n",
      "Trained batch 839 batch loss 1.41439342 epoch total loss 1.59476364\n",
      "Trained batch 840 batch loss 1.3361063 epoch total loss 1.59445572\n",
      "Trained batch 841 batch loss 1.33016419 epoch total loss 1.59414148\n",
      "Trained batch 842 batch loss 1.49971342 epoch total loss 1.59402943\n",
      "Trained batch 843 batch loss 1.49009538 epoch total loss 1.59390604\n",
      "Trained batch 844 batch loss 1.56318426 epoch total loss 1.59386981\n",
      "Trained batch 845 batch loss 1.58500326 epoch total loss 1.5938592\n",
      "Trained batch 846 batch loss 1.65922105 epoch total loss 1.59393644\n",
      "Trained batch 847 batch loss 1.53167486 epoch total loss 1.59386289\n",
      "Trained batch 848 batch loss 1.54799676 epoch total loss 1.59380877\n",
      "Trained batch 849 batch loss 1.53852415 epoch total loss 1.59374368\n",
      "Trained batch 850 batch loss 1.46359825 epoch total loss 1.59359062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 851 batch loss 1.45887685 epoch total loss 1.59343231\n",
      "Trained batch 852 batch loss 1.47465134 epoch total loss 1.59329283\n",
      "Trained batch 853 batch loss 1.56788063 epoch total loss 1.59326303\n",
      "Trained batch 854 batch loss 1.54994464 epoch total loss 1.59321225\n",
      "Trained batch 855 batch loss 1.66683185 epoch total loss 1.59329844\n",
      "Trained batch 856 batch loss 1.70673752 epoch total loss 1.593431\n",
      "Trained batch 857 batch loss 1.56768572 epoch total loss 1.59340084\n",
      "Trained batch 858 batch loss 1.54915357 epoch total loss 1.59334934\n",
      "Trained batch 859 batch loss 1.50435 epoch total loss 1.59324574\n",
      "Trained batch 860 batch loss 1.42699826 epoch total loss 1.59305251\n",
      "Trained batch 861 batch loss 1.45453155 epoch total loss 1.59289169\n",
      "Trained batch 862 batch loss 1.64164472 epoch total loss 1.5929482\n",
      "Trained batch 863 batch loss 1.57809377 epoch total loss 1.59293103\n",
      "Trained batch 864 batch loss 1.53485203 epoch total loss 1.59286392\n",
      "Trained batch 865 batch loss 1.47042418 epoch total loss 1.5927223\n",
      "Trained batch 866 batch loss 1.46230602 epoch total loss 1.59257174\n",
      "Trained batch 867 batch loss 1.52959156 epoch total loss 1.59249902\n",
      "Trained batch 868 batch loss 1.50527835 epoch total loss 1.59239852\n",
      "Trained batch 869 batch loss 1.54351151 epoch total loss 1.59234214\n",
      "Trained batch 870 batch loss 1.52825558 epoch total loss 1.59226847\n",
      "Trained batch 871 batch loss 1.56555736 epoch total loss 1.59223783\n",
      "Trained batch 872 batch loss 1.54867053 epoch total loss 1.59218788\n",
      "Trained batch 873 batch loss 1.51399469 epoch total loss 1.59209836\n",
      "Trained batch 874 batch loss 1.49204147 epoch total loss 1.59198391\n",
      "Trained batch 875 batch loss 1.57319307 epoch total loss 1.59196246\n",
      "Trained batch 876 batch loss 1.54260635 epoch total loss 1.59190607\n",
      "Trained batch 877 batch loss 1.56315863 epoch total loss 1.59187329\n",
      "Trained batch 878 batch loss 1.52052855 epoch total loss 1.59179199\n",
      "Trained batch 879 batch loss 1.52113032 epoch total loss 1.59171164\n",
      "Trained batch 880 batch loss 1.50224614 epoch total loss 1.59160984\n",
      "Trained batch 881 batch loss 1.53827846 epoch total loss 1.5915494\n",
      "Trained batch 882 batch loss 1.56777656 epoch total loss 1.59152246\n",
      "Trained batch 883 batch loss 1.46443796 epoch total loss 1.59137857\n",
      "Trained batch 884 batch loss 1.55667102 epoch total loss 1.59133923\n",
      "Trained batch 885 batch loss 1.55820251 epoch total loss 1.5913018\n",
      "Trained batch 886 batch loss 1.41623664 epoch total loss 1.59110427\n",
      "Trained batch 887 batch loss 1.47344398 epoch total loss 1.59097159\n",
      "Trained batch 888 batch loss 1.34637344 epoch total loss 1.5906961\n",
      "Trained batch 889 batch loss 1.387941 epoch total loss 1.59046793\n",
      "Trained batch 890 batch loss 1.47006679 epoch total loss 1.59033275\n",
      "Trained batch 891 batch loss 1.39675498 epoch total loss 1.59011543\n",
      "Trained batch 892 batch loss 1.52250922 epoch total loss 1.59003961\n",
      "Trained batch 893 batch loss 1.54916751 epoch total loss 1.58999383\n",
      "Trained batch 894 batch loss 1.49856377 epoch total loss 1.58989155\n",
      "Trained batch 895 batch loss 1.43311167 epoch total loss 1.58971632\n",
      "Trained batch 896 batch loss 1.43666101 epoch total loss 1.58954549\n",
      "Trained batch 897 batch loss 1.51076961 epoch total loss 1.58945763\n",
      "Trained batch 898 batch loss 1.52358699 epoch total loss 1.58938432\n",
      "Trained batch 899 batch loss 1.53297114 epoch total loss 1.58932149\n",
      "Trained batch 900 batch loss 1.56583536 epoch total loss 1.58929539\n",
      "Trained batch 901 batch loss 1.31740451 epoch total loss 1.58899355\n",
      "Trained batch 902 batch loss 1.36282587 epoch total loss 1.58874285\n",
      "Trained batch 903 batch loss 1.53678882 epoch total loss 1.58868527\n",
      "Trained batch 904 batch loss 1.55843472 epoch total loss 1.58865178\n",
      "Trained batch 905 batch loss 1.5266428 epoch total loss 1.58858323\n",
      "Trained batch 906 batch loss 1.62500119 epoch total loss 1.5886234\n",
      "Trained batch 907 batch loss 1.55260158 epoch total loss 1.58858371\n",
      "Trained batch 908 batch loss 1.57131135 epoch total loss 1.58856475\n",
      "Trained batch 909 batch loss 1.57468653 epoch total loss 1.58854949\n",
      "Trained batch 910 batch loss 1.44972098 epoch total loss 1.58839691\n",
      "Trained batch 911 batch loss 1.40245175 epoch total loss 1.58819282\n",
      "Trained batch 912 batch loss 1.50615144 epoch total loss 1.58810282\n",
      "Trained batch 913 batch loss 1.44088721 epoch total loss 1.58794153\n",
      "Trained batch 914 batch loss 1.4610033 epoch total loss 1.58780277\n",
      "Trained batch 915 batch loss 1.43676472 epoch total loss 1.58763766\n",
      "Trained batch 916 batch loss 1.5461446 epoch total loss 1.58759236\n",
      "Trained batch 917 batch loss 1.71163058 epoch total loss 1.58772767\n",
      "Trained batch 918 batch loss 1.69960368 epoch total loss 1.5878495\n",
      "Trained batch 919 batch loss 1.46149993 epoch total loss 1.58771205\n",
      "Trained batch 920 batch loss 1.63562703 epoch total loss 1.58776414\n",
      "Trained batch 921 batch loss 1.502707 epoch total loss 1.58767176\n",
      "Trained batch 922 batch loss 1.56141853 epoch total loss 1.58764327\n",
      "Trained batch 923 batch loss 1.5290817 epoch total loss 1.58757985\n",
      "Trained batch 924 batch loss 1.49628675 epoch total loss 1.58748114\n",
      "Trained batch 925 batch loss 1.59636676 epoch total loss 1.58749068\n",
      "Trained batch 926 batch loss 1.44056845 epoch total loss 1.58733189\n",
      "Trained batch 927 batch loss 1.51812887 epoch total loss 1.58725739\n",
      "Trained batch 928 batch loss 1.60143375 epoch total loss 1.58727264\n",
      "Trained batch 929 batch loss 1.5256592 epoch total loss 1.58720624\n",
      "Trained batch 930 batch loss 1.45497894 epoch total loss 1.58706415\n",
      "Trained batch 931 batch loss 1.46669769 epoch total loss 1.5869348\n",
      "Trained batch 932 batch loss 1.41547608 epoch total loss 1.58675086\n",
      "Trained batch 933 batch loss 1.41642165 epoch total loss 1.58656824\n",
      "Trained batch 934 batch loss 1.48675191 epoch total loss 1.58646131\n",
      "Trained batch 935 batch loss 1.52400732 epoch total loss 1.58639455\n",
      "Trained batch 936 batch loss 1.45226479 epoch total loss 1.58625126\n",
      "Trained batch 937 batch loss 1.49476612 epoch total loss 1.58615363\n",
      "Trained batch 938 batch loss 1.54258561 epoch total loss 1.58610725\n",
      "Trained batch 939 batch loss 1.44651794 epoch total loss 1.5859586\n",
      "Trained batch 940 batch loss 1.52208066 epoch total loss 1.58589065\n",
      "Trained batch 941 batch loss 1.46863592 epoch total loss 1.58576596\n",
      "Trained batch 942 batch loss 1.50792468 epoch total loss 1.58568335\n",
      "Trained batch 943 batch loss 1.41161537 epoch total loss 1.58549881\n",
      "Trained batch 944 batch loss 1.51531029 epoch total loss 1.58542442\n",
      "Trained batch 945 batch loss 1.50284863 epoch total loss 1.58533692\n",
      "Trained batch 946 batch loss 1.38177931 epoch total loss 1.58512187\n",
      "Trained batch 947 batch loss 1.42961276 epoch total loss 1.5849576\n",
      "Trained batch 948 batch loss 1.52532589 epoch total loss 1.58489466\n",
      "Trained batch 949 batch loss 1.50009811 epoch total loss 1.58480525\n",
      "Trained batch 950 batch loss 1.51145601 epoch total loss 1.58472812\n",
      "Trained batch 951 batch loss 1.52284157 epoch total loss 1.58466303\n",
      "Trained batch 952 batch loss 1.55390716 epoch total loss 1.58463073\n",
      "Trained batch 953 batch loss 1.51634336 epoch total loss 1.58455908\n",
      "Trained batch 954 batch loss 1.48468304 epoch total loss 1.58445454\n",
      "Trained batch 955 batch loss 1.46691072 epoch total loss 1.58433139\n",
      "Trained batch 956 batch loss 1.48596334 epoch total loss 1.58422852\n",
      "Trained batch 957 batch loss 1.52481854 epoch total loss 1.58416641\n",
      "Trained batch 958 batch loss 1.53270423 epoch total loss 1.58411264\n",
      "Trained batch 959 batch loss 1.5398463 epoch total loss 1.58406651\n",
      "Trained batch 960 batch loss 1.61234355 epoch total loss 1.58409584\n",
      "Trained batch 961 batch loss 1.672948 epoch total loss 1.58418834\n",
      "Trained batch 962 batch loss 1.43078148 epoch total loss 1.58402896\n",
      "Trained batch 963 batch loss 1.40081215 epoch total loss 1.58383858\n",
      "Trained batch 964 batch loss 1.44473195 epoch total loss 1.58369422\n",
      "Trained batch 965 batch loss 1.48594892 epoch total loss 1.58359301\n",
      "Trained batch 966 batch loss 1.4452312 epoch total loss 1.58344972\n",
      "Trained batch 967 batch loss 1.39163709 epoch total loss 1.58325136\n",
      "Trained batch 968 batch loss 1.49650931 epoch total loss 1.58316171\n",
      "Trained batch 969 batch loss 1.47275341 epoch total loss 1.58304775\n",
      "Trained batch 970 batch loss 1.43671727 epoch total loss 1.58289695\n",
      "Trained batch 971 batch loss 1.41709745 epoch total loss 1.58272624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 972 batch loss 1.4744724 epoch total loss 1.5826149\n",
      "Trained batch 973 batch loss 1.52643597 epoch total loss 1.5825572\n",
      "Trained batch 974 batch loss 1.40648973 epoch total loss 1.58237636\n",
      "Trained batch 975 batch loss 1.3939718 epoch total loss 1.58218312\n",
      "Trained batch 976 batch loss 1.4850266 epoch total loss 1.58208358\n",
      "Trained batch 977 batch loss 1.60492897 epoch total loss 1.58210695\n",
      "Trained batch 978 batch loss 1.53528798 epoch total loss 1.58205903\n",
      "Trained batch 979 batch loss 1.55597103 epoch total loss 1.58203244\n",
      "Trained batch 980 batch loss 1.48383 epoch total loss 1.58193231\n",
      "Trained batch 981 batch loss 1.58589613 epoch total loss 1.58193648\n",
      "Trained batch 982 batch loss 1.51545906 epoch total loss 1.58186877\n",
      "Trained batch 983 batch loss 1.52773118 epoch total loss 1.58181369\n",
      "Trained batch 984 batch loss 1.5143851 epoch total loss 1.58174515\n",
      "Trained batch 985 batch loss 1.37873805 epoch total loss 1.58153915\n",
      "Trained batch 986 batch loss 1.30181491 epoch total loss 1.58125532\n",
      "Trained batch 987 batch loss 1.33133256 epoch total loss 1.58100212\n",
      "Trained batch 988 batch loss 1.36689425 epoch total loss 1.58078551\n",
      "Trained batch 989 batch loss 1.3350513 epoch total loss 1.58053708\n",
      "Trained batch 990 batch loss 1.25139463 epoch total loss 1.58020449\n",
      "Trained batch 991 batch loss 1.21439588 epoch total loss 1.5798353\n",
      "Trained batch 992 batch loss 1.18320632 epoch total loss 1.57943559\n",
      "Trained batch 993 batch loss 1.47156596 epoch total loss 1.57932687\n",
      "Trained batch 994 batch loss 1.59637034 epoch total loss 1.57934403\n",
      "Trained batch 995 batch loss 1.50942218 epoch total loss 1.5792737\n",
      "Trained batch 996 batch loss 1.4555856 epoch total loss 1.57914948\n",
      "Trained batch 997 batch loss 1.52942789 epoch total loss 1.57909966\n",
      "Trained batch 998 batch loss 1.48529172 epoch total loss 1.57900572\n",
      "Trained batch 999 batch loss 1.56112206 epoch total loss 1.57898784\n",
      "Trained batch 1000 batch loss 1.55163729 epoch total loss 1.57896042\n",
      "Trained batch 1001 batch loss 1.45558488 epoch total loss 1.57883716\n",
      "Trained batch 1002 batch loss 1.53797805 epoch total loss 1.57879639\n",
      "Trained batch 1003 batch loss 1.50532472 epoch total loss 1.57872319\n",
      "Trained batch 1004 batch loss 1.48305881 epoch total loss 1.57862782\n",
      "Trained batch 1005 batch loss 1.51705027 epoch total loss 1.57856667\n",
      "Trained batch 1006 batch loss 1.38795137 epoch total loss 1.57837713\n",
      "Trained batch 1007 batch loss 1.42384863 epoch total loss 1.57822371\n",
      "Trained batch 1008 batch loss 1.45512486 epoch total loss 1.57810152\n",
      "Trained batch 1009 batch loss 1.4648211 epoch total loss 1.57798922\n",
      "Trained batch 1010 batch loss 1.52630472 epoch total loss 1.57793808\n",
      "Trained batch 1011 batch loss 1.57477403 epoch total loss 1.57793498\n",
      "Trained batch 1012 batch loss 1.71114862 epoch total loss 1.57806659\n",
      "Trained batch 1013 batch loss 1.59167719 epoch total loss 1.57808\n",
      "Trained batch 1014 batch loss 1.56468976 epoch total loss 1.57806683\n",
      "Trained batch 1015 batch loss 1.46873713 epoch total loss 1.57795918\n",
      "Trained batch 1016 batch loss 1.29899967 epoch total loss 1.57768452\n",
      "Trained batch 1017 batch loss 1.28295553 epoch total loss 1.57739472\n",
      "Trained batch 1018 batch loss 1.28437018 epoch total loss 1.57710695\n",
      "Trained batch 1019 batch loss 1.25845385 epoch total loss 1.57679415\n",
      "Trained batch 1020 batch loss 1.42326939 epoch total loss 1.57664359\n",
      "Trained batch 1021 batch loss 1.46053016 epoch total loss 1.57653\n",
      "Trained batch 1022 batch loss 1.40059125 epoch total loss 1.57635784\n",
      "Trained batch 1023 batch loss 1.40520942 epoch total loss 1.57619047\n",
      "Trained batch 1024 batch loss 1.43526459 epoch total loss 1.5760529\n",
      "Trained batch 1025 batch loss 1.47564888 epoch total loss 1.57595503\n",
      "Trained batch 1026 batch loss 1.5181694 epoch total loss 1.57589865\n",
      "Trained batch 1027 batch loss 1.52094817 epoch total loss 1.57584524\n",
      "Trained batch 1028 batch loss 1.44068146 epoch total loss 1.57571375\n",
      "Trained batch 1029 batch loss 1.43304408 epoch total loss 1.57557499\n",
      "Trained batch 1030 batch loss 1.52365327 epoch total loss 1.57552469\n",
      "Trained batch 1031 batch loss 1.42278099 epoch total loss 1.57537651\n",
      "Trained batch 1032 batch loss 1.39973962 epoch total loss 1.57520628\n",
      "Trained batch 1033 batch loss 1.3915844 epoch total loss 1.57502854\n",
      "Trained batch 1034 batch loss 1.39697838 epoch total loss 1.5748564\n",
      "Trained batch 1035 batch loss 1.50197315 epoch total loss 1.57478595\n",
      "Trained batch 1036 batch loss 1.46890581 epoch total loss 1.57468367\n",
      "Trained batch 1037 batch loss 1.53202701 epoch total loss 1.57464254\n",
      "Trained batch 1038 batch loss 1.52087629 epoch total loss 1.57459068\n",
      "Trained batch 1039 batch loss 1.46677947 epoch total loss 1.57448697\n",
      "Trained batch 1040 batch loss 1.51348853 epoch total loss 1.57442832\n",
      "Trained batch 1041 batch loss 1.37730372 epoch total loss 1.5742389\n",
      "Trained batch 1042 batch loss 1.46539736 epoch total loss 1.57413447\n",
      "Trained batch 1043 batch loss 1.47419119 epoch total loss 1.57403874\n",
      "Trained batch 1044 batch loss 1.50302887 epoch total loss 1.57397079\n",
      "Trained batch 1045 batch loss 1.4827069 epoch total loss 1.57388341\n",
      "Trained batch 1046 batch loss 1.35385036 epoch total loss 1.57367301\n",
      "Trained batch 1047 batch loss 1.44547045 epoch total loss 1.57355058\n",
      "Trained batch 1048 batch loss 1.38425875 epoch total loss 1.57337\n",
      "Trained batch 1049 batch loss 1.52360034 epoch total loss 1.57332253\n",
      "Trained batch 1050 batch loss 1.45762599 epoch total loss 1.57321227\n",
      "Trained batch 1051 batch loss 1.38015532 epoch total loss 1.57302856\n",
      "Trained batch 1052 batch loss 1.41076875 epoch total loss 1.57287431\n",
      "Trained batch 1053 batch loss 1.52019906 epoch total loss 1.57282424\n",
      "Trained batch 1054 batch loss 1.34958267 epoch total loss 1.57261252\n",
      "Trained batch 1055 batch loss 1.35165 epoch total loss 1.57240307\n",
      "Trained batch 1056 batch loss 1.42374885 epoch total loss 1.57226229\n",
      "Trained batch 1057 batch loss 1.43728435 epoch total loss 1.57213449\n",
      "Trained batch 1058 batch loss 1.67765641 epoch total loss 1.57223427\n",
      "Trained batch 1059 batch loss 1.53782153 epoch total loss 1.57220173\n",
      "Trained batch 1060 batch loss 1.55751455 epoch total loss 1.5721879\n",
      "Trained batch 1061 batch loss 1.52795029 epoch total loss 1.57214618\n",
      "Trained batch 1062 batch loss 1.50925577 epoch total loss 1.57208705\n",
      "Trained batch 1063 batch loss 1.47432745 epoch total loss 1.57199502\n",
      "Trained batch 1064 batch loss 1.43846345 epoch total loss 1.57186961\n",
      "Trained batch 1065 batch loss 1.50523114 epoch total loss 1.57180703\n",
      "Trained batch 1066 batch loss 1.52633 epoch total loss 1.57176435\n",
      "Trained batch 1067 batch loss 1.46635866 epoch total loss 1.57166553\n",
      "Trained batch 1068 batch loss 1.28160322 epoch total loss 1.57139397\n",
      "Trained batch 1069 batch loss 1.31653059 epoch total loss 1.57115555\n",
      "Trained batch 1070 batch loss 1.4449029 epoch total loss 1.57103765\n",
      "Trained batch 1071 batch loss 1.49587739 epoch total loss 1.57096744\n",
      "Trained batch 1072 batch loss 1.55732417 epoch total loss 1.57095468\n",
      "Trained batch 1073 batch loss 1.50869727 epoch total loss 1.57089674\n",
      "Trained batch 1074 batch loss 1.61129582 epoch total loss 1.5709343\n",
      "Trained batch 1075 batch loss 1.53924966 epoch total loss 1.57090485\n",
      "Trained batch 1076 batch loss 1.54467332 epoch total loss 1.57088053\n",
      "Trained batch 1077 batch loss 1.54075789 epoch total loss 1.57085264\n",
      "Trained batch 1078 batch loss 1.52216482 epoch total loss 1.57080746\n",
      "Trained batch 1079 batch loss 1.50365376 epoch total loss 1.57074523\n",
      "Trained batch 1080 batch loss 1.45382547 epoch total loss 1.57063699\n",
      "Trained batch 1081 batch loss 1.50723791 epoch total loss 1.57057834\n",
      "Trained batch 1082 batch loss 1.50252497 epoch total loss 1.57051551\n",
      "Trained batch 1083 batch loss 1.51357532 epoch total loss 1.57046282\n",
      "Trained batch 1084 batch loss 1.52318978 epoch total loss 1.57041931\n",
      "Trained batch 1085 batch loss 1.49399924 epoch total loss 1.57034886\n",
      "Trained batch 1086 batch loss 1.52969062 epoch total loss 1.57031143\n",
      "Trained batch 1087 batch loss 1.33462119 epoch total loss 1.57009447\n",
      "Trained batch 1088 batch loss 1.35860658 epoch total loss 1.56990016\n",
      "Trained batch 1089 batch loss 1.42260456 epoch total loss 1.56976497\n",
      "Trained batch 1090 batch loss 1.42647052 epoch total loss 1.56963348\n",
      "Trained batch 1091 batch loss 1.40923 epoch total loss 1.56948638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1092 batch loss 1.31063676 epoch total loss 1.56924939\n",
      "Trained batch 1093 batch loss 1.42933393 epoch total loss 1.56912136\n",
      "Trained batch 1094 batch loss 1.47434878 epoch total loss 1.56903481\n",
      "Trained batch 1095 batch loss 1.46376777 epoch total loss 1.56893861\n",
      "Trained batch 1096 batch loss 1.42924309 epoch total loss 1.56881118\n",
      "Trained batch 1097 batch loss 1.54737425 epoch total loss 1.56879163\n",
      "Trained batch 1098 batch loss 1.37462544 epoch total loss 1.56861472\n",
      "Trained batch 1099 batch loss 1.44208801 epoch total loss 1.56849968\n",
      "Trained batch 1100 batch loss 1.35797727 epoch total loss 1.56830835\n",
      "Trained batch 1101 batch loss 1.5322262 epoch total loss 1.56827557\n",
      "Trained batch 1102 batch loss 1.58586764 epoch total loss 1.56829143\n",
      "Trained batch 1103 batch loss 1.53471649 epoch total loss 1.56826103\n",
      "Trained batch 1104 batch loss 1.40293157 epoch total loss 1.5681113\n",
      "Trained batch 1105 batch loss 1.29818225 epoch total loss 1.56786704\n",
      "Trained batch 1106 batch loss 1.41779697 epoch total loss 1.56773138\n",
      "Trained batch 1107 batch loss 1.42069888 epoch total loss 1.56759846\n",
      "Trained batch 1108 batch loss 1.54459977 epoch total loss 1.56757772\n",
      "Trained batch 1109 batch loss 1.42841482 epoch total loss 1.56745231\n",
      "Trained batch 1110 batch loss 1.43007052 epoch total loss 1.56732845\n",
      "Trained batch 1111 batch loss 1.45575142 epoch total loss 1.56722808\n",
      "Trained batch 1112 batch loss 1.28976262 epoch total loss 1.56697857\n",
      "Trained batch 1113 batch loss 1.38672662 epoch total loss 1.56681669\n",
      "Trained batch 1114 batch loss 1.32089233 epoch total loss 1.56659591\n",
      "Trained batch 1115 batch loss 1.39817214 epoch total loss 1.56644487\n",
      "Trained batch 1116 batch loss 1.41236794 epoch total loss 1.56630683\n",
      "Trained batch 1117 batch loss 1.43647242 epoch total loss 1.5661906\n",
      "Trained batch 1118 batch loss 1.40522099 epoch total loss 1.56604671\n",
      "Trained batch 1119 batch loss 1.36703241 epoch total loss 1.56586885\n",
      "Trained batch 1120 batch loss 1.5289185 epoch total loss 1.56583595\n",
      "Trained batch 1121 batch loss 1.42292762 epoch total loss 1.5657084\n",
      "Trained batch 1122 batch loss 1.47630036 epoch total loss 1.56562877\n",
      "Trained batch 1123 batch loss 1.49215198 epoch total loss 1.56556344\n",
      "Trained batch 1124 batch loss 1.40860188 epoch total loss 1.56542373\n",
      "Trained batch 1125 batch loss 1.40901744 epoch total loss 1.56528473\n",
      "Trained batch 1126 batch loss 1.5124675 epoch total loss 1.56523776\n",
      "Trained batch 1127 batch loss 1.5595535 epoch total loss 1.56523275\n",
      "Trained batch 1128 batch loss 1.50428116 epoch total loss 1.56517875\n",
      "Trained batch 1129 batch loss 1.47257745 epoch total loss 1.56509662\n",
      "Trained batch 1130 batch loss 1.35406935 epoch total loss 1.56490993\n",
      "Trained batch 1131 batch loss 1.4327023 epoch total loss 1.56479311\n",
      "Trained batch 1132 batch loss 1.46339977 epoch total loss 1.56470346\n",
      "Trained batch 1133 batch loss 1.26576698 epoch total loss 1.56443965\n",
      "Trained batch 1134 batch loss 1.47432804 epoch total loss 1.56436026\n",
      "Trained batch 1135 batch loss 1.30307364 epoch total loss 1.56413007\n",
      "Trained batch 1136 batch loss 1.44927216 epoch total loss 1.56402886\n",
      "Trained batch 1137 batch loss 1.38317764 epoch total loss 1.56386983\n",
      "Trained batch 1138 batch loss 1.34252429 epoch total loss 1.56367528\n",
      "Trained batch 1139 batch loss 1.28797 epoch total loss 1.56343329\n",
      "Trained batch 1140 batch loss 1.26771379 epoch total loss 1.56317389\n",
      "Trained batch 1141 batch loss 1.32246053 epoch total loss 1.56296289\n",
      "Trained batch 1142 batch loss 1.41826046 epoch total loss 1.56283617\n",
      "Trained batch 1143 batch loss 1.42234612 epoch total loss 1.56271327\n",
      "Trained batch 1144 batch loss 1.42034638 epoch total loss 1.56258881\n",
      "Trained batch 1145 batch loss 1.45255494 epoch total loss 1.56249261\n",
      "Trained batch 1146 batch loss 1.52835619 epoch total loss 1.56246281\n",
      "Trained batch 1147 batch loss 1.47483253 epoch total loss 1.56238639\n",
      "Trained batch 1148 batch loss 1.51476634 epoch total loss 1.56234491\n",
      "Trained batch 1149 batch loss 1.46300018 epoch total loss 1.56225848\n",
      "Trained batch 1150 batch loss 1.48596156 epoch total loss 1.5621922\n",
      "Trained batch 1151 batch loss 1.45865989 epoch total loss 1.5621022\n",
      "Trained batch 1152 batch loss 1.29468679 epoch total loss 1.5618701\n",
      "Trained batch 1153 batch loss 1.27475667 epoch total loss 1.56162107\n",
      "Trained batch 1154 batch loss 1.25152636 epoch total loss 1.56135237\n",
      "Trained batch 1155 batch loss 1.45038915 epoch total loss 1.56125641\n",
      "Trained batch 1156 batch loss 1.41962183 epoch total loss 1.56113386\n",
      "Trained batch 1157 batch loss 1.48073471 epoch total loss 1.56106436\n",
      "Trained batch 1158 batch loss 1.54535484 epoch total loss 1.56105089\n",
      "Trained batch 1159 batch loss 1.47212434 epoch total loss 1.56097412\n",
      "Trained batch 1160 batch loss 1.48832178 epoch total loss 1.56091154\n",
      "Trained batch 1161 batch loss 1.55479097 epoch total loss 1.56090629\n",
      "Trained batch 1162 batch loss 1.54844499 epoch total loss 1.56089556\n",
      "Trained batch 1163 batch loss 1.60452151 epoch total loss 1.56093299\n",
      "Trained batch 1164 batch loss 1.44848704 epoch total loss 1.56083643\n",
      "Trained batch 1165 batch loss 1.56861842 epoch total loss 1.56084311\n",
      "Trained batch 1166 batch loss 1.63377762 epoch total loss 1.56090569\n",
      "Trained batch 1167 batch loss 1.53976011 epoch total loss 1.56088758\n",
      "Trained batch 1168 batch loss 1.54890871 epoch total loss 1.56087732\n",
      "Trained batch 1169 batch loss 1.42269206 epoch total loss 1.56075919\n",
      "Trained batch 1170 batch loss 1.38100541 epoch total loss 1.56060553\n",
      "Trained batch 1171 batch loss 1.32603252 epoch total loss 1.56040525\n",
      "Trained batch 1172 batch loss 1.40040982 epoch total loss 1.56026864\n",
      "Trained batch 1173 batch loss 1.36000717 epoch total loss 1.56009793\n",
      "Trained batch 1174 batch loss 1.37715876 epoch total loss 1.55994213\n",
      "Trained batch 1175 batch loss 1.3710053 epoch total loss 1.55978131\n",
      "Trained batch 1176 batch loss 1.4929291 epoch total loss 1.55972445\n",
      "Trained batch 1177 batch loss 1.45059061 epoch total loss 1.55963171\n",
      "Trained batch 1178 batch loss 1.52524674 epoch total loss 1.5596025\n",
      "Trained batch 1179 batch loss 1.52232111 epoch total loss 1.55957091\n",
      "Trained batch 1180 batch loss 1.37027931 epoch total loss 1.55941045\n",
      "Trained batch 1181 batch loss 1.57600534 epoch total loss 1.55942452\n",
      "Trained batch 1182 batch loss 1.51015902 epoch total loss 1.55938292\n",
      "Trained batch 1183 batch loss 1.55049336 epoch total loss 1.55937541\n",
      "Trained batch 1184 batch loss 1.4690578 epoch total loss 1.55929911\n",
      "Trained batch 1185 batch loss 1.57564855 epoch total loss 1.55931294\n",
      "Trained batch 1186 batch loss 1.52839923 epoch total loss 1.55928695\n",
      "Trained batch 1187 batch loss 1.47664714 epoch total loss 1.55921733\n",
      "Trained batch 1188 batch loss 1.44340622 epoch total loss 1.55911982\n",
      "Trained batch 1189 batch loss 1.59645867 epoch total loss 1.55915117\n",
      "Trained batch 1190 batch loss 1.56543911 epoch total loss 1.55915654\n",
      "Trained batch 1191 batch loss 1.55332744 epoch total loss 1.55915165\n",
      "Trained batch 1192 batch loss 1.41734076 epoch total loss 1.55903268\n",
      "Trained batch 1193 batch loss 1.60346413 epoch total loss 1.55907\n",
      "Trained batch 1194 batch loss 1.55931425 epoch total loss 1.55907023\n",
      "Trained batch 1195 batch loss 1.40742922 epoch total loss 1.55894327\n",
      "Trained batch 1196 batch loss 1.26856339 epoch total loss 1.55870056\n",
      "Trained batch 1197 batch loss 1.31641507 epoch total loss 1.55849814\n",
      "Trained batch 1198 batch loss 1.4525584 epoch total loss 1.55840957\n",
      "Trained batch 1199 batch loss 1.53050303 epoch total loss 1.55838633\n",
      "Trained batch 1200 batch loss 1.43648386 epoch total loss 1.55828476\n",
      "Trained batch 1201 batch loss 1.42984879 epoch total loss 1.55817783\n",
      "Trained batch 1202 batch loss 1.40604365 epoch total loss 1.55805123\n",
      "Trained batch 1203 batch loss 1.46257722 epoch total loss 1.55797184\n",
      "Trained batch 1204 batch loss 1.38966477 epoch total loss 1.557832\n",
      "Trained batch 1205 batch loss 1.43850541 epoch total loss 1.55773294\n",
      "Trained batch 1206 batch loss 1.3962208 epoch total loss 1.55759907\n",
      "Trained batch 1207 batch loss 1.43471146 epoch total loss 1.55749726\n",
      "Trained batch 1208 batch loss 1.3886292 epoch total loss 1.55735743\n",
      "Trained batch 1209 batch loss 1.43880248 epoch total loss 1.55725944\n",
      "Trained batch 1210 batch loss 1.42735863 epoch total loss 1.55715215\n",
      "Trained batch 1211 batch loss 1.46274626 epoch total loss 1.55707419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1212 batch loss 1.49689949 epoch total loss 1.5570246\n",
      "Trained batch 1213 batch loss 1.44048762 epoch total loss 1.5569284\n",
      "Trained batch 1214 batch loss 1.41957271 epoch total loss 1.55681527\n",
      "Trained batch 1215 batch loss 1.34276438 epoch total loss 1.55663908\n",
      "Trained batch 1216 batch loss 1.27134693 epoch total loss 1.55640447\n",
      "Trained batch 1217 batch loss 1.47452855 epoch total loss 1.55633724\n",
      "Trained batch 1218 batch loss 1.47553539 epoch total loss 1.55627096\n",
      "Trained batch 1219 batch loss 1.4088136 epoch total loss 1.55615\n",
      "Trained batch 1220 batch loss 1.40491927 epoch total loss 1.55602598\n",
      "Trained batch 1221 batch loss 1.40827215 epoch total loss 1.55590498\n",
      "Trained batch 1222 batch loss 1.38110542 epoch total loss 1.55576193\n",
      "Trained batch 1223 batch loss 1.42261326 epoch total loss 1.5556531\n",
      "Trained batch 1224 batch loss 1.49651933 epoch total loss 1.5556047\n",
      "Trained batch 1225 batch loss 1.39686954 epoch total loss 1.55547512\n",
      "Trained batch 1226 batch loss 1.36811399 epoch total loss 1.55532229\n",
      "Trained batch 1227 batch loss 1.30962944 epoch total loss 1.55512202\n",
      "Trained batch 1228 batch loss 1.40022969 epoch total loss 1.55499601\n",
      "Trained batch 1229 batch loss 1.52209675 epoch total loss 1.55496919\n",
      "Trained batch 1230 batch loss 1.45457041 epoch total loss 1.55488753\n",
      "Trained batch 1231 batch loss 1.50087965 epoch total loss 1.55484366\n",
      "Trained batch 1232 batch loss 1.34093 epoch total loss 1.5546701\n",
      "Trained batch 1233 batch loss 1.35812259 epoch total loss 1.55451071\n",
      "Trained batch 1234 batch loss 1.32025361 epoch total loss 1.55432093\n",
      "Trained batch 1235 batch loss 1.3368119 epoch total loss 1.55414474\n",
      "Trained batch 1236 batch loss 1.32038462 epoch total loss 1.55395567\n",
      "Trained batch 1237 batch loss 1.18425393 epoch total loss 1.55365682\n",
      "Trained batch 1238 batch loss 1.1579138 epoch total loss 1.5533371\n",
      "Trained batch 1239 batch loss 1.27957678 epoch total loss 1.5531162\n",
      "Trained batch 1240 batch loss 1.4402895 epoch total loss 1.55302513\n",
      "Trained batch 1241 batch loss 1.72090852 epoch total loss 1.55316055\n",
      "Trained batch 1242 batch loss 1.57815838 epoch total loss 1.55318058\n",
      "Trained batch 1243 batch loss 1.58007097 epoch total loss 1.55320227\n",
      "Trained batch 1244 batch loss 1.40158892 epoch total loss 1.55308032\n",
      "Trained batch 1245 batch loss 1.5324893 epoch total loss 1.55306387\n",
      "Trained batch 1246 batch loss 1.54910147 epoch total loss 1.55306065\n",
      "Trained batch 1247 batch loss 1.43082952 epoch total loss 1.55296254\n",
      "Trained batch 1248 batch loss 1.51950216 epoch total loss 1.55293572\n",
      "Trained batch 1249 batch loss 1.45773482 epoch total loss 1.55285954\n",
      "Trained batch 1250 batch loss 1.52262938 epoch total loss 1.55283535\n",
      "Trained batch 1251 batch loss 1.42949772 epoch total loss 1.55273676\n",
      "Trained batch 1252 batch loss 1.47333717 epoch total loss 1.55267334\n",
      "Trained batch 1253 batch loss 1.36882746 epoch total loss 1.55252659\n",
      "Trained batch 1254 batch loss 1.48506546 epoch total loss 1.55247283\n",
      "Trained batch 1255 batch loss 1.46345401 epoch total loss 1.5524019\n",
      "Trained batch 1256 batch loss 1.51001 epoch total loss 1.55236816\n",
      "Trained batch 1257 batch loss 1.28804159 epoch total loss 1.55215788\n",
      "Trained batch 1258 batch loss 1.20051455 epoch total loss 1.55187845\n",
      "Trained batch 1259 batch loss 1.22830963 epoch total loss 1.55162144\n",
      "Trained batch 1260 batch loss 1.32498467 epoch total loss 1.55144155\n",
      "Trained batch 1261 batch loss 1.30957162 epoch total loss 1.55124974\n",
      "Trained batch 1262 batch loss 1.35131586 epoch total loss 1.55109131\n",
      "Trained batch 1263 batch loss 1.44502568 epoch total loss 1.55100727\n",
      "Trained batch 1264 batch loss 1.3754077 epoch total loss 1.55086839\n",
      "Trained batch 1265 batch loss 1.40297973 epoch total loss 1.55075145\n",
      "Trained batch 1266 batch loss 1.49121988 epoch total loss 1.55070436\n",
      "Trained batch 1267 batch loss 1.38308811 epoch total loss 1.55057204\n",
      "Trained batch 1268 batch loss 1.53908396 epoch total loss 1.55056298\n",
      "Trained batch 1269 batch loss 1.3565321 epoch total loss 1.55041015\n",
      "Trained batch 1270 batch loss 1.33808208 epoch total loss 1.55024302\n",
      "Trained batch 1271 batch loss 1.3475908 epoch total loss 1.55008352\n",
      "Trained batch 1272 batch loss 1.42219567 epoch total loss 1.54998302\n",
      "Trained batch 1273 batch loss 1.36231494 epoch total loss 1.54983556\n",
      "Trained batch 1274 batch loss 1.56834245 epoch total loss 1.54985011\n",
      "Trained batch 1275 batch loss 1.57752609 epoch total loss 1.5498718\n",
      "Trained batch 1276 batch loss 1.58648503 epoch total loss 1.54990041\n",
      "Trained batch 1277 batch loss 1.39852846 epoch total loss 1.54978192\n",
      "Trained batch 1278 batch loss 1.45818734 epoch total loss 1.54971027\n",
      "Trained batch 1279 batch loss 1.35010552 epoch total loss 1.54955411\n",
      "Trained batch 1280 batch loss 1.33833921 epoch total loss 1.54938912\n",
      "Trained batch 1281 batch loss 1.40419912 epoch total loss 1.54927576\n",
      "Trained batch 1282 batch loss 1.46295094 epoch total loss 1.5492084\n",
      "Trained batch 1283 batch loss 1.34435058 epoch total loss 1.54904878\n",
      "Trained batch 1284 batch loss 1.3823607 epoch total loss 1.54891896\n",
      "Trained batch 1285 batch loss 1.42938113 epoch total loss 1.54882586\n",
      "Trained batch 1286 batch loss 1.4028461 epoch total loss 1.54871237\n",
      "Trained batch 1287 batch loss 1.28604746 epoch total loss 1.54850817\n",
      "Trained batch 1288 batch loss 1.2781024 epoch total loss 1.54829824\n",
      "Trained batch 1289 batch loss 1.10822558 epoch total loss 1.54795682\n",
      "Trained batch 1290 batch loss 1.21331167 epoch total loss 1.54769742\n",
      "Trained batch 1291 batch loss 1.44370425 epoch total loss 1.54761684\n",
      "Trained batch 1292 batch loss 1.52158284 epoch total loss 1.54759669\n",
      "Trained batch 1293 batch loss 1.48955071 epoch total loss 1.54755187\n",
      "Trained batch 1294 batch loss 1.40005279 epoch total loss 1.54743779\n",
      "Trained batch 1295 batch loss 1.53687286 epoch total loss 1.54742968\n",
      "Trained batch 1296 batch loss 1.37564814 epoch total loss 1.54729712\n",
      "Trained batch 1297 batch loss 1.34131658 epoch total loss 1.54713821\n",
      "Trained batch 1298 batch loss 1.30358708 epoch total loss 1.54695058\n",
      "Trained batch 1299 batch loss 1.4397428 epoch total loss 1.54686809\n",
      "Trained batch 1300 batch loss 1.41583252 epoch total loss 1.54676723\n",
      "Trained batch 1301 batch loss 1.35236287 epoch total loss 1.54661787\n",
      "Trained batch 1302 batch loss 1.36736631 epoch total loss 1.54648006\n",
      "Trained batch 1303 batch loss 1.39576709 epoch total loss 1.54636443\n",
      "Trained batch 1304 batch loss 1.43481922 epoch total loss 1.54627883\n",
      "Trained batch 1305 batch loss 1.41149592 epoch total loss 1.5461756\n",
      "Trained batch 1306 batch loss 1.49372816 epoch total loss 1.54613543\n",
      "Trained batch 1307 batch loss 1.36193216 epoch total loss 1.54599452\n",
      "Trained batch 1308 batch loss 1.37089801 epoch total loss 1.54586065\n",
      "Trained batch 1309 batch loss 1.39131618 epoch total loss 1.54574263\n",
      "Trained batch 1310 batch loss 1.37394261 epoch total loss 1.54561138\n",
      "Trained batch 1311 batch loss 1.41922081 epoch total loss 1.54551494\n",
      "Trained batch 1312 batch loss 1.39522457 epoch total loss 1.5454005\n",
      "Trained batch 1313 batch loss 1.37971568 epoch total loss 1.54527438\n",
      "Trained batch 1314 batch loss 1.46360707 epoch total loss 1.54521215\n",
      "Trained batch 1315 batch loss 1.43170547 epoch total loss 1.54512596\n",
      "Trained batch 1316 batch loss 1.59298229 epoch total loss 1.54516232\n",
      "Trained batch 1317 batch loss 1.54616594 epoch total loss 1.54516304\n",
      "Trained batch 1318 batch loss 1.5336864 epoch total loss 1.54515433\n",
      "Trained batch 1319 batch loss 1.67087913 epoch total loss 1.5452497\n",
      "Trained batch 1320 batch loss 1.54655313 epoch total loss 1.54525065\n",
      "Trained batch 1321 batch loss 1.5853647 epoch total loss 1.54528093\n",
      "Trained batch 1322 batch loss 1.34071922 epoch total loss 1.5451262\n",
      "Trained batch 1323 batch loss 1.46925044 epoch total loss 1.54506886\n",
      "Trained batch 1324 batch loss 1.56563711 epoch total loss 1.54508448\n",
      "Trained batch 1325 batch loss 1.50208092 epoch total loss 1.54505193\n",
      "Trained batch 1326 batch loss 1.51044416 epoch total loss 1.54502594\n",
      "Trained batch 1327 batch loss 1.53006971 epoch total loss 1.54501462\n",
      "Trained batch 1328 batch loss 1.49137092 epoch total loss 1.54497433\n",
      "Trained batch 1329 batch loss 1.49683726 epoch total loss 1.54493809\n",
      "Trained batch 1330 batch loss 1.52786636 epoch total loss 1.54492521\n",
      "Trained batch 1331 batch loss 1.49511933 epoch total loss 1.54488778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1332 batch loss 1.5011853 epoch total loss 1.544855\n",
      "Trained batch 1333 batch loss 1.4527607 epoch total loss 1.54478598\n",
      "Trained batch 1334 batch loss 1.45546484 epoch total loss 1.5447191\n",
      "Trained batch 1335 batch loss 1.44886911 epoch total loss 1.54464734\n",
      "Trained batch 1336 batch loss 1.42879736 epoch total loss 1.54456055\n",
      "Trained batch 1337 batch loss 1.41954947 epoch total loss 1.54446697\n",
      "Trained batch 1338 batch loss 1.42666519 epoch total loss 1.544379\n",
      "Trained batch 1339 batch loss 1.34654725 epoch total loss 1.54423118\n",
      "Trained batch 1340 batch loss 1.5276444 epoch total loss 1.54421878\n",
      "Trained batch 1341 batch loss 1.45814157 epoch total loss 1.54415464\n",
      "Trained batch 1342 batch loss 1.48042607 epoch total loss 1.5441072\n",
      "Trained batch 1343 batch loss 1.45551252 epoch total loss 1.54404128\n",
      "Trained batch 1344 batch loss 1.41298211 epoch total loss 1.54394388\n",
      "Trained batch 1345 batch loss 1.43751192 epoch total loss 1.54386473\n",
      "Trained batch 1346 batch loss 1.4837265 epoch total loss 1.54382\n",
      "Trained batch 1347 batch loss 1.46161914 epoch total loss 1.54375899\n",
      "Trained batch 1348 batch loss 1.38943791 epoch total loss 1.54364443\n",
      "Trained batch 1349 batch loss 1.25517511 epoch total loss 1.54343057\n",
      "Trained batch 1350 batch loss 1.37462032 epoch total loss 1.54330552\n",
      "Trained batch 1351 batch loss 1.40358436 epoch total loss 1.54320204\n",
      "Trained batch 1352 batch loss 1.41607678 epoch total loss 1.54310799\n",
      "Trained batch 1353 batch loss 1.44648111 epoch total loss 1.54303658\n",
      "Trained batch 1354 batch loss 1.49712825 epoch total loss 1.54300261\n",
      "Trained batch 1355 batch loss 1.4490273 epoch total loss 1.54293323\n",
      "Trained batch 1356 batch loss 1.50018418 epoch total loss 1.54290175\n",
      "Trained batch 1357 batch loss 1.50445712 epoch total loss 1.54287338\n",
      "Trained batch 1358 batch loss 1.47352469 epoch total loss 1.54282236\n",
      "Trained batch 1359 batch loss 1.37849474 epoch total loss 1.54270148\n",
      "Trained batch 1360 batch loss 1.4297502 epoch total loss 1.54261839\n",
      "Trained batch 1361 batch loss 1.45493448 epoch total loss 1.54255378\n",
      "Trained batch 1362 batch loss 1.37096751 epoch total loss 1.54242778\n",
      "Trained batch 1363 batch loss 1.30313492 epoch total loss 1.5422523\n",
      "Trained batch 1364 batch loss 1.41147792 epoch total loss 1.54215634\n",
      "Trained batch 1365 batch loss 1.36450648 epoch total loss 1.54202616\n",
      "Trained batch 1366 batch loss 1.48283577 epoch total loss 1.54198289\n",
      "Trained batch 1367 batch loss 1.63779 epoch total loss 1.54205287\n",
      "Trained batch 1368 batch loss 1.47951102 epoch total loss 1.54200721\n",
      "Trained batch 1369 batch loss 1.51465678 epoch total loss 1.54198718\n",
      "Trained batch 1370 batch loss 1.49822879 epoch total loss 1.54195535\n",
      "Trained batch 1371 batch loss 1.50317216 epoch total loss 1.54192698\n",
      "Trained batch 1372 batch loss 1.50354815 epoch total loss 1.54189909\n",
      "Trained batch 1373 batch loss 1.42195094 epoch total loss 1.5418117\n",
      "Trained batch 1374 batch loss 1.37898278 epoch total loss 1.54169309\n",
      "Trained batch 1375 batch loss 1.46011639 epoch total loss 1.54163384\n",
      "Trained batch 1376 batch loss 1.3226912 epoch total loss 1.54147482\n",
      "Trained batch 1377 batch loss 1.49176121 epoch total loss 1.5414387\n",
      "Trained batch 1378 batch loss 1.53160965 epoch total loss 1.54143143\n",
      "Trained batch 1379 batch loss 1.52138114 epoch total loss 1.541417\n",
      "Trained batch 1380 batch loss 1.46326053 epoch total loss 1.54136038\n",
      "Trained batch 1381 batch loss 1.50994694 epoch total loss 1.54133773\n",
      "Trained batch 1382 batch loss 1.47138369 epoch total loss 1.54128718\n",
      "Trained batch 1383 batch loss 1.4147892 epoch total loss 1.54119563\n",
      "Trained batch 1384 batch loss 1.32863641 epoch total loss 1.54104209\n",
      "Trained batch 1385 batch loss 1.39055789 epoch total loss 1.54093349\n",
      "Trained batch 1386 batch loss 1.42057049 epoch total loss 1.54084671\n",
      "Trained batch 1387 batch loss 1.44707489 epoch total loss 1.54077899\n",
      "Trained batch 1388 batch loss 1.37409246 epoch total loss 1.54065895\n",
      "Epoch 1 train loss 1.540658950805664\n",
      "Validated batch 1 batch loss 1.40742671\n",
      "Validated batch 2 batch loss 1.41384399\n",
      "Validated batch 3 batch loss 1.32810068\n",
      "Validated batch 4 batch loss 1.51075339\n",
      "Validated batch 5 batch loss 1.41119814\n",
      "Validated batch 6 batch loss 1.45184672\n",
      "Validated batch 7 batch loss 1.49925113\n",
      "Validated batch 8 batch loss 1.47305489\n",
      "Validated batch 9 batch loss 1.45939386\n",
      "Validated batch 10 batch loss 1.45520782\n",
      "Validated batch 11 batch loss 1.50401092\n",
      "Validated batch 12 batch loss 1.42571425\n",
      "Validated batch 13 batch loss 1.41492677\n",
      "Validated batch 14 batch loss 1.49514437\n",
      "Validated batch 15 batch loss 1.46172738\n",
      "Validated batch 16 batch loss 1.42580819\n",
      "Validated batch 17 batch loss 1.52535295\n",
      "Validated batch 18 batch loss 1.27344143\n",
      "Validated batch 19 batch loss 1.43306828\n",
      "Validated batch 20 batch loss 1.2856884\n",
      "Validated batch 21 batch loss 1.4439975\n",
      "Validated batch 22 batch loss 1.51728058\n",
      "Validated batch 23 batch loss 1.33920693\n",
      "Validated batch 24 batch loss 1.39590228\n",
      "Validated batch 25 batch loss 1.35154223\n",
      "Validated batch 26 batch loss 1.37140357\n",
      "Validated batch 27 batch loss 1.37321651\n",
      "Validated batch 28 batch loss 1.38772273\n",
      "Validated batch 29 batch loss 1.41974545\n",
      "Validated batch 30 batch loss 1.44235349\n",
      "Validated batch 31 batch loss 1.39152384\n",
      "Validated batch 32 batch loss 1.40441096\n",
      "Validated batch 33 batch loss 1.46499145\n",
      "Validated batch 34 batch loss 1.49961567\n",
      "Validated batch 35 batch loss 1.45358789\n",
      "Validated batch 36 batch loss 1.37911379\n",
      "Validated batch 37 batch loss 1.37700057\n",
      "Validated batch 38 batch loss 1.42936945\n",
      "Validated batch 39 batch loss 1.41640091\n",
      "Validated batch 40 batch loss 1.47629428\n",
      "Validated batch 41 batch loss 1.45511985\n",
      "Validated batch 42 batch loss 1.36083531\n",
      "Validated batch 43 batch loss 1.47419488\n",
      "Validated batch 44 batch loss 1.35102129\n",
      "Validated batch 45 batch loss 1.37597382\n",
      "Validated batch 46 batch loss 1.4677372\n",
      "Validated batch 47 batch loss 1.47586668\n",
      "Validated batch 48 batch loss 1.3947866\n",
      "Validated batch 49 batch loss 1.37906814\n",
      "Validated batch 50 batch loss 1.3391521\n",
      "Validated batch 51 batch loss 1.40057814\n",
      "Validated batch 52 batch loss 1.51399899\n",
      "Validated batch 53 batch loss 1.3442663\n",
      "Validated batch 54 batch loss 1.44364429\n",
      "Validated batch 55 batch loss 1.43814921\n",
      "Validated batch 56 batch loss 1.40363562\n",
      "Validated batch 57 batch loss 1.40984201\n",
      "Validated batch 58 batch loss 1.29469943\n",
      "Validated batch 59 batch loss 1.57064116\n",
      "Validated batch 60 batch loss 1.40950167\n",
      "Validated batch 61 batch loss 1.43467009\n",
      "Validated batch 62 batch loss 1.38089108\n",
      "Validated batch 63 batch loss 1.45496249\n",
      "Validated batch 64 batch loss 1.28840876\n",
      "Validated batch 65 batch loss 1.37892306\n",
      "Validated batch 66 batch loss 1.36396623\n",
      "Validated batch 67 batch loss 1.35579836\n",
      "Validated batch 68 batch loss 1.41970897\n",
      "Validated batch 69 batch loss 1.44554341\n",
      "Validated batch 70 batch loss 1.39079607\n",
      "Validated batch 71 batch loss 1.4126991\n",
      "Validated batch 72 batch loss 1.35662615\n",
      "Validated batch 73 batch loss 1.26302981\n",
      "Validated batch 74 batch loss 1.35998273\n",
      "Validated batch 75 batch loss 1.49822521\n",
      "Validated batch 76 batch loss 1.34145486\n",
      "Validated batch 77 batch loss 1.34025884\n",
      "Validated batch 78 batch loss 1.37485683\n",
      "Validated batch 79 batch loss 1.41907847\n",
      "Validated batch 80 batch loss 1.35301113\n",
      "Validated batch 81 batch loss 1.4100728\n",
      "Validated batch 82 batch loss 1.4372015\n",
      "Validated batch 83 batch loss 1.37254167\n",
      "Validated batch 84 batch loss 1.42872441\n",
      "Validated batch 85 batch loss 1.5167917\n",
      "Validated batch 86 batch loss 1.37901533\n",
      "Validated batch 87 batch loss 1.4904567\n",
      "Validated batch 88 batch loss 1.32931554\n",
      "Validated batch 89 batch loss 1.34374774\n",
      "Validated batch 90 batch loss 1.39270329\n",
      "Validated batch 91 batch loss 1.43024373\n",
      "Validated batch 92 batch loss 1.54718542\n",
      "Validated batch 93 batch loss 1.46795511\n",
      "Validated batch 94 batch loss 1.52079916\n",
      "Validated batch 95 batch loss 1.3579874\n",
      "Validated batch 96 batch loss 1.35200882\n",
      "Validated batch 97 batch loss 1.43407035\n",
      "Validated batch 98 batch loss 1.37602019\n",
      "Validated batch 99 batch loss 1.33402979\n",
      "Validated batch 100 batch loss 1.41913569\n",
      "Validated batch 101 batch loss 1.31306148\n",
      "Validated batch 102 batch loss 1.49187565\n",
      "Validated batch 103 batch loss 1.36403084\n",
      "Validated batch 104 batch loss 1.34117663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 105 batch loss 1.36888027\n",
      "Validated batch 106 batch loss 1.50058985\n",
      "Validated batch 107 batch loss 1.43170547\n",
      "Validated batch 108 batch loss 1.48508251\n",
      "Validated batch 109 batch loss 1.34236729\n",
      "Validated batch 110 batch loss 1.50559556\n",
      "Validated batch 111 batch loss 1.43501651\n",
      "Validated batch 112 batch loss 1.45817399\n",
      "Validated batch 113 batch loss 1.46613777\n",
      "Validated batch 114 batch loss 1.2128818\n",
      "Validated batch 115 batch loss 1.46011448\n",
      "Validated batch 116 batch loss 1.51779044\n",
      "Validated batch 117 batch loss 1.41243505\n",
      "Validated batch 118 batch loss 1.42282748\n",
      "Validated batch 119 batch loss 1.41245508\n",
      "Validated batch 120 batch loss 1.38077927\n",
      "Validated batch 121 batch loss 1.45012259\n",
      "Validated batch 122 batch loss 1.41778946\n",
      "Validated batch 123 batch loss 1.35342574\n",
      "Validated batch 124 batch loss 1.37858236\n",
      "Validated batch 125 batch loss 1.39877141\n",
      "Validated batch 126 batch loss 1.43808961\n",
      "Validated batch 127 batch loss 1.44467199\n",
      "Validated batch 128 batch loss 1.40489054\n",
      "Validated batch 129 batch loss 1.34292912\n",
      "Validated batch 130 batch loss 1.36352527\n",
      "Validated batch 131 batch loss 1.41965532\n",
      "Validated batch 132 batch loss 1.41668797\n",
      "Validated batch 133 batch loss 1.439008\n",
      "Validated batch 134 batch loss 1.51343405\n",
      "Validated batch 135 batch loss 1.65924931\n",
      "Validated batch 136 batch loss 1.55329394\n",
      "Validated batch 137 batch loss 1.41401708\n",
      "Validated batch 138 batch loss 1.33794653\n",
      "Validated batch 139 batch loss 1.29327202\n",
      "Validated batch 140 batch loss 1.38002205\n",
      "Validated batch 141 batch loss 1.38038707\n",
      "Validated batch 142 batch loss 1.37881541\n",
      "Validated batch 143 batch loss 1.3579793\n",
      "Validated batch 144 batch loss 1.48346424\n",
      "Validated batch 145 batch loss 1.4275161\n",
      "Validated batch 146 batch loss 1.46750259\n",
      "Validated batch 147 batch loss 1.5010711\n",
      "Validated batch 148 batch loss 1.48452973\n",
      "Validated batch 149 batch loss 1.42516208\n",
      "Validated batch 150 batch loss 1.46585608\n",
      "Validated batch 151 batch loss 1.43153059\n",
      "Validated batch 152 batch loss 1.45140529\n",
      "Validated batch 153 batch loss 1.49758792\n",
      "Validated batch 154 batch loss 1.51821709\n",
      "Validated batch 155 batch loss 1.42509711\n",
      "Validated batch 156 batch loss 1.31576073\n",
      "Validated batch 157 batch loss 1.38232517\n",
      "Validated batch 158 batch loss 1.42681813\n",
      "Validated batch 159 batch loss 1.48983574\n",
      "Validated batch 160 batch loss 1.44356441\n",
      "Validated batch 161 batch loss 1.40896893\n",
      "Validated batch 162 batch loss 1.36170435\n",
      "Validated batch 163 batch loss 1.4211607\n",
      "Validated batch 164 batch loss 1.48158181\n",
      "Validated batch 165 batch loss 1.41788971\n",
      "Validated batch 166 batch loss 1.39093685\n",
      "Validated batch 167 batch loss 1.55413008\n",
      "Validated batch 168 batch loss 1.31504989\n",
      "Validated batch 169 batch loss 1.45181465\n",
      "Validated batch 170 batch loss 1.4021771\n",
      "Validated batch 171 batch loss 1.41746712\n",
      "Validated batch 172 batch loss 1.45216334\n",
      "Validated batch 173 batch loss 1.39581859\n",
      "Validated batch 174 batch loss 1.19612145\n",
      "Validated batch 175 batch loss 1.42502236\n",
      "Validated batch 176 batch loss 1.41653728\n",
      "Validated batch 177 batch loss 1.39070404\n",
      "Validated batch 178 batch loss 1.43444479\n",
      "Validated batch 179 batch loss 1.34183788\n",
      "Validated batch 180 batch loss 1.47258\n",
      "Validated batch 181 batch loss 1.48776078\n",
      "Validated batch 182 batch loss 1.42534542\n",
      "Validated batch 183 batch loss 1.44068074\n",
      "Validated batch 184 batch loss 1.36688209\n",
      "Validated batch 185 batch loss 1.39966238\n",
      "Epoch 1 val loss 1.416380524635315\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-1-loss-1.4164.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.41095686 epoch total loss 1.41095686\n",
      "Trained batch 2 batch loss 1.35173154 epoch total loss 1.3813442\n",
      "Trained batch 3 batch loss 1.30382669 epoch total loss 1.35550499\n",
      "Trained batch 4 batch loss 1.45808125 epoch total loss 1.38114905\n",
      "Trained batch 5 batch loss 1.48068953 epoch total loss 1.40105712\n",
      "Trained batch 6 batch loss 1.48223877 epoch total loss 1.41458738\n",
      "Trained batch 7 batch loss 1.48389661 epoch total loss 1.42448866\n",
      "Trained batch 8 batch loss 1.40277886 epoch total loss 1.42177486\n",
      "Trained batch 9 batch loss 1.4673202 epoch total loss 1.42683554\n",
      "Trained batch 10 batch loss 1.52470446 epoch total loss 1.43662238\n",
      "Trained batch 11 batch loss 1.50917244 epoch total loss 1.44321787\n",
      "Trained batch 12 batch loss 1.41723633 epoch total loss 1.44105279\n",
      "Trained batch 13 batch loss 1.37807119 epoch total loss 1.43620801\n",
      "Trained batch 14 batch loss 1.44518256 epoch total loss 1.436849\n",
      "Trained batch 15 batch loss 1.34223819 epoch total loss 1.43054163\n",
      "Trained batch 16 batch loss 1.33450627 epoch total loss 1.42453945\n",
      "Trained batch 17 batch loss 1.47561502 epoch total loss 1.42754388\n",
      "Trained batch 18 batch loss 1.46292543 epoch total loss 1.42950952\n",
      "Trained batch 19 batch loss 1.43547416 epoch total loss 1.4298234\n",
      "Trained batch 20 batch loss 1.36231565 epoch total loss 1.42644811\n",
      "Trained batch 21 batch loss 1.33383203 epoch total loss 1.42203772\n",
      "Trained batch 22 batch loss 1.4398613 epoch total loss 1.42284787\n",
      "Trained batch 23 batch loss 1.38290656 epoch total loss 1.42111135\n",
      "Trained batch 24 batch loss 1.3945142 epoch total loss 1.42000329\n",
      "Trained batch 25 batch loss 1.3170383 epoch total loss 1.41588473\n",
      "Trained batch 26 batch loss 1.41212964 epoch total loss 1.41574025\n",
      "Trained batch 27 batch loss 1.36804235 epoch total loss 1.41397369\n",
      "Trained batch 28 batch loss 1.46675503 epoch total loss 1.41585863\n",
      "Trained batch 29 batch loss 1.47586036 epoch total loss 1.41792774\n",
      "Trained batch 30 batch loss 1.4250598 epoch total loss 1.41816545\n",
      "Trained batch 31 batch loss 1.3858695 epoch total loss 1.41712356\n",
      "Trained batch 32 batch loss 1.38281763 epoch total loss 1.41605151\n",
      "Trained batch 33 batch loss 1.34044766 epoch total loss 1.41376042\n",
      "Trained batch 34 batch loss 1.41719198 epoch total loss 1.41386127\n",
      "Trained batch 35 batch loss 1.34029424 epoch total loss 1.41175938\n",
      "Trained batch 36 batch loss 1.35659122 epoch total loss 1.41022694\n",
      "Trained batch 37 batch loss 1.40732658 epoch total loss 1.4101485\n",
      "Trained batch 38 batch loss 1.47001696 epoch total loss 1.41172397\n",
      "Trained batch 39 batch loss 1.42330527 epoch total loss 1.41202092\n",
      "Trained batch 40 batch loss 1.45633674 epoch total loss 1.41312885\n",
      "Trained batch 41 batch loss 1.52917826 epoch total loss 1.41595936\n",
      "Trained batch 42 batch loss 1.41164327 epoch total loss 1.4158566\n",
      "Trained batch 43 batch loss 1.35300636 epoch total loss 1.41439497\n",
      "Trained batch 44 batch loss 1.35173571 epoch total loss 1.4129709\n",
      "Trained batch 45 batch loss 1.38509524 epoch total loss 1.41235137\n",
      "Trained batch 46 batch loss 1.36356235 epoch total loss 1.41129076\n",
      "Trained batch 47 batch loss 1.35794449 epoch total loss 1.41015565\n",
      "Trained batch 48 batch loss 1.43864501 epoch total loss 1.41074908\n",
      "Trained batch 49 batch loss 1.27957165 epoch total loss 1.40807199\n",
      "Trained batch 50 batch loss 1.43954825 epoch total loss 1.40870142\n",
      "Trained batch 51 batch loss 1.43555105 epoch total loss 1.40922797\n",
      "Trained batch 52 batch loss 1.43934178 epoch total loss 1.40980709\n",
      "Trained batch 53 batch loss 1.40003192 epoch total loss 1.40962267\n",
      "Trained batch 54 batch loss 1.31267798 epoch total loss 1.40782726\n",
      "Trained batch 55 batch loss 1.30016196 epoch total loss 1.40586972\n",
      "Trained batch 56 batch loss 1.30850101 epoch total loss 1.40413105\n",
      "Trained batch 57 batch loss 1.26582813 epoch total loss 1.40170479\n",
      "Trained batch 58 batch loss 1.35204971 epoch total loss 1.40084863\n",
      "Trained batch 59 batch loss 1.27815807 epoch total loss 1.39876914\n",
      "Trained batch 60 batch loss 1.35269535 epoch total loss 1.39800119\n",
      "Trained batch 61 batch loss 1.36751533 epoch total loss 1.39750147\n",
      "Trained batch 62 batch loss 1.48018718 epoch total loss 1.39883506\n",
      "Trained batch 63 batch loss 1.44383872 epoch total loss 1.39954948\n",
      "Trained batch 64 batch loss 1.51196027 epoch total loss 1.40130591\n",
      "Trained batch 65 batch loss 1.33920836 epoch total loss 1.40035057\n",
      "Trained batch 66 batch loss 1.36233521 epoch total loss 1.39977455\n",
      "Trained batch 67 batch loss 1.35963559 epoch total loss 1.39917552\n",
      "Trained batch 68 batch loss 1.37375116 epoch total loss 1.39880157\n",
      "Trained batch 69 batch loss 1.44969845 epoch total loss 1.39953923\n",
      "Trained batch 70 batch loss 1.47985649 epoch total loss 1.40068662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 71 batch loss 1.40509725 epoch total loss 1.40074873\n",
      "Trained batch 72 batch loss 1.44636667 epoch total loss 1.40138233\n",
      "Trained batch 73 batch loss 1.39324617 epoch total loss 1.40127099\n",
      "Trained batch 74 batch loss 1.25644553 epoch total loss 1.39931381\n",
      "Trained batch 75 batch loss 1.33418608 epoch total loss 1.39844549\n",
      "Trained batch 76 batch loss 1.38361132 epoch total loss 1.39825022\n",
      "Trained batch 77 batch loss 1.40804398 epoch total loss 1.39837742\n",
      "Trained batch 78 batch loss 1.42569339 epoch total loss 1.39872766\n",
      "Trained batch 79 batch loss 1.45394349 epoch total loss 1.39942658\n",
      "Trained batch 80 batch loss 1.42488754 epoch total loss 1.39974475\n",
      "Trained batch 81 batch loss 1.41606283 epoch total loss 1.39994621\n",
      "Trained batch 82 batch loss 1.45191169 epoch total loss 1.40057993\n",
      "Trained batch 83 batch loss 1.4343729 epoch total loss 1.40098715\n",
      "Trained batch 84 batch loss 1.39351189 epoch total loss 1.4008981\n",
      "Trained batch 85 batch loss 1.36356831 epoch total loss 1.40045893\n",
      "Trained batch 86 batch loss 1.44486201 epoch total loss 1.40097523\n",
      "Trained batch 87 batch loss 1.41677535 epoch total loss 1.4011569\n",
      "Trained batch 88 batch loss 1.41916847 epoch total loss 1.40136158\n",
      "Trained batch 89 batch loss 1.31917942 epoch total loss 1.40043807\n",
      "Trained batch 90 batch loss 1.38422012 epoch total loss 1.40025783\n",
      "Trained batch 91 batch loss 1.32774878 epoch total loss 1.39946115\n",
      "Trained batch 92 batch loss 1.41236329 epoch total loss 1.39960134\n",
      "Trained batch 93 batch loss 1.38830566 epoch total loss 1.39947987\n",
      "Trained batch 94 batch loss 1.55199027 epoch total loss 1.4011023\n",
      "Trained batch 95 batch loss 1.42800796 epoch total loss 1.40138555\n",
      "Trained batch 96 batch loss 1.40189743 epoch total loss 1.40139091\n",
      "Trained batch 97 batch loss 1.41488779 epoch total loss 1.40153\n",
      "Trained batch 98 batch loss 1.38554883 epoch total loss 1.40136683\n",
      "Trained batch 99 batch loss 1.32418466 epoch total loss 1.40058732\n",
      "Trained batch 100 batch loss 1.27736688 epoch total loss 1.39935517\n",
      "Trained batch 101 batch loss 1.33062124 epoch total loss 1.39867473\n",
      "Trained batch 102 batch loss 1.47432089 epoch total loss 1.39941633\n",
      "Trained batch 103 batch loss 1.43244934 epoch total loss 1.399737\n",
      "Trained batch 104 batch loss 1.40737069 epoch total loss 1.39981031\n",
      "Trained batch 105 batch loss 1.3682909 epoch total loss 1.39951015\n",
      "Trained batch 106 batch loss 1.28656173 epoch total loss 1.39844453\n",
      "Trained batch 107 batch loss 1.36429775 epoch total loss 1.39812553\n",
      "Trained batch 108 batch loss 1.27968884 epoch total loss 1.39702892\n",
      "Trained batch 109 batch loss 1.34889436 epoch total loss 1.39658725\n",
      "Trained batch 110 batch loss 1.44715631 epoch total loss 1.39704704\n",
      "Trained batch 111 batch loss 1.4252317 epoch total loss 1.39730096\n",
      "Trained batch 112 batch loss 1.27520227 epoch total loss 1.39621079\n",
      "Trained batch 113 batch loss 1.41579 epoch total loss 1.396384\n",
      "Trained batch 114 batch loss 1.39077353 epoch total loss 1.39633489\n",
      "Trained batch 115 batch loss 1.24701846 epoch total loss 1.39503646\n",
      "Trained batch 116 batch loss 1.37416101 epoch total loss 1.39485657\n",
      "Trained batch 117 batch loss 1.46776319 epoch total loss 1.39547968\n",
      "Trained batch 118 batch loss 1.42506564 epoch total loss 1.39573038\n",
      "Trained batch 119 batch loss 1.43486059 epoch total loss 1.39605916\n",
      "Trained batch 120 batch loss 1.38185692 epoch total loss 1.39594078\n",
      "Trained batch 121 batch loss 1.41654551 epoch total loss 1.39611113\n",
      "Trained batch 122 batch loss 1.3624686 epoch total loss 1.3958354\n",
      "Trained batch 123 batch loss 1.32603705 epoch total loss 1.39526784\n",
      "Trained batch 124 batch loss 1.27988529 epoch total loss 1.39433742\n",
      "Trained batch 125 batch loss 1.48348379 epoch total loss 1.39505064\n",
      "Trained batch 126 batch loss 1.40749609 epoch total loss 1.39514947\n",
      "Trained batch 127 batch loss 1.42216742 epoch total loss 1.39536214\n",
      "Trained batch 128 batch loss 1.36924613 epoch total loss 1.39515817\n",
      "Trained batch 129 batch loss 1.34967268 epoch total loss 1.39480555\n",
      "Trained batch 130 batch loss 1.4263823 epoch total loss 1.39504838\n",
      "Trained batch 131 batch loss 1.4219327 epoch total loss 1.39525366\n",
      "Trained batch 132 batch loss 1.43663049 epoch total loss 1.39556706\n",
      "Trained batch 133 batch loss 1.4036 epoch total loss 1.3956275\n",
      "Trained batch 134 batch loss 1.36222482 epoch total loss 1.39537823\n",
      "Trained batch 135 batch loss 1.44023848 epoch total loss 1.39571047\n",
      "Trained batch 136 batch loss 1.5692482 epoch total loss 1.39698648\n",
      "Trained batch 137 batch loss 1.41640484 epoch total loss 1.39712822\n",
      "Trained batch 138 batch loss 1.46519494 epoch total loss 1.39762151\n",
      "Trained batch 139 batch loss 1.54028511 epoch total loss 1.39864779\n",
      "Trained batch 140 batch loss 1.41779399 epoch total loss 1.39878464\n",
      "Trained batch 141 batch loss 1.48991632 epoch total loss 1.39943099\n",
      "Trained batch 142 batch loss 1.3135047 epoch total loss 1.39882588\n",
      "Trained batch 143 batch loss 1.31445467 epoch total loss 1.3982358\n",
      "Trained batch 144 batch loss 1.42839634 epoch total loss 1.39844525\n",
      "Trained batch 145 batch loss 1.41472757 epoch total loss 1.39855754\n",
      "Trained batch 146 batch loss 1.3775568 epoch total loss 1.39841378\n",
      "Trained batch 147 batch loss 1.48811173 epoch total loss 1.39902401\n",
      "Trained batch 148 batch loss 1.30493665 epoch total loss 1.39838827\n",
      "Trained batch 149 batch loss 1.3435185 epoch total loss 1.39802\n",
      "Trained batch 150 batch loss 1.37003255 epoch total loss 1.39783335\n",
      "Trained batch 151 batch loss 1.28152061 epoch total loss 1.39706314\n",
      "Trained batch 152 batch loss 1.37044752 epoch total loss 1.39688802\n",
      "Trained batch 153 batch loss 1.48873258 epoch total loss 1.39748836\n",
      "Trained batch 154 batch loss 1.38347423 epoch total loss 1.39739728\n",
      "Trained batch 155 batch loss 1.39912689 epoch total loss 1.39740849\n",
      "Trained batch 156 batch loss 1.30347717 epoch total loss 1.39680636\n",
      "Trained batch 157 batch loss 1.32704234 epoch total loss 1.39636207\n",
      "Trained batch 158 batch loss 1.33890247 epoch total loss 1.39599836\n",
      "Trained batch 159 batch loss 1.24664569 epoch total loss 1.39505899\n",
      "Trained batch 160 batch loss 1.41466856 epoch total loss 1.39518154\n",
      "Trained batch 161 batch loss 1.30175877 epoch total loss 1.39460135\n",
      "Trained batch 162 batch loss 1.38629842 epoch total loss 1.39455\n",
      "Trained batch 163 batch loss 1.32917535 epoch total loss 1.39414895\n",
      "Trained batch 164 batch loss 1.35046875 epoch total loss 1.39388251\n",
      "Trained batch 165 batch loss 1.36693323 epoch total loss 1.3937192\n",
      "Trained batch 166 batch loss 1.44861 epoch total loss 1.39404988\n",
      "Trained batch 167 batch loss 1.39725113 epoch total loss 1.39406896\n",
      "Trained batch 168 batch loss 1.5549649 epoch total loss 1.39502668\n",
      "Trained batch 169 batch loss 1.50101614 epoch total loss 1.39565384\n",
      "Trained batch 170 batch loss 1.41385245 epoch total loss 1.39576089\n",
      "Trained batch 171 batch loss 1.42178547 epoch total loss 1.39591312\n",
      "Trained batch 172 batch loss 1.45546818 epoch total loss 1.39625943\n",
      "Trained batch 173 batch loss 1.36268425 epoch total loss 1.39606535\n",
      "Trained batch 174 batch loss 1.532552 epoch total loss 1.39684975\n",
      "Trained batch 175 batch loss 1.40580034 epoch total loss 1.39690089\n",
      "Trained batch 176 batch loss 1.34821045 epoch total loss 1.39662421\n",
      "Trained batch 177 batch loss 1.49757481 epoch total loss 1.3971945\n",
      "Trained batch 178 batch loss 1.34428942 epoch total loss 1.39689732\n",
      "Trained batch 179 batch loss 1.37682855 epoch total loss 1.39678526\n",
      "Trained batch 180 batch loss 1.395751 epoch total loss 1.39677942\n",
      "Trained batch 181 batch loss 1.40006959 epoch total loss 1.39679766\n",
      "Trained batch 182 batch loss 1.33625317 epoch total loss 1.39646506\n",
      "Trained batch 183 batch loss 1.4142822 epoch total loss 1.39656234\n",
      "Trained batch 184 batch loss 1.34625912 epoch total loss 1.39628899\n",
      "Trained batch 185 batch loss 1.40684056 epoch total loss 1.39634597\n",
      "Trained batch 186 batch loss 1.41287661 epoch total loss 1.39643478\n",
      "Trained batch 187 batch loss 1.34785831 epoch total loss 1.39617515\n",
      "Trained batch 188 batch loss 1.41432202 epoch total loss 1.39627171\n",
      "Trained batch 189 batch loss 1.56085324 epoch total loss 1.39714253\n",
      "Trained batch 190 batch loss 1.36457717 epoch total loss 1.39697111\n",
      "Trained batch 191 batch loss 1.44681501 epoch total loss 1.39723194\n",
      "Trained batch 192 batch loss 1.37538075 epoch total loss 1.39711809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 193 batch loss 1.29265332 epoch total loss 1.39657688\n",
      "Trained batch 194 batch loss 1.36432302 epoch total loss 1.39641058\n",
      "Trained batch 195 batch loss 1.47742808 epoch total loss 1.39682603\n",
      "Trained batch 196 batch loss 1.60384345 epoch total loss 1.39788222\n",
      "Trained batch 197 batch loss 1.51761949 epoch total loss 1.39849007\n",
      "Trained batch 198 batch loss 1.54555523 epoch total loss 1.39923286\n",
      "Trained batch 199 batch loss 1.46893358 epoch total loss 1.3995831\n",
      "Trained batch 200 batch loss 1.489465 epoch total loss 1.40003252\n",
      "Trained batch 201 batch loss 1.40188015 epoch total loss 1.4000417\n",
      "Trained batch 202 batch loss 1.35559928 epoch total loss 1.39982164\n",
      "Trained batch 203 batch loss 1.29731059 epoch total loss 1.39931667\n",
      "Trained batch 204 batch loss 1.29032397 epoch total loss 1.39878237\n",
      "Trained batch 205 batch loss 1.42433274 epoch total loss 1.39890707\n",
      "Trained batch 206 batch loss 1.38299966 epoch total loss 1.39882982\n",
      "Trained batch 207 batch loss 1.53142405 epoch total loss 1.39947033\n",
      "Trained batch 208 batch loss 1.47897959 epoch total loss 1.39985263\n",
      "Trained batch 209 batch loss 1.46454096 epoch total loss 1.4001621\n",
      "Trained batch 210 batch loss 1.47235763 epoch total loss 1.4005059\n",
      "Trained batch 211 batch loss 1.45464206 epoch total loss 1.40076244\n",
      "Trained batch 212 batch loss 1.31421661 epoch total loss 1.40035415\n",
      "Trained batch 213 batch loss 1.3670311 epoch total loss 1.40019774\n",
      "Trained batch 214 batch loss 1.34805119 epoch total loss 1.39995408\n",
      "Trained batch 215 batch loss 1.39486694 epoch total loss 1.39993048\n",
      "Trained batch 216 batch loss 1.36793816 epoch total loss 1.39978242\n",
      "Trained batch 217 batch loss 1.35493541 epoch total loss 1.39957583\n",
      "Trained batch 218 batch loss 1.46112144 epoch total loss 1.39985812\n",
      "Trained batch 219 batch loss 1.41766775 epoch total loss 1.39993942\n",
      "Trained batch 220 batch loss 1.29478478 epoch total loss 1.39946151\n",
      "Trained batch 221 batch loss 1.28469813 epoch total loss 1.39894223\n",
      "Trained batch 222 batch loss 1.3417958 epoch total loss 1.39868486\n",
      "Trained batch 223 batch loss 1.34250665 epoch total loss 1.39843285\n",
      "Trained batch 224 batch loss 1.37151146 epoch total loss 1.39831269\n",
      "Trained batch 225 batch loss 1.37168813 epoch total loss 1.39819431\n",
      "Trained batch 226 batch loss 1.47702217 epoch total loss 1.39854312\n",
      "Trained batch 227 batch loss 1.28609633 epoch total loss 1.3980478\n",
      "Trained batch 228 batch loss 1.42062271 epoch total loss 1.39814675\n",
      "Trained batch 229 batch loss 1.42047286 epoch total loss 1.39824426\n",
      "Trained batch 230 batch loss 1.56625056 epoch total loss 1.39897478\n",
      "Trained batch 231 batch loss 1.52808917 epoch total loss 1.39953363\n",
      "Trained batch 232 batch loss 1.65247464 epoch total loss 1.4006238\n",
      "Trained batch 233 batch loss 1.39977288 epoch total loss 1.40062022\n",
      "Trained batch 234 batch loss 1.29706573 epoch total loss 1.4001776\n",
      "Trained batch 235 batch loss 1.32286167 epoch total loss 1.3998487\n",
      "Trained batch 236 batch loss 1.56389952 epoch total loss 1.40054381\n",
      "Trained batch 237 batch loss 1.55979764 epoch total loss 1.40121579\n",
      "Trained batch 238 batch loss 1.50046539 epoch total loss 1.40163279\n",
      "Trained batch 239 batch loss 1.43095708 epoch total loss 1.40175545\n",
      "Trained batch 240 batch loss 1.43738019 epoch total loss 1.40190387\n",
      "Trained batch 241 batch loss 1.48081386 epoch total loss 1.40223134\n",
      "Trained batch 242 batch loss 1.55649257 epoch total loss 1.40286875\n",
      "Trained batch 243 batch loss 1.72796929 epoch total loss 1.40420663\n",
      "Trained batch 244 batch loss 1.6355114 epoch total loss 1.40515447\n",
      "Trained batch 245 batch loss 1.48132312 epoch total loss 1.40546536\n",
      "Trained batch 246 batch loss 1.53363073 epoch total loss 1.40598643\n",
      "Trained batch 247 batch loss 1.40641 epoch total loss 1.4059881\n",
      "Trained batch 248 batch loss 1.3302691 epoch total loss 1.40568268\n",
      "Trained batch 249 batch loss 1.40482163 epoch total loss 1.40567923\n",
      "Trained batch 250 batch loss 1.49202681 epoch total loss 1.40602469\n",
      "Trained batch 251 batch loss 1.50551856 epoch total loss 1.40642107\n",
      "Trained batch 252 batch loss 1.3348217 epoch total loss 1.40613687\n",
      "Trained batch 253 batch loss 1.35994029 epoch total loss 1.40595436\n",
      "Trained batch 254 batch loss 1.40081668 epoch total loss 1.4059341\n",
      "Trained batch 255 batch loss 1.36822689 epoch total loss 1.40578628\n",
      "Trained batch 256 batch loss 1.32766497 epoch total loss 1.4054811\n",
      "Trained batch 257 batch loss 1.28834 epoch total loss 1.40502524\n",
      "Trained batch 258 batch loss 1.34567678 epoch total loss 1.40479517\n",
      "Trained batch 259 batch loss 1.36114144 epoch total loss 1.40462673\n",
      "Trained batch 260 batch loss 1.42476153 epoch total loss 1.40470421\n",
      "Trained batch 261 batch loss 1.39633191 epoch total loss 1.40467215\n",
      "Trained batch 262 batch loss 1.46821117 epoch total loss 1.40491462\n",
      "Trained batch 263 batch loss 1.36770916 epoch total loss 1.40477312\n",
      "Trained batch 264 batch loss 1.43076587 epoch total loss 1.40487146\n",
      "Trained batch 265 batch loss 1.51296771 epoch total loss 1.4052794\n",
      "Trained batch 266 batch loss 1.42005253 epoch total loss 1.40533495\n",
      "Trained batch 267 batch loss 1.3899765 epoch total loss 1.40527749\n",
      "Trained batch 268 batch loss 1.36318946 epoch total loss 1.40512037\n",
      "Trained batch 269 batch loss 1.3992852 epoch total loss 1.40509868\n",
      "Trained batch 270 batch loss 1.27639627 epoch total loss 1.40462208\n",
      "Trained batch 271 batch loss 1.36908412 epoch total loss 1.40449095\n",
      "Trained batch 272 batch loss 1.37463331 epoch total loss 1.40438116\n",
      "Trained batch 273 batch loss 1.40746129 epoch total loss 1.40439248\n",
      "Trained batch 274 batch loss 1.58193564 epoch total loss 1.40504038\n",
      "Trained batch 275 batch loss 1.62474406 epoch total loss 1.40583944\n",
      "Trained batch 276 batch loss 1.5674634 epoch total loss 1.406425\n",
      "Trained batch 277 batch loss 1.52309537 epoch total loss 1.40684628\n",
      "Trained batch 278 batch loss 1.32598805 epoch total loss 1.40655541\n",
      "Trained batch 279 batch loss 1.21494722 epoch total loss 1.40586853\n",
      "Trained batch 280 batch loss 1.21478963 epoch total loss 1.40518618\n",
      "Trained batch 281 batch loss 1.1568706 epoch total loss 1.40430236\n",
      "Trained batch 282 batch loss 1.31400084 epoch total loss 1.40398216\n",
      "Trained batch 283 batch loss 1.32185042 epoch total loss 1.40369189\n",
      "Trained batch 284 batch loss 1.41886103 epoch total loss 1.40374529\n",
      "Trained batch 285 batch loss 1.41733909 epoch total loss 1.40379298\n",
      "Trained batch 286 batch loss 1.37486792 epoch total loss 1.40369189\n",
      "Trained batch 287 batch loss 1.25820303 epoch total loss 1.40318501\n",
      "Trained batch 288 batch loss 1.23260021 epoch total loss 1.40259266\n",
      "Trained batch 289 batch loss 1.31013513 epoch total loss 1.40227282\n",
      "Trained batch 290 batch loss 1.30235851 epoch total loss 1.40192831\n",
      "Trained batch 291 batch loss 1.22975779 epoch total loss 1.40133667\n",
      "Trained batch 292 batch loss 1.35807431 epoch total loss 1.40118849\n",
      "Trained batch 293 batch loss 1.33360028 epoch total loss 1.4009577\n",
      "Trained batch 294 batch loss 1.28492105 epoch total loss 1.400563\n",
      "Trained batch 295 batch loss 1.39709759 epoch total loss 1.40055132\n",
      "Trained batch 296 batch loss 1.53524232 epoch total loss 1.40100634\n",
      "Trained batch 297 batch loss 1.39244008 epoch total loss 1.40097749\n",
      "Trained batch 298 batch loss 1.36225426 epoch total loss 1.40084743\n",
      "Trained batch 299 batch loss 1.33351421 epoch total loss 1.40062237\n",
      "Trained batch 300 batch loss 1.34324408 epoch total loss 1.40043104\n",
      "Trained batch 301 batch loss 1.38207698 epoch total loss 1.40037\n",
      "Trained batch 302 batch loss 1.43426013 epoch total loss 1.4004823\n",
      "Trained batch 303 batch loss 1.49812758 epoch total loss 1.40080452\n",
      "Trained batch 304 batch loss 1.35225964 epoch total loss 1.4006449\n",
      "Trained batch 305 batch loss 1.28258848 epoch total loss 1.40025783\n",
      "Trained batch 306 batch loss 1.46001208 epoch total loss 1.40045321\n",
      "Trained batch 307 batch loss 1.48903072 epoch total loss 1.4007417\n",
      "Trained batch 308 batch loss 1.44928443 epoch total loss 1.40089929\n",
      "Trained batch 309 batch loss 1.52070129 epoch total loss 1.40128696\n",
      "Trained batch 310 batch loss 1.5261302 epoch total loss 1.40168965\n",
      "Trained batch 311 batch loss 1.3999908 epoch total loss 1.40168428\n",
      "Trained batch 312 batch loss 1.22974968 epoch total loss 1.40113318\n",
      "Trained batch 313 batch loss 1.3688252 epoch total loss 1.40103\n",
      "Trained batch 314 batch loss 1.44759679 epoch total loss 1.40117824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 315 batch loss 1.38628054 epoch total loss 1.40113103\n",
      "Trained batch 316 batch loss 1.40213 epoch total loss 1.40113413\n",
      "Trained batch 317 batch loss 1.48263478 epoch total loss 1.40139127\n",
      "Trained batch 318 batch loss 1.42050457 epoch total loss 1.40145135\n",
      "Trained batch 319 batch loss 1.55876648 epoch total loss 1.40194452\n",
      "Trained batch 320 batch loss 1.46748137 epoch total loss 1.40214932\n",
      "Trained batch 321 batch loss 1.5472753 epoch total loss 1.40260136\n",
      "Trained batch 322 batch loss 1.53888679 epoch total loss 1.40302467\n",
      "Trained batch 323 batch loss 1.43904 epoch total loss 1.40313601\n",
      "Trained batch 324 batch loss 1.53059125 epoch total loss 1.40352941\n",
      "Trained batch 325 batch loss 1.46786523 epoch total loss 1.40372741\n",
      "Trained batch 326 batch loss 1.52996206 epoch total loss 1.4041146\n",
      "Trained batch 327 batch loss 1.42180681 epoch total loss 1.40416873\n",
      "Trained batch 328 batch loss 1.39210677 epoch total loss 1.40413201\n",
      "Trained batch 329 batch loss 1.41718674 epoch total loss 1.40417171\n",
      "Trained batch 330 batch loss 1.40272355 epoch total loss 1.40416718\n",
      "Trained batch 331 batch loss 1.49715877 epoch total loss 1.40444815\n",
      "Trained batch 332 batch loss 1.40143788 epoch total loss 1.40443909\n",
      "Trained batch 333 batch loss 1.35662293 epoch total loss 1.40429544\n",
      "Trained batch 334 batch loss 1.51099896 epoch total loss 1.40461493\n",
      "Trained batch 335 batch loss 1.55500031 epoch total loss 1.40506387\n",
      "Trained batch 336 batch loss 1.46717048 epoch total loss 1.40524864\n",
      "Trained batch 337 batch loss 1.42712116 epoch total loss 1.40531361\n",
      "Trained batch 338 batch loss 1.48747194 epoch total loss 1.40555656\n",
      "Trained batch 339 batch loss 1.34946561 epoch total loss 1.4053911\n",
      "Trained batch 340 batch loss 1.34210992 epoch total loss 1.40520501\n",
      "Trained batch 341 batch loss 1.40096116 epoch total loss 1.40519249\n",
      "Trained batch 342 batch loss 1.35766387 epoch total loss 1.40505362\n",
      "Trained batch 343 batch loss 1.39671922 epoch total loss 1.4050293\n",
      "Trained batch 344 batch loss 1.40943551 epoch total loss 1.40504205\n",
      "Trained batch 345 batch loss 1.34763074 epoch total loss 1.40487564\n",
      "Trained batch 346 batch loss 1.3733741 epoch total loss 1.40478468\n",
      "Trained batch 347 batch loss 1.41921473 epoch total loss 1.40482628\n",
      "Trained batch 348 batch loss 1.37144637 epoch total loss 1.40473032\n",
      "Trained batch 349 batch loss 1.43705213 epoch total loss 1.40482295\n",
      "Trained batch 350 batch loss 1.4371 epoch total loss 1.40491521\n",
      "Trained batch 351 batch loss 1.46600008 epoch total loss 1.40508926\n",
      "Trained batch 352 batch loss 1.52174115 epoch total loss 1.40542054\n",
      "Trained batch 353 batch loss 1.61887729 epoch total loss 1.40602517\n",
      "Trained batch 354 batch loss 1.51287925 epoch total loss 1.40632713\n",
      "Trained batch 355 batch loss 1.40766239 epoch total loss 1.40633082\n",
      "Trained batch 356 batch loss 1.33249044 epoch total loss 1.4061234\n",
      "Trained batch 357 batch loss 1.30827475 epoch total loss 1.40584934\n",
      "Trained batch 358 batch loss 1.32183385 epoch total loss 1.40561461\n",
      "Trained batch 359 batch loss 1.32889986 epoch total loss 1.40540099\n",
      "Trained batch 360 batch loss 1.32775939 epoch total loss 1.40518522\n",
      "Trained batch 361 batch loss 1.36663604 epoch total loss 1.40507853\n",
      "Trained batch 362 batch loss 1.32722414 epoch total loss 1.40486336\n",
      "Trained batch 363 batch loss 1.22821236 epoch total loss 1.40437675\n",
      "Trained batch 364 batch loss 1.26833332 epoch total loss 1.40400302\n",
      "Trained batch 365 batch loss 1.31018615 epoch total loss 1.40374601\n",
      "Trained batch 366 batch loss 1.41001248 epoch total loss 1.40376318\n",
      "Trained batch 367 batch loss 1.35951185 epoch total loss 1.40364265\n",
      "Trained batch 368 batch loss 1.2816906 epoch total loss 1.40331113\n",
      "Trained batch 369 batch loss 1.27713227 epoch total loss 1.40296936\n",
      "Trained batch 370 batch loss 1.32055748 epoch total loss 1.40274656\n",
      "Trained batch 371 batch loss 1.3509233 epoch total loss 1.40260696\n",
      "Trained batch 372 batch loss 1.38244772 epoch total loss 1.40255272\n",
      "Trained batch 373 batch loss 1.40001416 epoch total loss 1.40254605\n",
      "Trained batch 374 batch loss 1.32115018 epoch total loss 1.40232837\n",
      "Trained batch 375 batch loss 1.34109521 epoch total loss 1.40216517\n",
      "Trained batch 376 batch loss 1.26347947 epoch total loss 1.40179634\n",
      "Trained batch 377 batch loss 1.33889961 epoch total loss 1.40162957\n",
      "Trained batch 378 batch loss 1.38076806 epoch total loss 1.40157449\n",
      "Trained batch 379 batch loss 1.3668617 epoch total loss 1.40148294\n",
      "Trained batch 380 batch loss 1.28720117 epoch total loss 1.40118229\n",
      "Trained batch 381 batch loss 1.36399853 epoch total loss 1.40108478\n",
      "Trained batch 382 batch loss 1.33108497 epoch total loss 1.40090144\n",
      "Trained batch 383 batch loss 1.49519825 epoch total loss 1.4011476\n",
      "Trained batch 384 batch loss 1.44154334 epoch total loss 1.40125275\n",
      "Trained batch 385 batch loss 1.44217026 epoch total loss 1.40135908\n",
      "Trained batch 386 batch loss 1.38376701 epoch total loss 1.40131354\n",
      "Trained batch 387 batch loss 1.33678782 epoch total loss 1.40114689\n",
      "Trained batch 388 batch loss 1.30158508 epoch total loss 1.40089023\n",
      "Trained batch 389 batch loss 1.32740772 epoch total loss 1.40070128\n",
      "Trained batch 390 batch loss 1.35047793 epoch total loss 1.40057242\n",
      "Trained batch 391 batch loss 1.31260526 epoch total loss 1.40034759\n",
      "Trained batch 392 batch loss 1.31862259 epoch total loss 1.40013897\n",
      "Trained batch 393 batch loss 1.3299675 epoch total loss 1.3999604\n",
      "Trained batch 394 batch loss 1.47867453 epoch total loss 1.40016031\n",
      "Trained batch 395 batch loss 1.42690432 epoch total loss 1.4002279\n",
      "Trained batch 396 batch loss 1.3030144 epoch total loss 1.39998245\n",
      "Trained batch 397 batch loss 1.28773153 epoch total loss 1.39969969\n",
      "Trained batch 398 batch loss 1.25680161 epoch total loss 1.39934063\n",
      "Trained batch 399 batch loss 1.28782308 epoch total loss 1.3990612\n",
      "Trained batch 400 batch loss 1.34663486 epoch total loss 1.39893007\n",
      "Trained batch 401 batch loss 1.38256097 epoch total loss 1.3988893\n",
      "Trained batch 402 batch loss 1.42882299 epoch total loss 1.39896369\n",
      "Trained batch 403 batch loss 1.31055224 epoch total loss 1.39874434\n",
      "Trained batch 404 batch loss 1.32090533 epoch total loss 1.3985517\n",
      "Trained batch 405 batch loss 1.35734808 epoch total loss 1.39845\n",
      "Trained batch 406 batch loss 1.47082686 epoch total loss 1.39862823\n",
      "Trained batch 407 batch loss 1.42634034 epoch total loss 1.3986963\n",
      "Trained batch 408 batch loss 1.30213022 epoch total loss 1.39845967\n",
      "Trained batch 409 batch loss 1.32819009 epoch total loss 1.39828777\n",
      "Trained batch 410 batch loss 1.18234491 epoch total loss 1.39776123\n",
      "Trained batch 411 batch loss 1.2059685 epoch total loss 1.39729464\n",
      "Trained batch 412 batch loss 1.2902106 epoch total loss 1.39703476\n",
      "Trained batch 413 batch loss 1.29939461 epoch total loss 1.39679825\n",
      "Trained batch 414 batch loss 1.5435909 epoch total loss 1.39715278\n",
      "Trained batch 415 batch loss 1.60571313 epoch total loss 1.39765537\n",
      "Trained batch 416 batch loss 1.58162642 epoch total loss 1.39809752\n",
      "Trained batch 417 batch loss 1.51685333 epoch total loss 1.39838231\n",
      "Trained batch 418 batch loss 1.43585753 epoch total loss 1.39847195\n",
      "Trained batch 419 batch loss 1.41111588 epoch total loss 1.39850223\n",
      "Trained batch 420 batch loss 1.30875063 epoch total loss 1.39828849\n",
      "Trained batch 421 batch loss 1.37375712 epoch total loss 1.39823031\n",
      "Trained batch 422 batch loss 1.49545717 epoch total loss 1.39846075\n",
      "Trained batch 423 batch loss 1.42037225 epoch total loss 1.39851248\n",
      "Trained batch 424 batch loss 1.33511484 epoch total loss 1.39836311\n",
      "Trained batch 425 batch loss 1.30828524 epoch total loss 1.39815116\n",
      "Trained batch 426 batch loss 1.26702976 epoch total loss 1.39784336\n",
      "Trained batch 427 batch loss 1.37647367 epoch total loss 1.39779329\n",
      "Trained batch 428 batch loss 1.45399451 epoch total loss 1.39792454\n",
      "Trained batch 429 batch loss 1.56855512 epoch total loss 1.39832222\n",
      "Trained batch 430 batch loss 1.61587787 epoch total loss 1.39882827\n",
      "Trained batch 431 batch loss 1.47851574 epoch total loss 1.39901316\n",
      "Trained batch 432 batch loss 1.46666801 epoch total loss 1.3991698\n",
      "Trained batch 433 batch loss 1.61360383 epoch total loss 1.399665\n",
      "Trained batch 434 batch loss 1.52684903 epoch total loss 1.39995801\n",
      "Trained batch 435 batch loss 1.45649576 epoch total loss 1.40008795\n",
      "Trained batch 436 batch loss 1.29532719 epoch total loss 1.39984775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 437 batch loss 1.32139301 epoch total loss 1.39966822\n",
      "Trained batch 438 batch loss 1.199525 epoch total loss 1.39921129\n",
      "Trained batch 439 batch loss 1.19002235 epoch total loss 1.39873481\n",
      "Trained batch 440 batch loss 1.58116269 epoch total loss 1.39914942\n",
      "Trained batch 441 batch loss 1.48031855 epoch total loss 1.39933348\n",
      "Trained batch 442 batch loss 1.49302948 epoch total loss 1.39954555\n",
      "Trained batch 443 batch loss 1.3437978 epoch total loss 1.39941967\n",
      "Trained batch 444 batch loss 1.36538029 epoch total loss 1.39934301\n",
      "Trained batch 445 batch loss 1.36329627 epoch total loss 1.39926195\n",
      "Trained batch 446 batch loss 1.28858876 epoch total loss 1.39901376\n",
      "Trained batch 447 batch loss 1.38706505 epoch total loss 1.39898705\n",
      "Trained batch 448 batch loss 1.42633808 epoch total loss 1.39904809\n",
      "Trained batch 449 batch loss 1.32178855 epoch total loss 1.39887607\n",
      "Trained batch 450 batch loss 1.42508316 epoch total loss 1.39893436\n",
      "Trained batch 451 batch loss 1.38997114 epoch total loss 1.39891446\n",
      "Trained batch 452 batch loss 1.35464799 epoch total loss 1.39881659\n",
      "Trained batch 453 batch loss 1.33550608 epoch total loss 1.39867675\n",
      "Trained batch 454 batch loss 1.33204746 epoch total loss 1.39853\n",
      "Trained batch 455 batch loss 1.3180511 epoch total loss 1.3983531\n",
      "Trained batch 456 batch loss 1.33002532 epoch total loss 1.39820325\n",
      "Trained batch 457 batch loss 1.28449941 epoch total loss 1.39795446\n",
      "Trained batch 458 batch loss 1.44237852 epoch total loss 1.39805138\n",
      "Trained batch 459 batch loss 1.42462969 epoch total loss 1.39810932\n",
      "Trained batch 460 batch loss 1.50861168 epoch total loss 1.39834952\n",
      "Trained batch 461 batch loss 1.61904573 epoch total loss 1.39882815\n",
      "Trained batch 462 batch loss 1.49516439 epoch total loss 1.39903677\n",
      "Trained batch 463 batch loss 1.35559893 epoch total loss 1.39894295\n",
      "Trained batch 464 batch loss 1.44265544 epoch total loss 1.39903712\n",
      "Trained batch 465 batch loss 1.3023026 epoch total loss 1.3988291\n",
      "Trained batch 466 batch loss 1.38224602 epoch total loss 1.39879346\n",
      "Trained batch 467 batch loss 1.41651595 epoch total loss 1.39883137\n",
      "Trained batch 468 batch loss 1.46854377 epoch total loss 1.39898038\n",
      "Trained batch 469 batch loss 1.54436314 epoch total loss 1.39929044\n",
      "Trained batch 470 batch loss 1.49263072 epoch total loss 1.39948905\n",
      "Trained batch 471 batch loss 1.5568099 epoch total loss 1.39982307\n",
      "Trained batch 472 batch loss 1.584553 epoch total loss 1.40021431\n",
      "Trained batch 473 batch loss 1.48650777 epoch total loss 1.40039682\n",
      "Trained batch 474 batch loss 1.54993463 epoch total loss 1.40071225\n",
      "Trained batch 475 batch loss 1.46942174 epoch total loss 1.40085697\n",
      "Trained batch 476 batch loss 1.43809605 epoch total loss 1.40093517\n",
      "Trained batch 477 batch loss 1.44704676 epoch total loss 1.40103185\n",
      "Trained batch 478 batch loss 1.33339965 epoch total loss 1.40089023\n",
      "Trained batch 479 batch loss 1.43805993 epoch total loss 1.40096784\n",
      "Trained batch 480 batch loss 1.43609154 epoch total loss 1.40104103\n",
      "Trained batch 481 batch loss 1.40082502 epoch total loss 1.40104055\n",
      "Trained batch 482 batch loss 1.40160179 epoch total loss 1.40104175\n",
      "Trained batch 483 batch loss 1.42072153 epoch total loss 1.40108252\n",
      "Trained batch 484 batch loss 1.3218205 epoch total loss 1.40091872\n",
      "Trained batch 485 batch loss 1.39841521 epoch total loss 1.4009136\n",
      "Trained batch 486 batch loss 1.38340116 epoch total loss 1.40087759\n",
      "Trained batch 487 batch loss 1.48214877 epoch total loss 1.40104461\n",
      "Trained batch 488 batch loss 1.40483749 epoch total loss 1.40105236\n",
      "Trained batch 489 batch loss 1.28479683 epoch total loss 1.40081465\n",
      "Trained batch 490 batch loss 1.30023217 epoch total loss 1.40060937\n",
      "Trained batch 491 batch loss 1.4440043 epoch total loss 1.40069783\n",
      "Trained batch 492 batch loss 1.47106659 epoch total loss 1.40084088\n",
      "Trained batch 493 batch loss 1.40135908 epoch total loss 1.40084183\n",
      "Trained batch 494 batch loss 1.46999502 epoch total loss 1.40098178\n",
      "Trained batch 495 batch loss 1.55094588 epoch total loss 1.40128481\n",
      "Trained batch 496 batch loss 1.51084161 epoch total loss 1.40150571\n",
      "Trained batch 497 batch loss 1.40054262 epoch total loss 1.4015038\n",
      "Trained batch 498 batch loss 1.32027912 epoch total loss 1.4013406\n",
      "Trained batch 499 batch loss 1.34211612 epoch total loss 1.40122187\n",
      "Trained batch 500 batch loss 1.23196411 epoch total loss 1.40088332\n",
      "Trained batch 501 batch loss 1.35705638 epoch total loss 1.40079582\n",
      "Trained batch 502 batch loss 1.24145246 epoch total loss 1.40047836\n",
      "Trained batch 503 batch loss 1.40302956 epoch total loss 1.40048349\n",
      "Trained batch 504 batch loss 1.41883 epoch total loss 1.40051985\n",
      "Trained batch 505 batch loss 1.36269844 epoch total loss 1.40044487\n",
      "Trained batch 506 batch loss 1.35650861 epoch total loss 1.40035808\n",
      "Trained batch 507 batch loss 1.44666862 epoch total loss 1.4004494\n",
      "Trained batch 508 batch loss 1.35871 epoch total loss 1.40036714\n",
      "Trained batch 509 batch loss 1.43508244 epoch total loss 1.40043533\n",
      "Trained batch 510 batch loss 1.40011656 epoch total loss 1.40043473\n",
      "Trained batch 511 batch loss 1.31490886 epoch total loss 1.40026736\n",
      "Trained batch 512 batch loss 1.3421278 epoch total loss 1.40015376\n",
      "Trained batch 513 batch loss 1.33059931 epoch total loss 1.40001822\n",
      "Trained batch 514 batch loss 1.35796738 epoch total loss 1.39993644\n",
      "Trained batch 515 batch loss 1.37601662 epoch total loss 1.39989007\n",
      "Trained batch 516 batch loss 1.387568 epoch total loss 1.3998661\n",
      "Trained batch 517 batch loss 1.42974186 epoch total loss 1.39992392\n",
      "Trained batch 518 batch loss 1.36042953 epoch total loss 1.39984763\n",
      "Trained batch 519 batch loss 1.32398438 epoch total loss 1.39970148\n",
      "Trained batch 520 batch loss 1.33255744 epoch total loss 1.39957237\n",
      "Trained batch 521 batch loss 1.4047277 epoch total loss 1.39958227\n",
      "Trained batch 522 batch loss 1.39075637 epoch total loss 1.39956534\n",
      "Trained batch 523 batch loss 1.31835067 epoch total loss 1.39941013\n",
      "Trained batch 524 batch loss 1.17855799 epoch total loss 1.3989886\n",
      "Trained batch 525 batch loss 1.26870918 epoch total loss 1.39874041\n",
      "Trained batch 526 batch loss 1.29633677 epoch total loss 1.39854574\n",
      "Trained batch 527 batch loss 1.29186201 epoch total loss 1.39834332\n",
      "Trained batch 528 batch loss 1.33264852 epoch total loss 1.39821887\n",
      "Trained batch 529 batch loss 1.39943671 epoch total loss 1.39822114\n",
      "Trained batch 530 batch loss 1.2926265 epoch total loss 1.39802194\n",
      "Trained batch 531 batch loss 1.32948351 epoch total loss 1.39789283\n",
      "Trained batch 532 batch loss 1.44543827 epoch total loss 1.39798212\n",
      "Trained batch 533 batch loss 1.43645477 epoch total loss 1.39805436\n",
      "Trained batch 534 batch loss 1.36176705 epoch total loss 1.39798641\n",
      "Trained batch 535 batch loss 1.3263967 epoch total loss 1.39785254\n",
      "Trained batch 536 batch loss 1.32756078 epoch total loss 1.39772153\n",
      "Trained batch 537 batch loss 1.38928652 epoch total loss 1.39770579\n",
      "Trained batch 538 batch loss 1.17352176 epoch total loss 1.39728904\n",
      "Trained batch 539 batch loss 1.36616111 epoch total loss 1.39723134\n",
      "Trained batch 540 batch loss 1.36267042 epoch total loss 1.39716733\n",
      "Trained batch 541 batch loss 1.21808946 epoch total loss 1.39683628\n",
      "Trained batch 542 batch loss 1.29028368 epoch total loss 1.3966397\n",
      "Trained batch 543 batch loss 1.25335562 epoch total loss 1.39637578\n",
      "Trained batch 544 batch loss 1.38926399 epoch total loss 1.39636278\n",
      "Trained batch 545 batch loss 1.66613173 epoch total loss 1.39685774\n",
      "Trained batch 546 batch loss 1.38273191 epoch total loss 1.39683187\n",
      "Trained batch 547 batch loss 1.24690259 epoch total loss 1.39655781\n",
      "Trained batch 548 batch loss 1.21875525 epoch total loss 1.39623332\n",
      "Trained batch 549 batch loss 1.33548903 epoch total loss 1.39612269\n",
      "Trained batch 550 batch loss 1.3892411 epoch total loss 1.39611018\n",
      "Trained batch 551 batch loss 1.50324988 epoch total loss 1.39630461\n",
      "Trained batch 552 batch loss 1.41210437 epoch total loss 1.39633322\n",
      "Trained batch 553 batch loss 1.3575964 epoch total loss 1.39626324\n",
      "Trained batch 554 batch loss 1.37383556 epoch total loss 1.39622271\n",
      "Trained batch 555 batch loss 1.32670379 epoch total loss 1.39609754\n",
      "Trained batch 556 batch loss 1.35058987 epoch total loss 1.39601564\n",
      "Trained batch 557 batch loss 1.44628966 epoch total loss 1.39610589\n",
      "Trained batch 558 batch loss 1.44419062 epoch total loss 1.39619207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 559 batch loss 1.45292401 epoch total loss 1.39629364\n",
      "Trained batch 560 batch loss 1.42513502 epoch total loss 1.39634514\n",
      "Trained batch 561 batch loss 1.40105259 epoch total loss 1.39635348\n",
      "Trained batch 562 batch loss 1.28093684 epoch total loss 1.39614809\n",
      "Trained batch 563 batch loss 1.42148113 epoch total loss 1.39619315\n",
      "Trained batch 564 batch loss 1.3055 epoch total loss 1.39603233\n",
      "Trained batch 565 batch loss 1.04902029 epoch total loss 1.39541817\n",
      "Trained batch 566 batch loss 1.09199381 epoch total loss 1.39488208\n",
      "Trained batch 567 batch loss 1.36734021 epoch total loss 1.39483356\n",
      "Trained batch 568 batch loss 1.52079582 epoch total loss 1.39505529\n",
      "Trained batch 569 batch loss 1.70692706 epoch total loss 1.39560342\n",
      "Trained batch 570 batch loss 1.44104648 epoch total loss 1.39568305\n",
      "Trained batch 571 batch loss 1.24166536 epoch total loss 1.39541328\n",
      "Trained batch 572 batch loss 1.36572981 epoch total loss 1.39536142\n",
      "Trained batch 573 batch loss 1.51370013 epoch total loss 1.39556789\n",
      "Trained batch 574 batch loss 1.44965196 epoch total loss 1.39566207\n",
      "Trained batch 575 batch loss 1.44230974 epoch total loss 1.39574325\n",
      "Trained batch 576 batch loss 1.3880682 epoch total loss 1.3957299\n",
      "Trained batch 577 batch loss 1.46854138 epoch total loss 1.39585614\n",
      "Trained batch 578 batch loss 1.42011261 epoch total loss 1.3958981\n",
      "Trained batch 579 batch loss 1.40927136 epoch total loss 1.39592123\n",
      "Trained batch 580 batch loss 1.34432 epoch total loss 1.3958323\n",
      "Trained batch 581 batch loss 1.27070141 epoch total loss 1.39561689\n",
      "Trained batch 582 batch loss 1.37444735 epoch total loss 1.39558053\n",
      "Trained batch 583 batch loss 1.38273096 epoch total loss 1.39555848\n",
      "Trained batch 584 batch loss 1.33720279 epoch total loss 1.39545858\n",
      "Trained batch 585 batch loss 1.28083694 epoch total loss 1.3952626\n",
      "Trained batch 586 batch loss 1.43292356 epoch total loss 1.39532685\n",
      "Trained batch 587 batch loss 1.35684776 epoch total loss 1.39526141\n",
      "Trained batch 588 batch loss 1.417714 epoch total loss 1.39529955\n",
      "Trained batch 589 batch loss 1.3187933 epoch total loss 1.39516973\n",
      "Trained batch 590 batch loss 1.23315024 epoch total loss 1.39489508\n",
      "Trained batch 591 batch loss 1.36315727 epoch total loss 1.39484143\n",
      "Trained batch 592 batch loss 1.38449728 epoch total loss 1.39482391\n",
      "Trained batch 593 batch loss 1.19779146 epoch total loss 1.39449167\n",
      "Trained batch 594 batch loss 1.20667624 epoch total loss 1.39417553\n",
      "Trained batch 595 batch loss 1.17166924 epoch total loss 1.39380157\n",
      "Trained batch 596 batch loss 1.25470138 epoch total loss 1.39356816\n",
      "Trained batch 597 batch loss 1.28579259 epoch total loss 1.39338768\n",
      "Trained batch 598 batch loss 1.21809208 epoch total loss 1.39309442\n",
      "Trained batch 599 batch loss 1.35126424 epoch total loss 1.39302468\n",
      "Trained batch 600 batch loss 1.34260845 epoch total loss 1.39294052\n",
      "Trained batch 601 batch loss 1.32038784 epoch total loss 1.39281988\n",
      "Trained batch 602 batch loss 1.27582622 epoch total loss 1.39262545\n",
      "Trained batch 603 batch loss 1.27840745 epoch total loss 1.39243603\n",
      "Trained batch 604 batch loss 1.35951972 epoch total loss 1.39238143\n",
      "Trained batch 605 batch loss 1.41509092 epoch total loss 1.39241898\n",
      "Trained batch 606 batch loss 1.40990067 epoch total loss 1.39244795\n",
      "Trained batch 607 batch loss 1.41560042 epoch total loss 1.39248598\n",
      "Trained batch 608 batch loss 1.4165225 epoch total loss 1.39252555\n",
      "Trained batch 609 batch loss 1.41906333 epoch total loss 1.39256907\n",
      "Trained batch 610 batch loss 1.43059027 epoch total loss 1.39263141\n",
      "Trained batch 611 batch loss 1.2861824 epoch total loss 1.39245725\n",
      "Trained batch 612 batch loss 1.39579701 epoch total loss 1.39246273\n",
      "Trained batch 613 batch loss 1.34751964 epoch total loss 1.39238942\n",
      "Trained batch 614 batch loss 1.27690053 epoch total loss 1.39220142\n",
      "Trained batch 615 batch loss 1.29787362 epoch total loss 1.392048\n",
      "Trained batch 616 batch loss 1.41802073 epoch total loss 1.39209008\n",
      "Trained batch 617 batch loss 1.43711126 epoch total loss 1.39216316\n",
      "Trained batch 618 batch loss 1.50258756 epoch total loss 1.39234173\n",
      "Trained batch 619 batch loss 1.35007894 epoch total loss 1.39227355\n",
      "Trained batch 620 batch loss 1.46728468 epoch total loss 1.39239454\n",
      "Trained batch 621 batch loss 1.40458703 epoch total loss 1.39241421\n",
      "Trained batch 622 batch loss 1.45487285 epoch total loss 1.39251459\n",
      "Trained batch 623 batch loss 1.42024684 epoch total loss 1.39255917\n",
      "Trained batch 624 batch loss 1.37543559 epoch total loss 1.39253163\n",
      "Trained batch 625 batch loss 1.38669884 epoch total loss 1.39252234\n",
      "Trained batch 626 batch loss 1.30348086 epoch total loss 1.39238012\n",
      "Trained batch 627 batch loss 1.3302834 epoch total loss 1.39228106\n",
      "Trained batch 628 batch loss 1.21462214 epoch total loss 1.39199805\n",
      "Trained batch 629 batch loss 1.2903477 epoch total loss 1.39183652\n",
      "Trained batch 630 batch loss 1.31857109 epoch total loss 1.39172018\n",
      "Trained batch 631 batch loss 1.28008199 epoch total loss 1.39154327\n",
      "Trained batch 632 batch loss 1.11713266 epoch total loss 1.39110899\n",
      "Trained batch 633 batch loss 1.15159988 epoch total loss 1.39073062\n",
      "Trained batch 634 batch loss 1.37540483 epoch total loss 1.39070654\n",
      "Trained batch 635 batch loss 1.21242106 epoch total loss 1.3904258\n",
      "Trained batch 636 batch loss 1.32165873 epoch total loss 1.39031756\n",
      "Trained batch 637 batch loss 1.2943368 epoch total loss 1.39016688\n",
      "Trained batch 638 batch loss 1.51574671 epoch total loss 1.39036369\n",
      "Trained batch 639 batch loss 1.30833745 epoch total loss 1.39023542\n",
      "Trained batch 640 batch loss 1.26951456 epoch total loss 1.39004683\n",
      "Trained batch 641 batch loss 1.34046781 epoch total loss 1.38996947\n",
      "Trained batch 642 batch loss 1.29846549 epoch total loss 1.38982689\n",
      "Trained batch 643 batch loss 1.40393627 epoch total loss 1.38984883\n",
      "Trained batch 644 batch loss 1.12777078 epoch total loss 1.38944185\n",
      "Trained batch 645 batch loss 1.06730247 epoch total loss 1.38894236\n",
      "Trained batch 646 batch loss 1.11968935 epoch total loss 1.38852561\n",
      "Trained batch 647 batch loss 1.42001629 epoch total loss 1.38857436\n",
      "Trained batch 648 batch loss 1.51521766 epoch total loss 1.38876975\n",
      "Trained batch 649 batch loss 1.54464602 epoch total loss 1.38900983\n",
      "Trained batch 650 batch loss 1.5131067 epoch total loss 1.38920081\n",
      "Trained batch 651 batch loss 1.44955575 epoch total loss 1.38929355\n",
      "Trained batch 652 batch loss 1.38496387 epoch total loss 1.38928688\n",
      "Trained batch 653 batch loss 1.4708699 epoch total loss 1.38941193\n",
      "Trained batch 654 batch loss 1.35721397 epoch total loss 1.38936269\n",
      "Trained batch 655 batch loss 1.40649581 epoch total loss 1.3893888\n",
      "Trained batch 656 batch loss 1.32070565 epoch total loss 1.38928413\n",
      "Trained batch 657 batch loss 1.35768127 epoch total loss 1.38923597\n",
      "Trained batch 658 batch loss 1.34888268 epoch total loss 1.38917458\n",
      "Trained batch 659 batch loss 1.34774721 epoch total loss 1.38911176\n",
      "Trained batch 660 batch loss 1.41875625 epoch total loss 1.3891567\n",
      "Trained batch 661 batch loss 1.36149633 epoch total loss 1.38911486\n",
      "Trained batch 662 batch loss 1.33718324 epoch total loss 1.3890363\n",
      "Trained batch 663 batch loss 1.45109963 epoch total loss 1.38913\n",
      "Trained batch 664 batch loss 1.40141344 epoch total loss 1.38914847\n",
      "Trained batch 665 batch loss 1.40958178 epoch total loss 1.38917923\n",
      "Trained batch 666 batch loss 1.30918324 epoch total loss 1.38905919\n",
      "Trained batch 667 batch loss 1.25442803 epoch total loss 1.38885736\n",
      "Trained batch 668 batch loss 1.3030473 epoch total loss 1.38872886\n",
      "Trained batch 669 batch loss 1.31182432 epoch total loss 1.38861394\n",
      "Trained batch 670 batch loss 1.2872299 epoch total loss 1.38846266\n",
      "Trained batch 671 batch loss 1.4282459 epoch total loss 1.38852191\n",
      "Trained batch 672 batch loss 1.3990941 epoch total loss 1.38853765\n",
      "Trained batch 673 batch loss 1.34711027 epoch total loss 1.38847601\n",
      "Trained batch 674 batch loss 1.26851511 epoch total loss 1.38829803\n",
      "Trained batch 675 batch loss 1.22266901 epoch total loss 1.3880527\n",
      "Trained batch 676 batch loss 1.35177338 epoch total loss 1.38799894\n",
      "Trained batch 677 batch loss 1.33522463 epoch total loss 1.38792098\n",
      "Trained batch 678 batch loss 1.47828579 epoch total loss 1.38805425\n",
      "Trained batch 679 batch loss 1.44064403 epoch total loss 1.38813174\n",
      "Trained batch 680 batch loss 1.39704812 epoch total loss 1.38814485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 681 batch loss 1.34707701 epoch total loss 1.38808453\n",
      "Trained batch 682 batch loss 1.42715538 epoch total loss 1.38814187\n",
      "Trained batch 683 batch loss 1.51888335 epoch total loss 1.38833332\n",
      "Trained batch 684 batch loss 1.34304929 epoch total loss 1.38826716\n",
      "Trained batch 685 batch loss 1.39830148 epoch total loss 1.38828182\n",
      "Trained batch 686 batch loss 1.5270977 epoch total loss 1.38848412\n",
      "Trained batch 687 batch loss 1.46804559 epoch total loss 1.38859987\n",
      "Trained batch 688 batch loss 1.41535425 epoch total loss 1.38863873\n",
      "Trained batch 689 batch loss 1.40290177 epoch total loss 1.38865948\n",
      "Trained batch 690 batch loss 1.44897604 epoch total loss 1.38874686\n",
      "Trained batch 691 batch loss 1.50894833 epoch total loss 1.3889209\n",
      "Trained batch 692 batch loss 1.37776649 epoch total loss 1.38890469\n",
      "Trained batch 693 batch loss 1.34297276 epoch total loss 1.38883841\n",
      "Trained batch 694 batch loss 1.40944386 epoch total loss 1.38886809\n",
      "Trained batch 695 batch loss 1.35428703 epoch total loss 1.38881838\n",
      "Trained batch 696 batch loss 1.35362566 epoch total loss 1.38876784\n",
      "Trained batch 697 batch loss 1.28128195 epoch total loss 1.3886137\n",
      "Trained batch 698 batch loss 1.24650681 epoch total loss 1.38841009\n",
      "Trained batch 699 batch loss 1.28734767 epoch total loss 1.38826549\n",
      "Trained batch 700 batch loss 1.33348262 epoch total loss 1.38818729\n",
      "Trained batch 701 batch loss 1.29771292 epoch total loss 1.38805819\n",
      "Trained batch 702 batch loss 1.25842571 epoch total loss 1.38787353\n",
      "Trained batch 703 batch loss 1.30229247 epoch total loss 1.38775182\n",
      "Trained batch 704 batch loss 1.25190568 epoch total loss 1.38755882\n",
      "Trained batch 705 batch loss 1.25973678 epoch total loss 1.38737762\n",
      "Trained batch 706 batch loss 1.24916434 epoch total loss 1.38718176\n",
      "Trained batch 707 batch loss 1.19508696 epoch total loss 1.38691008\n",
      "Trained batch 708 batch loss 1.323174 epoch total loss 1.38682008\n",
      "Trained batch 709 batch loss 1.26014876 epoch total loss 1.38664138\n",
      "Trained batch 710 batch loss 1.3172698 epoch total loss 1.38654363\n",
      "Trained batch 711 batch loss 1.39106691 epoch total loss 1.38655\n",
      "Trained batch 712 batch loss 1.40911102 epoch total loss 1.38658166\n",
      "Trained batch 713 batch loss 1.36531258 epoch total loss 1.38655186\n",
      "Trained batch 714 batch loss 1.44540346 epoch total loss 1.38663423\n",
      "Trained batch 715 batch loss 1.36740732 epoch total loss 1.38660741\n",
      "Trained batch 716 batch loss 1.38064146 epoch total loss 1.38659894\n",
      "Trained batch 717 batch loss 1.49526513 epoch total loss 1.38675046\n",
      "Trained batch 718 batch loss 1.35960913 epoch total loss 1.38671267\n",
      "Trained batch 719 batch loss 1.34369171 epoch total loss 1.38665283\n",
      "Trained batch 720 batch loss 1.26986527 epoch total loss 1.38649058\n",
      "Trained batch 721 batch loss 1.35701346 epoch total loss 1.38644969\n",
      "Trained batch 722 batch loss 1.35859 epoch total loss 1.38641107\n",
      "Trained batch 723 batch loss 1.43435597 epoch total loss 1.38647735\n",
      "Trained batch 724 batch loss 1.26012921 epoch total loss 1.38630283\n",
      "Trained batch 725 batch loss 1.34454107 epoch total loss 1.38624525\n",
      "Trained batch 726 batch loss 1.42631721 epoch total loss 1.38630044\n",
      "Trained batch 727 batch loss 1.37616682 epoch total loss 1.3862865\n",
      "Trained batch 728 batch loss 1.45141113 epoch total loss 1.38637602\n",
      "Trained batch 729 batch loss 1.45608723 epoch total loss 1.38647163\n",
      "Trained batch 730 batch loss 1.42711711 epoch total loss 1.38652742\n",
      "Trained batch 731 batch loss 1.320158 epoch total loss 1.38643658\n",
      "Trained batch 732 batch loss 1.41274488 epoch total loss 1.38647246\n",
      "Trained batch 733 batch loss 1.3054986 epoch total loss 1.38636196\n",
      "Trained batch 734 batch loss 1.315979 epoch total loss 1.38626611\n",
      "Trained batch 735 batch loss 1.2261045 epoch total loss 1.38604808\n",
      "Trained batch 736 batch loss 1.21154439 epoch total loss 1.38581097\n",
      "Trained batch 737 batch loss 1.29994977 epoch total loss 1.3856945\n",
      "Trained batch 738 batch loss 1.15212595 epoch total loss 1.385378\n",
      "Trained batch 739 batch loss 1.14043462 epoch total loss 1.38504648\n",
      "Trained batch 740 batch loss 1.40890503 epoch total loss 1.38507867\n",
      "Trained batch 741 batch loss 1.53354049 epoch total loss 1.38527906\n",
      "Trained batch 742 batch loss 1.47539091 epoch total loss 1.38540053\n",
      "Trained batch 743 batch loss 1.43505692 epoch total loss 1.38546729\n",
      "Trained batch 744 batch loss 1.49777019 epoch total loss 1.38561833\n",
      "Trained batch 745 batch loss 1.43999934 epoch total loss 1.38569129\n",
      "Trained batch 746 batch loss 1.38312483 epoch total loss 1.38568783\n",
      "Trained batch 747 batch loss 1.43899226 epoch total loss 1.38575923\n",
      "Trained batch 748 batch loss 1.49377549 epoch total loss 1.3859036\n",
      "Trained batch 749 batch loss 1.39920974 epoch total loss 1.38592136\n",
      "Trained batch 750 batch loss 1.30322504 epoch total loss 1.38581109\n",
      "Trained batch 751 batch loss 1.34046161 epoch total loss 1.38575065\n",
      "Trained batch 752 batch loss 1.19267249 epoch total loss 1.38549387\n",
      "Trained batch 753 batch loss 1.23206496 epoch total loss 1.38529\n",
      "Trained batch 754 batch loss 1.31520176 epoch total loss 1.38519704\n",
      "Trained batch 755 batch loss 1.43297291 epoch total loss 1.38526034\n",
      "Trained batch 756 batch loss 1.39712119 epoch total loss 1.38527608\n",
      "Trained batch 757 batch loss 1.41371512 epoch total loss 1.38531363\n",
      "Trained batch 758 batch loss 1.50047612 epoch total loss 1.3854655\n",
      "Trained batch 759 batch loss 1.36095667 epoch total loss 1.3854332\n",
      "Trained batch 760 batch loss 1.31829119 epoch total loss 1.38534486\n",
      "Trained batch 761 batch loss 1.19341159 epoch total loss 1.3850925\n",
      "Trained batch 762 batch loss 1.3560853 epoch total loss 1.38505447\n",
      "Trained batch 763 batch loss 1.31014979 epoch total loss 1.38495636\n",
      "Trained batch 764 batch loss 1.40134311 epoch total loss 1.38497782\n",
      "Trained batch 765 batch loss 1.43745267 epoch total loss 1.38504648\n",
      "Trained batch 766 batch loss 1.46062779 epoch total loss 1.38514507\n",
      "Trained batch 767 batch loss 1.4780184 epoch total loss 1.38526618\n",
      "Trained batch 768 batch loss 1.55566454 epoch total loss 1.38548803\n",
      "Trained batch 769 batch loss 1.45756912 epoch total loss 1.38558173\n",
      "Trained batch 770 batch loss 1.32426429 epoch total loss 1.38550198\n",
      "Trained batch 771 batch loss 1.33630121 epoch total loss 1.3854382\n",
      "Trained batch 772 batch loss 1.4059763 epoch total loss 1.38546479\n",
      "Trained batch 773 batch loss 1.45716381 epoch total loss 1.38555753\n",
      "Trained batch 774 batch loss 1.41070628 epoch total loss 1.3855902\n",
      "Trained batch 775 batch loss 1.30608034 epoch total loss 1.38548744\n",
      "Trained batch 776 batch loss 1.29146576 epoch total loss 1.38536644\n",
      "Trained batch 777 batch loss 1.2996918 epoch total loss 1.38525605\n",
      "Trained batch 778 batch loss 1.27949584 epoch total loss 1.38512027\n",
      "Trained batch 779 batch loss 1.36950433 epoch total loss 1.38510013\n",
      "Trained batch 780 batch loss 1.28253758 epoch total loss 1.38496876\n",
      "Trained batch 781 batch loss 1.45683122 epoch total loss 1.38506067\n",
      "Trained batch 782 batch loss 1.53045058 epoch total loss 1.38524652\n",
      "Trained batch 783 batch loss 1.35646248 epoch total loss 1.3852098\n",
      "Trained batch 784 batch loss 1.17995739 epoch total loss 1.3849479\n",
      "Trained batch 785 batch loss 1.18911409 epoch total loss 1.38469839\n",
      "Trained batch 786 batch loss 1.48463356 epoch total loss 1.38482559\n",
      "Trained batch 787 batch loss 1.47526705 epoch total loss 1.38494039\n",
      "Trained batch 788 batch loss 1.40589011 epoch total loss 1.38496697\n",
      "Trained batch 789 batch loss 1.36651206 epoch total loss 1.3849436\n",
      "Trained batch 790 batch loss 1.30487132 epoch total loss 1.38484228\n",
      "Trained batch 791 batch loss 1.40865421 epoch total loss 1.38487244\n",
      "Trained batch 792 batch loss 1.3407923 epoch total loss 1.38481677\n",
      "Trained batch 793 batch loss 1.43323863 epoch total loss 1.3848778\n",
      "Trained batch 794 batch loss 1.31019604 epoch total loss 1.38478374\n",
      "Trained batch 795 batch loss 1.2790215 epoch total loss 1.38465083\n",
      "Trained batch 796 batch loss 1.35808277 epoch total loss 1.38461733\n",
      "Trained batch 797 batch loss 1.3703984 epoch total loss 1.38459945\n",
      "Trained batch 798 batch loss 1.32913196 epoch total loss 1.38453\n",
      "Trained batch 799 batch loss 1.38009822 epoch total loss 1.38452435\n",
      "Trained batch 800 batch loss 1.40491033 epoch total loss 1.38454986\n",
      "Trained batch 801 batch loss 1.40885425 epoch total loss 1.38458014\n",
      "Trained batch 802 batch loss 1.3819772 epoch total loss 1.38457692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 803 batch loss 1.35677874 epoch total loss 1.38454235\n",
      "Trained batch 804 batch loss 1.40378332 epoch total loss 1.38456631\n",
      "Trained batch 805 batch loss 1.37559474 epoch total loss 1.3845551\n",
      "Trained batch 806 batch loss 1.31695151 epoch total loss 1.38447118\n",
      "Trained batch 807 batch loss 1.3648181 epoch total loss 1.38444686\n",
      "Trained batch 808 batch loss 1.42238677 epoch total loss 1.38449383\n",
      "Trained batch 809 batch loss 1.36173832 epoch total loss 1.38446569\n",
      "Trained batch 810 batch loss 1.31685245 epoch total loss 1.38438225\n",
      "Trained batch 811 batch loss 1.2654599 epoch total loss 1.38423562\n",
      "Trained batch 812 batch loss 1.26978755 epoch total loss 1.38409472\n",
      "Trained batch 813 batch loss 1.46275258 epoch total loss 1.38419151\n",
      "Trained batch 814 batch loss 1.39463186 epoch total loss 1.38420427\n",
      "Trained batch 815 batch loss 1.42521262 epoch total loss 1.38425457\n",
      "Trained batch 816 batch loss 1.36190403 epoch total loss 1.38422728\n",
      "Trained batch 817 batch loss 1.41432786 epoch total loss 1.38426399\n",
      "Trained batch 818 batch loss 1.40430057 epoch total loss 1.38428855\n",
      "Trained batch 819 batch loss 1.31253624 epoch total loss 1.38420093\n",
      "Trained batch 820 batch loss 1.18127894 epoch total loss 1.38395345\n",
      "Trained batch 821 batch loss 1.37614501 epoch total loss 1.3839438\n",
      "Trained batch 822 batch loss 1.41666293 epoch total loss 1.38398361\n",
      "Trained batch 823 batch loss 1.43042505 epoch total loss 1.38404\n",
      "Trained batch 824 batch loss 1.3735137 epoch total loss 1.38402724\n",
      "Trained batch 825 batch loss 1.29931819 epoch total loss 1.3839246\n",
      "Trained batch 826 batch loss 1.34885597 epoch total loss 1.38388216\n",
      "Trained batch 827 batch loss 1.35037851 epoch total loss 1.38384163\n",
      "Trained batch 828 batch loss 1.25138807 epoch total loss 1.38368154\n",
      "Trained batch 829 batch loss 1.26721346 epoch total loss 1.38354111\n",
      "Trained batch 830 batch loss 1.28084099 epoch total loss 1.38341737\n",
      "Trained batch 831 batch loss 1.40702379 epoch total loss 1.38344574\n",
      "Trained batch 832 batch loss 1.35877538 epoch total loss 1.38341606\n",
      "Trained batch 833 batch loss 1.24589682 epoch total loss 1.38325095\n",
      "Trained batch 834 batch loss 1.46681869 epoch total loss 1.38335109\n",
      "Trained batch 835 batch loss 1.43242049 epoch total loss 1.38340986\n",
      "Trained batch 836 batch loss 1.41256404 epoch total loss 1.38344479\n",
      "Trained batch 837 batch loss 1.38702965 epoch total loss 1.38344908\n",
      "Trained batch 838 batch loss 1.28494322 epoch total loss 1.38333154\n",
      "Trained batch 839 batch loss 1.36547279 epoch total loss 1.3833102\n",
      "Trained batch 840 batch loss 1.43556166 epoch total loss 1.38337243\n",
      "Trained batch 841 batch loss 1.51790106 epoch total loss 1.3835324\n",
      "Trained batch 842 batch loss 1.3393265 epoch total loss 1.38348\n",
      "Trained batch 843 batch loss 1.35594988 epoch total loss 1.38344729\n",
      "Trained batch 844 batch loss 1.28449786 epoch total loss 1.38333011\n",
      "Trained batch 845 batch loss 1.42603314 epoch total loss 1.38338065\n",
      "Trained batch 846 batch loss 1.44020951 epoch total loss 1.38344777\n",
      "Trained batch 847 batch loss 1.46120954 epoch total loss 1.38353956\n",
      "Trained batch 848 batch loss 1.47057915 epoch total loss 1.3836422\n",
      "Trained batch 849 batch loss 1.30002272 epoch total loss 1.38354373\n",
      "Trained batch 850 batch loss 1.44729757 epoch total loss 1.38361871\n",
      "Trained batch 851 batch loss 1.45018339 epoch total loss 1.38369691\n",
      "Trained batch 852 batch loss 1.3901825 epoch total loss 1.38370454\n",
      "Trained batch 853 batch loss 1.33938313 epoch total loss 1.38365257\n",
      "Trained batch 854 batch loss 1.25549877 epoch total loss 1.38350248\n",
      "Trained batch 855 batch loss 1.17636454 epoch total loss 1.38326025\n",
      "Trained batch 856 batch loss 1.22822964 epoch total loss 1.38307917\n",
      "Trained batch 857 batch loss 1.20318258 epoch total loss 1.38286924\n",
      "Trained batch 858 batch loss 1.31658041 epoch total loss 1.38279188\n",
      "Trained batch 859 batch loss 1.13343477 epoch total loss 1.3825016\n",
      "Trained batch 860 batch loss 1.06689894 epoch total loss 1.38213456\n",
      "Trained batch 861 batch loss 1.05283165 epoch total loss 1.38175213\n",
      "Trained batch 862 batch loss 1.12795377 epoch total loss 1.38145769\n",
      "Trained batch 863 batch loss 1.37636542 epoch total loss 1.38145173\n",
      "Trained batch 864 batch loss 1.34949505 epoch total loss 1.38141477\n",
      "Trained batch 865 batch loss 1.45148683 epoch total loss 1.38149583\n",
      "Trained batch 866 batch loss 1.1838547 epoch total loss 1.38126755\n",
      "Trained batch 867 batch loss 1.42295587 epoch total loss 1.38131571\n",
      "Trained batch 868 batch loss 1.37192535 epoch total loss 1.38130486\n",
      "Trained batch 869 batch loss 1.26954556 epoch total loss 1.38117623\n",
      "Trained batch 870 batch loss 1.33265758 epoch total loss 1.38112044\n",
      "Trained batch 871 batch loss 1.4718504 epoch total loss 1.38122463\n",
      "Trained batch 872 batch loss 1.30617702 epoch total loss 1.38113856\n",
      "Trained batch 873 batch loss 1.26407444 epoch total loss 1.38100433\n",
      "Trained batch 874 batch loss 1.35468888 epoch total loss 1.38097429\n",
      "Trained batch 875 batch loss 1.39024901 epoch total loss 1.3809849\n",
      "Trained batch 876 batch loss 1.47239792 epoch total loss 1.38108933\n",
      "Trained batch 877 batch loss 1.15475321 epoch total loss 1.38083124\n",
      "Trained batch 878 batch loss 1.17125273 epoch total loss 1.38059258\n",
      "Trained batch 879 batch loss 1.22723567 epoch total loss 1.38041818\n",
      "Trained batch 880 batch loss 1.34708214 epoch total loss 1.38038027\n",
      "Trained batch 881 batch loss 1.32227516 epoch total loss 1.38031423\n",
      "Trained batch 882 batch loss 1.25841546 epoch total loss 1.38017607\n",
      "Trained batch 883 batch loss 1.33842611 epoch total loss 1.38012874\n",
      "Trained batch 884 batch loss 1.40802562 epoch total loss 1.38016033\n",
      "Trained batch 885 batch loss 1.43910861 epoch total loss 1.38022697\n",
      "Trained batch 886 batch loss 1.34796834 epoch total loss 1.38019061\n",
      "Trained batch 887 batch loss 1.42649031 epoch total loss 1.38024282\n",
      "Trained batch 888 batch loss 1.68151629 epoch total loss 1.38058209\n",
      "Trained batch 889 batch loss 1.54687917 epoch total loss 1.38076913\n",
      "Trained batch 890 batch loss 1.4128741 epoch total loss 1.38080525\n",
      "Trained batch 891 batch loss 1.40835381 epoch total loss 1.38083613\n",
      "Trained batch 892 batch loss 1.43107808 epoch total loss 1.3808924\n",
      "Trained batch 893 batch loss 1.46415269 epoch total loss 1.3809855\n",
      "Trained batch 894 batch loss 1.47167528 epoch total loss 1.38108695\n",
      "Trained batch 895 batch loss 1.32273936 epoch total loss 1.38102186\n",
      "Trained batch 896 batch loss 1.3943584 epoch total loss 1.38103676\n",
      "Trained batch 897 batch loss 1.43003559 epoch total loss 1.38109136\n",
      "Trained batch 898 batch loss 1.37868619 epoch total loss 1.38108873\n",
      "Trained batch 899 batch loss 1.35508108 epoch total loss 1.38105977\n",
      "Trained batch 900 batch loss 1.45951557 epoch total loss 1.38114691\n",
      "Trained batch 901 batch loss 1.41163814 epoch total loss 1.38118076\n",
      "Trained batch 902 batch loss 1.39565063 epoch total loss 1.38119674\n",
      "Trained batch 903 batch loss 1.41346574 epoch total loss 1.3812325\n",
      "Trained batch 904 batch loss 1.34760201 epoch total loss 1.38119531\n",
      "Trained batch 905 batch loss 1.28834057 epoch total loss 1.38109267\n",
      "Trained batch 906 batch loss 1.35388875 epoch total loss 1.38106275\n",
      "Trained batch 907 batch loss 1.48723578 epoch total loss 1.38117969\n",
      "Trained batch 908 batch loss 1.31480718 epoch total loss 1.38110662\n",
      "Trained batch 909 batch loss 1.3065176 epoch total loss 1.3810246\n",
      "Trained batch 910 batch loss 1.15362656 epoch total loss 1.38077474\n",
      "Trained batch 911 batch loss 1.27395093 epoch total loss 1.38065743\n",
      "Trained batch 912 batch loss 1.41147852 epoch total loss 1.38069129\n",
      "Trained batch 913 batch loss 1.29381657 epoch total loss 1.38059616\n",
      "Trained batch 914 batch loss 1.35436058 epoch total loss 1.38056743\n",
      "Trained batch 915 batch loss 1.34272051 epoch total loss 1.38052607\n",
      "Trained batch 916 batch loss 1.12416577 epoch total loss 1.38024628\n",
      "Trained batch 917 batch loss 1.2273488 epoch total loss 1.38007939\n",
      "Trained batch 918 batch loss 1.32234919 epoch total loss 1.38001657\n",
      "Trained batch 919 batch loss 1.32887328 epoch total loss 1.37996089\n",
      "Trained batch 920 batch loss 1.44461453 epoch total loss 1.38003111\n",
      "Trained batch 921 batch loss 1.45848656 epoch total loss 1.38011634\n",
      "Trained batch 922 batch loss 1.50399458 epoch total loss 1.38025069\n",
      "Trained batch 923 batch loss 1.43113029 epoch total loss 1.38030589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 924 batch loss 1.37420559 epoch total loss 1.38029921\n",
      "Trained batch 925 batch loss 1.40195382 epoch total loss 1.38032269\n",
      "Trained batch 926 batch loss 1.40513325 epoch total loss 1.38034952\n",
      "Trained batch 927 batch loss 1.41979909 epoch total loss 1.38039207\n",
      "Trained batch 928 batch loss 1.45173848 epoch total loss 1.38046896\n",
      "Trained batch 929 batch loss 1.41070402 epoch total loss 1.38050139\n",
      "Trained batch 930 batch loss 1.46007717 epoch total loss 1.38058698\n",
      "Trained batch 931 batch loss 1.36463189 epoch total loss 1.38056982\n",
      "Trained batch 932 batch loss 1.32206106 epoch total loss 1.38050699\n",
      "Trained batch 933 batch loss 1.36450636 epoch total loss 1.38049\n",
      "Trained batch 934 batch loss 1.3749882 epoch total loss 1.38048398\n",
      "Trained batch 935 batch loss 1.44699073 epoch total loss 1.38055515\n",
      "Trained batch 936 batch loss 1.4284184 epoch total loss 1.38060641\n",
      "Trained batch 937 batch loss 1.34480262 epoch total loss 1.38056815\n",
      "Trained batch 938 batch loss 1.42534721 epoch total loss 1.38061583\n",
      "Trained batch 939 batch loss 1.32386088 epoch total loss 1.38055539\n",
      "Trained batch 940 batch loss 1.35921562 epoch total loss 1.38053274\n",
      "Trained batch 941 batch loss 1.37641096 epoch total loss 1.38052845\n",
      "Trained batch 942 batch loss 1.32461357 epoch total loss 1.38046908\n",
      "Trained batch 943 batch loss 1.30138028 epoch total loss 1.38038516\n",
      "Trained batch 944 batch loss 1.36706102 epoch total loss 1.38037109\n",
      "Trained batch 945 batch loss 1.31619334 epoch total loss 1.38030314\n",
      "Trained batch 946 batch loss 1.33482289 epoch total loss 1.3802551\n",
      "Trained batch 947 batch loss 1.27396417 epoch total loss 1.38014281\n",
      "Trained batch 948 batch loss 1.20922506 epoch total loss 1.37996256\n",
      "Trained batch 949 batch loss 1.30211043 epoch total loss 1.37988055\n",
      "Trained batch 950 batch loss 1.28203511 epoch total loss 1.37977743\n",
      "Trained batch 951 batch loss 1.40415883 epoch total loss 1.37980306\n",
      "Trained batch 952 batch loss 1.37627137 epoch total loss 1.37979937\n",
      "Trained batch 953 batch loss 1.31875396 epoch total loss 1.37973523\n",
      "Trained batch 954 batch loss 1.46244299 epoch total loss 1.3798219\n",
      "Trained batch 955 batch loss 1.36924958 epoch total loss 1.37981081\n",
      "Trained batch 956 batch loss 1.4381783 epoch total loss 1.37987196\n",
      "Trained batch 957 batch loss 1.46804631 epoch total loss 1.37996411\n",
      "Trained batch 958 batch loss 1.46943831 epoch total loss 1.38005745\n",
      "Trained batch 959 batch loss 1.39240909 epoch total loss 1.38007045\n",
      "Trained batch 960 batch loss 1.45611322 epoch total loss 1.3801496\n",
      "Trained batch 961 batch loss 1.37856054 epoch total loss 1.38014793\n",
      "Trained batch 962 batch loss 1.35040939 epoch total loss 1.38011706\n",
      "Trained batch 963 batch loss 1.36234856 epoch total loss 1.38009858\n",
      "Trained batch 964 batch loss 1.50506759 epoch total loss 1.38022828\n",
      "Trained batch 965 batch loss 1.46845043 epoch total loss 1.38031971\n",
      "Trained batch 966 batch loss 1.42986178 epoch total loss 1.38037097\n",
      "Trained batch 967 batch loss 1.39083028 epoch total loss 1.38038182\n",
      "Trained batch 968 batch loss 1.32041311 epoch total loss 1.38032\n",
      "Trained batch 969 batch loss 1.38619614 epoch total loss 1.38032603\n",
      "Trained batch 970 batch loss 1.4103353 epoch total loss 1.38035691\n",
      "Trained batch 971 batch loss 1.50271535 epoch total loss 1.38048291\n",
      "Trained batch 972 batch loss 1.49647021 epoch total loss 1.38060212\n",
      "Trained batch 973 batch loss 1.38747263 epoch total loss 1.38060915\n",
      "Trained batch 974 batch loss 1.38356864 epoch total loss 1.38061225\n",
      "Trained batch 975 batch loss 1.24353611 epoch total loss 1.38047159\n",
      "Trained batch 976 batch loss 1.276016 epoch total loss 1.38036454\n",
      "Trained batch 977 batch loss 1.28058279 epoch total loss 1.38026249\n",
      "Trained batch 978 batch loss 1.34743643 epoch total loss 1.38022888\n",
      "Trained batch 979 batch loss 1.35533321 epoch total loss 1.38020349\n",
      "Trained batch 980 batch loss 1.39957404 epoch total loss 1.38022327\n",
      "Trained batch 981 batch loss 1.40076649 epoch total loss 1.38024414\n",
      "Trained batch 982 batch loss 1.31698978 epoch total loss 1.38017976\n",
      "Trained batch 983 batch loss 1.33853972 epoch total loss 1.38013744\n",
      "Trained batch 984 batch loss 1.31940591 epoch total loss 1.38007569\n",
      "Trained batch 985 batch loss 1.25858867 epoch total loss 1.37995231\n",
      "Trained batch 986 batch loss 1.42123973 epoch total loss 1.37999427\n",
      "Trained batch 987 batch loss 1.34830725 epoch total loss 1.37996209\n",
      "Trained batch 988 batch loss 1.39689624 epoch total loss 1.37997913\n",
      "Trained batch 989 batch loss 1.4422332 epoch total loss 1.3800422\n",
      "Trained batch 990 batch loss 1.34353447 epoch total loss 1.38000524\n",
      "Trained batch 991 batch loss 1.40818787 epoch total loss 1.38003373\n",
      "Trained batch 992 batch loss 1.36534905 epoch total loss 1.38001895\n",
      "Trained batch 993 batch loss 1.38258624 epoch total loss 1.38002145\n",
      "Trained batch 994 batch loss 1.27585363 epoch total loss 1.37991667\n",
      "Trained batch 995 batch loss 1.40591645 epoch total loss 1.37994277\n",
      "Trained batch 996 batch loss 1.29031456 epoch total loss 1.37985277\n",
      "Trained batch 997 batch loss 1.32329392 epoch total loss 1.37979603\n",
      "Trained batch 998 batch loss 1.34503901 epoch total loss 1.37976122\n",
      "Trained batch 999 batch loss 1.33642077 epoch total loss 1.37971783\n",
      "Trained batch 1000 batch loss 1.30660307 epoch total loss 1.37964475\n",
      "Trained batch 1001 batch loss 1.31943345 epoch total loss 1.37958467\n",
      "Trained batch 1002 batch loss 1.45296919 epoch total loss 1.37965786\n",
      "Trained batch 1003 batch loss 1.4018178 epoch total loss 1.37968\n",
      "Trained batch 1004 batch loss 1.4359982 epoch total loss 1.37973619\n",
      "Trained batch 1005 batch loss 1.39203918 epoch total loss 1.37974846\n",
      "Trained batch 1006 batch loss 1.36441684 epoch total loss 1.3797332\n",
      "Trained batch 1007 batch loss 1.33640182 epoch total loss 1.37969017\n",
      "Trained batch 1008 batch loss 1.35200572 epoch total loss 1.37966275\n",
      "Trained batch 1009 batch loss 1.39018464 epoch total loss 1.37967312\n",
      "Trained batch 1010 batch loss 1.57540381 epoch total loss 1.37986696\n",
      "Trained batch 1011 batch loss 1.56921279 epoch total loss 1.38005424\n",
      "Trained batch 1012 batch loss 1.46008372 epoch total loss 1.38013339\n",
      "Trained batch 1013 batch loss 1.49573994 epoch total loss 1.38024747\n",
      "Trained batch 1014 batch loss 1.38081598 epoch total loss 1.38024807\n",
      "Trained batch 1015 batch loss 1.54015434 epoch total loss 1.38040566\n",
      "Trained batch 1016 batch loss 1.47602439 epoch total loss 1.38049972\n",
      "Trained batch 1017 batch loss 1.41022813 epoch total loss 1.38052905\n",
      "Trained batch 1018 batch loss 1.31749499 epoch total loss 1.38046718\n",
      "Trained batch 1019 batch loss 1.3031404 epoch total loss 1.38039124\n",
      "Trained batch 1020 batch loss 1.27639282 epoch total loss 1.3802892\n",
      "Trained batch 1021 batch loss 1.26553583 epoch total loss 1.38017678\n",
      "Trained batch 1022 batch loss 1.32506788 epoch total loss 1.3801229\n",
      "Trained batch 1023 batch loss 1.36107874 epoch total loss 1.3801043\n",
      "Trained batch 1024 batch loss 1.40131569 epoch total loss 1.38012505\n",
      "Trained batch 1025 batch loss 1.37919617 epoch total loss 1.38012409\n",
      "Trained batch 1026 batch loss 1.21707952 epoch total loss 1.37996519\n",
      "Trained batch 1027 batch loss 1.27285266 epoch total loss 1.37986088\n",
      "Trained batch 1028 batch loss 1.39167237 epoch total loss 1.37987232\n",
      "Trained batch 1029 batch loss 1.33453643 epoch total loss 1.37982833\n",
      "Trained batch 1030 batch loss 1.23922348 epoch total loss 1.37969184\n",
      "Trained batch 1031 batch loss 1.22226191 epoch total loss 1.37953925\n",
      "Trained batch 1032 batch loss 1.30610943 epoch total loss 1.37946808\n",
      "Trained batch 1033 batch loss 1.19745994 epoch total loss 1.37929201\n",
      "Trained batch 1034 batch loss 1.43754756 epoch total loss 1.37934828\n",
      "Trained batch 1035 batch loss 1.38813138 epoch total loss 1.37935674\n",
      "Trained batch 1036 batch loss 1.39737749 epoch total loss 1.37937415\n",
      "Trained batch 1037 batch loss 1.26540136 epoch total loss 1.37926424\n",
      "Trained batch 1038 batch loss 1.16692352 epoch total loss 1.37905955\n",
      "Trained batch 1039 batch loss 1.12414074 epoch total loss 1.37881422\n",
      "Trained batch 1040 batch loss 1.1585145 epoch total loss 1.3786025\n",
      "Trained batch 1041 batch loss 1.31425977 epoch total loss 1.37854064\n",
      "Trained batch 1042 batch loss 1.26147699 epoch total loss 1.37842822\n",
      "Trained batch 1043 batch loss 1.25569391 epoch total loss 1.37831068\n",
      "Trained batch 1044 batch loss 1.24498618 epoch total loss 1.37818301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1045 batch loss 1.314798 epoch total loss 1.37812233\n",
      "Trained batch 1046 batch loss 1.20167851 epoch total loss 1.37795365\n",
      "Trained batch 1047 batch loss 1.31727779 epoch total loss 1.37789559\n",
      "Trained batch 1048 batch loss 1.32898581 epoch total loss 1.37784898\n",
      "Trained batch 1049 batch loss 1.22330379 epoch total loss 1.37770164\n",
      "Trained batch 1050 batch loss 1.1644845 epoch total loss 1.37749851\n",
      "Trained batch 1051 batch loss 1.01526988 epoch total loss 1.37715387\n",
      "Trained batch 1052 batch loss 1.2040751 epoch total loss 1.37698936\n",
      "Trained batch 1053 batch loss 1.44855785 epoch total loss 1.37705731\n",
      "Trained batch 1054 batch loss 1.429721 epoch total loss 1.37710726\n",
      "Trained batch 1055 batch loss 1.38583589 epoch total loss 1.37711561\n",
      "Trained batch 1056 batch loss 1.34619689 epoch total loss 1.37708628\n",
      "Trained batch 1057 batch loss 1.33677876 epoch total loss 1.37704813\n",
      "Trained batch 1058 batch loss 1.14214778 epoch total loss 1.37682605\n",
      "Trained batch 1059 batch loss 1.25457036 epoch total loss 1.37671065\n",
      "Trained batch 1060 batch loss 1.27790296 epoch total loss 1.37661743\n",
      "Trained batch 1061 batch loss 1.46201813 epoch total loss 1.3766979\n",
      "Trained batch 1062 batch loss 1.27808499 epoch total loss 1.37660503\n",
      "Trained batch 1063 batch loss 1.30535758 epoch total loss 1.37653804\n",
      "Trained batch 1064 batch loss 1.42051625 epoch total loss 1.37657928\n",
      "Trained batch 1065 batch loss 1.31366479 epoch total loss 1.37652028\n",
      "Trained batch 1066 batch loss 1.22195601 epoch total loss 1.37637532\n",
      "Trained batch 1067 batch loss 1.21573222 epoch total loss 1.37622476\n",
      "Trained batch 1068 batch loss 1.22341549 epoch total loss 1.37608159\n",
      "Trained batch 1069 batch loss 1.2334367 epoch total loss 1.37594807\n",
      "Trained batch 1070 batch loss 1.30775654 epoch total loss 1.37588441\n",
      "Trained batch 1071 batch loss 1.31824386 epoch total loss 1.37583053\n",
      "Trained batch 1072 batch loss 1.28766739 epoch total loss 1.3757484\n",
      "Trained batch 1073 batch loss 1.35873318 epoch total loss 1.37573254\n",
      "Trained batch 1074 batch loss 1.36953092 epoch total loss 1.3757267\n",
      "Trained batch 1075 batch loss 1.42796373 epoch total loss 1.37577534\n",
      "Trained batch 1076 batch loss 1.33409083 epoch total loss 1.37573659\n",
      "Trained batch 1077 batch loss 1.22486973 epoch total loss 1.37559652\n",
      "Trained batch 1078 batch loss 1.2396512 epoch total loss 1.3754704\n",
      "Trained batch 1079 batch loss 1.3734889 epoch total loss 1.37546861\n",
      "Trained batch 1080 batch loss 1.4111867 epoch total loss 1.37550163\n",
      "Trained batch 1081 batch loss 1.43088806 epoch total loss 1.37555289\n",
      "Trained batch 1082 batch loss 1.37587786 epoch total loss 1.37555313\n",
      "Trained batch 1083 batch loss 1.31539106 epoch total loss 1.37549758\n",
      "Trained batch 1084 batch loss 1.23660946 epoch total loss 1.37536943\n",
      "Trained batch 1085 batch loss 1.36473536 epoch total loss 1.37535965\n",
      "Trained batch 1086 batch loss 1.54586041 epoch total loss 1.37551677\n",
      "Trained batch 1087 batch loss 1.43651438 epoch total loss 1.3755728\n",
      "Trained batch 1088 batch loss 1.37633371 epoch total loss 1.37557352\n",
      "Trained batch 1089 batch loss 1.33277535 epoch total loss 1.3755343\n",
      "Trained batch 1090 batch loss 1.3392123 epoch total loss 1.37550092\n",
      "Trained batch 1091 batch loss 1.39770854 epoch total loss 1.3755213\n",
      "Trained batch 1092 batch loss 1.27264237 epoch total loss 1.37542701\n",
      "Trained batch 1093 batch loss 1.33289945 epoch total loss 1.37538815\n",
      "Trained batch 1094 batch loss 1.37849689 epoch total loss 1.37539101\n",
      "Trained batch 1095 batch loss 1.29846847 epoch total loss 1.37532067\n",
      "Trained batch 1096 batch loss 1.31418204 epoch total loss 1.375265\n",
      "Trained batch 1097 batch loss 1.33890307 epoch total loss 1.37523174\n",
      "Trained batch 1098 batch loss 1.30803776 epoch total loss 1.37517059\n",
      "Trained batch 1099 batch loss 1.32887661 epoch total loss 1.37512839\n",
      "Trained batch 1100 batch loss 1.30773365 epoch total loss 1.37506711\n",
      "Trained batch 1101 batch loss 1.33762765 epoch total loss 1.37503314\n",
      "Trained batch 1102 batch loss 1.27690566 epoch total loss 1.37494409\n",
      "Trained batch 1103 batch loss 1.25095689 epoch total loss 1.37483168\n",
      "Trained batch 1104 batch loss 1.23391962 epoch total loss 1.374704\n",
      "Trained batch 1105 batch loss 1.29841149 epoch total loss 1.37463498\n",
      "Trained batch 1106 batch loss 1.35779083 epoch total loss 1.37461972\n",
      "Trained batch 1107 batch loss 1.4358108 epoch total loss 1.37467504\n",
      "Trained batch 1108 batch loss 1.19879436 epoch total loss 1.37451637\n",
      "Trained batch 1109 batch loss 1.40541542 epoch total loss 1.37454414\n",
      "Trained batch 1110 batch loss 1.35096371 epoch total loss 1.37452292\n",
      "Trained batch 1111 batch loss 1.40708756 epoch total loss 1.37455225\n",
      "Trained batch 1112 batch loss 1.37084293 epoch total loss 1.37454891\n",
      "Trained batch 1113 batch loss 1.21325326 epoch total loss 1.37440407\n",
      "Trained batch 1114 batch loss 1.27236283 epoch total loss 1.3743124\n",
      "Trained batch 1115 batch loss 1.22796488 epoch total loss 1.37418103\n",
      "Trained batch 1116 batch loss 1.22459292 epoch total loss 1.37404704\n",
      "Trained batch 1117 batch loss 1.25117779 epoch total loss 1.37393713\n",
      "Trained batch 1118 batch loss 1.23176384 epoch total loss 1.37380993\n",
      "Trained batch 1119 batch loss 1.22850168 epoch total loss 1.37368011\n",
      "Trained batch 1120 batch loss 1.27095962 epoch total loss 1.37358844\n",
      "Trained batch 1121 batch loss 1.27914643 epoch total loss 1.37350428\n",
      "Trained batch 1122 batch loss 1.31865227 epoch total loss 1.37345529\n",
      "Trained batch 1123 batch loss 1.27290058 epoch total loss 1.37336576\n",
      "Trained batch 1124 batch loss 1.41143298 epoch total loss 1.37339962\n",
      "Trained batch 1125 batch loss 1.28459954 epoch total loss 1.37332058\n",
      "Trained batch 1126 batch loss 1.34374642 epoch total loss 1.37329435\n",
      "Trained batch 1127 batch loss 1.33590984 epoch total loss 1.37326121\n",
      "Trained batch 1128 batch loss 1.32768297 epoch total loss 1.3732208\n",
      "Trained batch 1129 batch loss 1.33157361 epoch total loss 1.37318385\n",
      "Trained batch 1130 batch loss 1.20633125 epoch total loss 1.37303615\n",
      "Trained batch 1131 batch loss 1.17744243 epoch total loss 1.37286329\n",
      "Trained batch 1132 batch loss 1.10544205 epoch total loss 1.37262702\n",
      "Trained batch 1133 batch loss 1.19533992 epoch total loss 1.37247062\n",
      "Trained batch 1134 batch loss 1.28611362 epoch total loss 1.37239444\n",
      "Trained batch 1135 batch loss 1.45347595 epoch total loss 1.37246585\n",
      "Trained batch 1136 batch loss 1.49210393 epoch total loss 1.37257111\n",
      "Trained batch 1137 batch loss 1.44930649 epoch total loss 1.3726387\n",
      "Trained batch 1138 batch loss 1.29589891 epoch total loss 1.37257123\n",
      "Trained batch 1139 batch loss 1.25272441 epoch total loss 1.37246597\n",
      "Trained batch 1140 batch loss 1.42616653 epoch total loss 1.37251306\n",
      "Trained batch 1141 batch loss 1.2090627 epoch total loss 1.37236989\n",
      "Trained batch 1142 batch loss 1.44156265 epoch total loss 1.37243044\n",
      "Trained batch 1143 batch loss 1.40561938 epoch total loss 1.37245953\n",
      "Trained batch 1144 batch loss 1.52486861 epoch total loss 1.37259269\n",
      "Trained batch 1145 batch loss 1.42745364 epoch total loss 1.37264073\n",
      "Trained batch 1146 batch loss 1.38327241 epoch total loss 1.37265\n",
      "Trained batch 1147 batch loss 1.36992824 epoch total loss 1.37264752\n",
      "Trained batch 1148 batch loss 1.31110322 epoch total loss 1.372594\n",
      "Trained batch 1149 batch loss 1.21217597 epoch total loss 1.3724544\n",
      "Trained batch 1150 batch loss 1.17440152 epoch total loss 1.37228215\n",
      "Trained batch 1151 batch loss 1.37667394 epoch total loss 1.37228608\n",
      "Trained batch 1152 batch loss 1.57459176 epoch total loss 1.37246168\n",
      "Trained batch 1153 batch loss 1.41987824 epoch total loss 1.3725028\n",
      "Trained batch 1154 batch loss 1.40833855 epoch total loss 1.3725338\n",
      "Trained batch 1155 batch loss 1.3761 epoch total loss 1.3725369\n",
      "Trained batch 1156 batch loss 1.3710022 epoch total loss 1.37253559\n",
      "Trained batch 1157 batch loss 1.2916894 epoch total loss 1.37246573\n",
      "Trained batch 1158 batch loss 1.38318455 epoch total loss 1.37247503\n",
      "Trained batch 1159 batch loss 1.38307285 epoch total loss 1.37248409\n",
      "Trained batch 1160 batch loss 1.31394958 epoch total loss 1.37243366\n",
      "Trained batch 1161 batch loss 1.35870123 epoch total loss 1.37242186\n",
      "Trained batch 1162 batch loss 1.33980942 epoch total loss 1.37239373\n",
      "Trained batch 1163 batch loss 1.21856713 epoch total loss 1.37226152\n",
      "Trained batch 1164 batch loss 1.25489974 epoch total loss 1.37216067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1165 batch loss 1.1740222 epoch total loss 1.37199068\n",
      "Trained batch 1166 batch loss 1.11283684 epoch total loss 1.37176836\n",
      "Trained batch 1167 batch loss 1.19911492 epoch total loss 1.37162042\n",
      "Trained batch 1168 batch loss 1.44134974 epoch total loss 1.37168014\n",
      "Trained batch 1169 batch loss 1.4458369 epoch total loss 1.37174356\n",
      "Trained batch 1170 batch loss 1.58223033 epoch total loss 1.37192357\n",
      "Trained batch 1171 batch loss 1.55653095 epoch total loss 1.37208116\n",
      "Trained batch 1172 batch loss 1.58812 epoch total loss 1.37226546\n",
      "Trained batch 1173 batch loss 1.55560124 epoch total loss 1.37242174\n",
      "Trained batch 1174 batch loss 1.43992424 epoch total loss 1.3724792\n",
      "Trained batch 1175 batch loss 1.21010959 epoch total loss 1.37234104\n",
      "Trained batch 1176 batch loss 1.20929146 epoch total loss 1.3722024\n",
      "Trained batch 1177 batch loss 1.30850184 epoch total loss 1.37214828\n",
      "Trained batch 1178 batch loss 1.35221124 epoch total loss 1.37213135\n",
      "Trained batch 1179 batch loss 1.26150453 epoch total loss 1.37203753\n",
      "Trained batch 1180 batch loss 1.39652014 epoch total loss 1.37205815\n",
      "Trained batch 1181 batch loss 1.47818244 epoch total loss 1.37214804\n",
      "Trained batch 1182 batch loss 1.47168648 epoch total loss 1.3722322\n",
      "Trained batch 1183 batch loss 1.51848662 epoch total loss 1.37235582\n",
      "Trained batch 1184 batch loss 1.24916077 epoch total loss 1.37225175\n",
      "Trained batch 1185 batch loss 1.34112167 epoch total loss 1.3722254\n",
      "Trained batch 1186 batch loss 1.3052336 epoch total loss 1.3721689\n",
      "Trained batch 1187 batch loss 1.29427934 epoch total loss 1.37210333\n",
      "Trained batch 1188 batch loss 1.3606751 epoch total loss 1.3720938\n",
      "Trained batch 1189 batch loss 1.33861351 epoch total loss 1.37206554\n",
      "Trained batch 1190 batch loss 1.2724489 epoch total loss 1.37198186\n",
      "Trained batch 1191 batch loss 1.26122594 epoch total loss 1.37188888\n",
      "Trained batch 1192 batch loss 1.19451022 epoch total loss 1.3717401\n",
      "Trained batch 1193 batch loss 1.17426491 epoch total loss 1.37157452\n",
      "Trained batch 1194 batch loss 1.16598213 epoch total loss 1.37140238\n",
      "Trained batch 1195 batch loss 1.27037287 epoch total loss 1.37131786\n",
      "Trained batch 1196 batch loss 1.13873696 epoch total loss 1.37112343\n",
      "Trained batch 1197 batch loss 1.24598241 epoch total loss 1.37101889\n",
      "Trained batch 1198 batch loss 1.25195074 epoch total loss 1.37091947\n",
      "Trained batch 1199 batch loss 1.29225135 epoch total loss 1.3708539\n",
      "Trained batch 1200 batch loss 1.28496766 epoch total loss 1.37078226\n",
      "Trained batch 1201 batch loss 1.34549546 epoch total loss 1.37076116\n",
      "Trained batch 1202 batch loss 1.30165172 epoch total loss 1.3707037\n",
      "Trained batch 1203 batch loss 1.29144573 epoch total loss 1.37063789\n",
      "Trained batch 1204 batch loss 1.24450803 epoch total loss 1.37053311\n",
      "Trained batch 1205 batch loss 1.14435363 epoch total loss 1.37034547\n",
      "Trained batch 1206 batch loss 1.30252314 epoch total loss 1.37028921\n",
      "Trained batch 1207 batch loss 1.23972809 epoch total loss 1.37018096\n",
      "Trained batch 1208 batch loss 1.39325452 epoch total loss 1.37020016\n",
      "Trained batch 1209 batch loss 1.44809246 epoch total loss 1.37026465\n",
      "Trained batch 1210 batch loss 1.4954716 epoch total loss 1.37036812\n",
      "Trained batch 1211 batch loss 1.45112252 epoch total loss 1.37043476\n",
      "Trained batch 1212 batch loss 1.29311419 epoch total loss 1.37037098\n",
      "Trained batch 1213 batch loss 1.27594471 epoch total loss 1.37029314\n",
      "Trained batch 1214 batch loss 1.18708503 epoch total loss 1.37014234\n",
      "Trained batch 1215 batch loss 1.36619246 epoch total loss 1.37013912\n",
      "Trained batch 1216 batch loss 1.36889374 epoch total loss 1.37013805\n",
      "Trained batch 1217 batch loss 1.34243822 epoch total loss 1.37011528\n",
      "Trained batch 1218 batch loss 1.26652265 epoch total loss 1.37003016\n",
      "Trained batch 1219 batch loss 1.18499506 epoch total loss 1.36987841\n",
      "Trained batch 1220 batch loss 1.19002068 epoch total loss 1.36973095\n",
      "Trained batch 1221 batch loss 1.28143883 epoch total loss 1.36965871\n",
      "Trained batch 1222 batch loss 1.4178828 epoch total loss 1.36969817\n",
      "Trained batch 1223 batch loss 1.49848127 epoch total loss 1.36980343\n",
      "Trained batch 1224 batch loss 1.52001309 epoch total loss 1.36992621\n",
      "Trained batch 1225 batch loss 1.49758172 epoch total loss 1.3700304\n",
      "Trained batch 1226 batch loss 1.39133883 epoch total loss 1.37004781\n",
      "Trained batch 1227 batch loss 1.420017 epoch total loss 1.37008858\n",
      "Trained batch 1228 batch loss 1.27314305 epoch total loss 1.37000966\n",
      "Trained batch 1229 batch loss 1.20121908 epoch total loss 1.36987221\n",
      "Trained batch 1230 batch loss 1.31972492 epoch total loss 1.36983144\n",
      "Trained batch 1231 batch loss 1.37161696 epoch total loss 1.36983287\n",
      "Trained batch 1232 batch loss 1.37058663 epoch total loss 1.36983347\n",
      "Trained batch 1233 batch loss 1.34120822 epoch total loss 1.36981022\n",
      "Trained batch 1234 batch loss 1.33091271 epoch total loss 1.36977875\n",
      "Trained batch 1235 batch loss 1.30846334 epoch total loss 1.36972916\n",
      "Trained batch 1236 batch loss 1.37217033 epoch total loss 1.36973107\n",
      "Trained batch 1237 batch loss 1.34476638 epoch total loss 1.36971092\n",
      "Trained batch 1238 batch loss 1.40716445 epoch total loss 1.36974108\n",
      "Trained batch 1239 batch loss 1.40816617 epoch total loss 1.3697722\n",
      "Trained batch 1240 batch loss 1.30072451 epoch total loss 1.36971653\n",
      "Trained batch 1241 batch loss 1.51480532 epoch total loss 1.36983335\n",
      "Trained batch 1242 batch loss 1.57290053 epoch total loss 1.36999691\n",
      "Trained batch 1243 batch loss 1.37860882 epoch total loss 1.37000382\n",
      "Trained batch 1244 batch loss 1.3136946 epoch total loss 1.36995864\n",
      "Trained batch 1245 batch loss 1.22145736 epoch total loss 1.36983931\n",
      "Trained batch 1246 batch loss 1.10202694 epoch total loss 1.36962438\n",
      "Trained batch 1247 batch loss 1.24902797 epoch total loss 1.3695277\n",
      "Trained batch 1248 batch loss 1.29852974 epoch total loss 1.36947083\n",
      "Trained batch 1249 batch loss 1.04798448 epoch total loss 1.36921346\n",
      "Trained batch 1250 batch loss 1.06587052 epoch total loss 1.36897075\n",
      "Trained batch 1251 batch loss 1.11431134 epoch total loss 1.36876714\n",
      "Trained batch 1252 batch loss 1.1826067 epoch total loss 1.36861849\n",
      "Trained batch 1253 batch loss 1.18552017 epoch total loss 1.36847246\n",
      "Trained batch 1254 batch loss 1.34813237 epoch total loss 1.36845624\n",
      "Trained batch 1255 batch loss 1.37362814 epoch total loss 1.3684603\n",
      "Trained batch 1256 batch loss 1.43394017 epoch total loss 1.36851251\n",
      "Trained batch 1257 batch loss 1.40878248 epoch total loss 1.36854458\n",
      "Trained batch 1258 batch loss 1.51146746 epoch total loss 1.36865819\n",
      "Trained batch 1259 batch loss 1.41408706 epoch total loss 1.36869419\n",
      "Trained batch 1260 batch loss 1.32948411 epoch total loss 1.36866307\n",
      "Trained batch 1261 batch loss 1.35372472 epoch total loss 1.36865127\n",
      "Trained batch 1262 batch loss 1.37836277 epoch total loss 1.36865902\n",
      "Trained batch 1263 batch loss 1.37946129 epoch total loss 1.3686676\n",
      "Trained batch 1264 batch loss 1.36943078 epoch total loss 1.3686682\n",
      "Trained batch 1265 batch loss 1.32738352 epoch total loss 1.36863554\n",
      "Trained batch 1266 batch loss 1.37125397 epoch total loss 1.36863756\n",
      "Trained batch 1267 batch loss 1.37844872 epoch total loss 1.36864531\n",
      "Trained batch 1268 batch loss 1.3160162 epoch total loss 1.36860383\n",
      "Trained batch 1269 batch loss 1.40465176 epoch total loss 1.3686322\n",
      "Trained batch 1270 batch loss 1.41480494 epoch total loss 1.36866856\n",
      "Trained batch 1271 batch loss 1.41306067 epoch total loss 1.36870348\n",
      "Trained batch 1272 batch loss 1.21485174 epoch total loss 1.36858261\n",
      "Trained batch 1273 batch loss 1.20915341 epoch total loss 1.36845732\n",
      "Trained batch 1274 batch loss 1.15904355 epoch total loss 1.36829293\n",
      "Trained batch 1275 batch loss 1.16487658 epoch total loss 1.36813343\n",
      "Trained batch 1276 batch loss 1.23170745 epoch total loss 1.36802649\n",
      "Trained batch 1277 batch loss 1.32758403 epoch total loss 1.3679949\n",
      "Trained batch 1278 batch loss 1.2255168 epoch total loss 1.36788332\n",
      "Trained batch 1279 batch loss 1.18288314 epoch total loss 1.36773872\n",
      "Trained batch 1280 batch loss 1.2084049 epoch total loss 1.36761415\n",
      "Trained batch 1281 batch loss 1.38337588 epoch total loss 1.36762655\n",
      "Trained batch 1282 batch loss 1.40337098 epoch total loss 1.36765432\n",
      "Trained batch 1283 batch loss 1.25421202 epoch total loss 1.36756599\n",
      "Trained batch 1284 batch loss 1.39729428 epoch total loss 1.36758912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1285 batch loss 1.33982825 epoch total loss 1.36756754\n",
      "Trained batch 1286 batch loss 1.37868392 epoch total loss 1.36757624\n",
      "Trained batch 1287 batch loss 1.3401165 epoch total loss 1.3675549\n",
      "Trained batch 1288 batch loss 1.43955791 epoch total loss 1.36761081\n",
      "Trained batch 1289 batch loss 1.44506085 epoch total loss 1.36767089\n",
      "Trained batch 1290 batch loss 1.31363356 epoch total loss 1.36762893\n",
      "Trained batch 1291 batch loss 1.20045221 epoch total loss 1.36749947\n",
      "Trained batch 1292 batch loss 1.46502566 epoch total loss 1.36757493\n",
      "Trained batch 1293 batch loss 1.30238831 epoch total loss 1.36752439\n",
      "Trained batch 1294 batch loss 1.32369 epoch total loss 1.36749065\n",
      "Trained batch 1295 batch loss 1.38265264 epoch total loss 1.36750233\n",
      "Trained batch 1296 batch loss 1.30011487 epoch total loss 1.36745036\n",
      "Trained batch 1297 batch loss 1.40238309 epoch total loss 1.3674773\n",
      "Trained batch 1298 batch loss 1.23818064 epoch total loss 1.36737764\n",
      "Trained batch 1299 batch loss 1.47629607 epoch total loss 1.36746156\n",
      "Trained batch 1300 batch loss 1.22862434 epoch total loss 1.36735475\n",
      "Trained batch 1301 batch loss 1.34921134 epoch total loss 1.3673408\n",
      "Trained batch 1302 batch loss 1.20786166 epoch total loss 1.36721838\n",
      "Trained batch 1303 batch loss 1.39427924 epoch total loss 1.36723912\n",
      "Trained batch 1304 batch loss 1.4284246 epoch total loss 1.36728609\n",
      "Trained batch 1305 batch loss 1.25328517 epoch total loss 1.36719871\n",
      "Trained batch 1306 batch loss 1.28370452 epoch total loss 1.36713481\n",
      "Trained batch 1307 batch loss 1.3167758 epoch total loss 1.3670963\n",
      "Trained batch 1308 batch loss 1.22954035 epoch total loss 1.36699104\n",
      "Trained batch 1309 batch loss 1.26681781 epoch total loss 1.36691451\n",
      "Trained batch 1310 batch loss 1.30962086 epoch total loss 1.36687076\n",
      "Trained batch 1311 batch loss 1.30291963 epoch total loss 1.366822\n",
      "Trained batch 1312 batch loss 1.1990968 epoch total loss 1.36669421\n",
      "Trained batch 1313 batch loss 1.36440408 epoch total loss 1.36669242\n",
      "Trained batch 1314 batch loss 1.23326635 epoch total loss 1.36659086\n",
      "Trained batch 1315 batch loss 1.21893811 epoch total loss 1.36647868\n",
      "Trained batch 1316 batch loss 1.36834455 epoch total loss 1.36648\n",
      "Trained batch 1317 batch loss 1.20215547 epoch total loss 1.3663553\n",
      "Trained batch 1318 batch loss 1.22471488 epoch total loss 1.36624777\n",
      "Trained batch 1319 batch loss 1.30067706 epoch total loss 1.36619806\n",
      "Trained batch 1320 batch loss 1.28964686 epoch total loss 1.36614013\n",
      "Trained batch 1321 batch loss 1.37190652 epoch total loss 1.36614454\n",
      "Trained batch 1322 batch loss 1.30362737 epoch total loss 1.36609721\n",
      "Trained batch 1323 batch loss 1.30212092 epoch total loss 1.36604881\n",
      "Trained batch 1324 batch loss 1.29498029 epoch total loss 1.36599505\n",
      "Trained batch 1325 batch loss 1.23763192 epoch total loss 1.36589825\n",
      "Trained batch 1326 batch loss 1.28304911 epoch total loss 1.36583579\n",
      "Trained batch 1327 batch loss 1.21994734 epoch total loss 1.36572587\n",
      "Trained batch 1328 batch loss 1.41520238 epoch total loss 1.36576307\n",
      "Trained batch 1329 batch loss 1.57954645 epoch total loss 1.365924\n",
      "Trained batch 1330 batch loss 1.59113145 epoch total loss 1.3660934\n",
      "Trained batch 1331 batch loss 1.45819116 epoch total loss 1.36616266\n",
      "Trained batch 1332 batch loss 1.47809327 epoch total loss 1.3662467\n",
      "Trained batch 1333 batch loss 1.42429543 epoch total loss 1.36629021\n",
      "Trained batch 1334 batch loss 1.36929536 epoch total loss 1.36629248\n",
      "Trained batch 1335 batch loss 1.4136939 epoch total loss 1.366328\n",
      "Trained batch 1336 batch loss 1.2675153 epoch total loss 1.36625397\n",
      "Trained batch 1337 batch loss 1.38926148 epoch total loss 1.36627114\n",
      "Trained batch 1338 batch loss 1.31424165 epoch total loss 1.36623228\n",
      "Trained batch 1339 batch loss 1.3109 epoch total loss 1.36619103\n",
      "Trained batch 1340 batch loss 1.40000081 epoch total loss 1.36621618\n",
      "Trained batch 1341 batch loss 1.40475416 epoch total loss 1.36624503\n",
      "Trained batch 1342 batch loss 1.25992584 epoch total loss 1.36616576\n",
      "Trained batch 1343 batch loss 1.30541539 epoch total loss 1.36612046\n",
      "Trained batch 1344 batch loss 1.25112295 epoch total loss 1.36603487\n",
      "Trained batch 1345 batch loss 1.30229163 epoch total loss 1.36598754\n",
      "Trained batch 1346 batch loss 1.29872298 epoch total loss 1.36593747\n",
      "Trained batch 1347 batch loss 1.34563243 epoch total loss 1.36592233\n",
      "Trained batch 1348 batch loss 1.35837746 epoch total loss 1.36591685\n",
      "Trained batch 1349 batch loss 1.24021125 epoch total loss 1.36582363\n",
      "Trained batch 1350 batch loss 1.2529676 epoch total loss 1.36574006\n",
      "Trained batch 1351 batch loss 1.26867533 epoch total loss 1.36566818\n",
      "Trained batch 1352 batch loss 1.28029561 epoch total loss 1.365605\n",
      "Trained batch 1353 batch loss 1.25173318 epoch total loss 1.36552083\n",
      "Trained batch 1354 batch loss 1.15734851 epoch total loss 1.36536705\n",
      "Trained batch 1355 batch loss 1.19222128 epoch total loss 1.36523938\n",
      "Trained batch 1356 batch loss 1.1984508 epoch total loss 1.36511636\n",
      "Trained batch 1357 batch loss 1.23054194 epoch total loss 1.36501718\n",
      "Trained batch 1358 batch loss 1.2226131 epoch total loss 1.36491239\n",
      "Trained batch 1359 batch loss 1.1932925 epoch total loss 1.36478603\n",
      "Trained batch 1360 batch loss 1.28023052 epoch total loss 1.36472392\n",
      "Trained batch 1361 batch loss 1.32384431 epoch total loss 1.36469388\n",
      "Trained batch 1362 batch loss 1.31028438 epoch total loss 1.36465394\n",
      "Trained batch 1363 batch loss 1.53668213 epoch total loss 1.36478007\n",
      "Trained batch 1364 batch loss 1.36867821 epoch total loss 1.36478293\n",
      "Trained batch 1365 batch loss 1.42834282 epoch total loss 1.36482954\n",
      "Trained batch 1366 batch loss 1.31885469 epoch total loss 1.36479592\n",
      "Trained batch 1367 batch loss 1.18605077 epoch total loss 1.36466515\n",
      "Trained batch 1368 batch loss 1.24441087 epoch total loss 1.36457717\n",
      "Trained batch 1369 batch loss 1.30474412 epoch total loss 1.36453342\n",
      "Trained batch 1370 batch loss 1.20542121 epoch total loss 1.36441731\n",
      "Trained batch 1371 batch loss 1.26612329 epoch total loss 1.36434555\n",
      "Trained batch 1372 batch loss 1.21179247 epoch total loss 1.36423445\n",
      "Trained batch 1373 batch loss 1.17104673 epoch total loss 1.36409366\n",
      "Trained batch 1374 batch loss 1.17802262 epoch total loss 1.36395824\n",
      "Trained batch 1375 batch loss 1.20639944 epoch total loss 1.36384368\n",
      "Trained batch 1376 batch loss 1.29474974 epoch total loss 1.36379349\n",
      "Trained batch 1377 batch loss 1.17937648 epoch total loss 1.3636595\n",
      "Trained batch 1378 batch loss 1.28366947 epoch total loss 1.36360145\n",
      "Trained batch 1379 batch loss 1.19987679 epoch total loss 1.36348271\n",
      "Trained batch 1380 batch loss 1.45735145 epoch total loss 1.36355078\n",
      "Trained batch 1381 batch loss 1.45265841 epoch total loss 1.36361527\n",
      "Trained batch 1382 batch loss 1.35542965 epoch total loss 1.36360943\n",
      "Trained batch 1383 batch loss 1.38309324 epoch total loss 1.3636235\n",
      "Trained batch 1384 batch loss 1.43790007 epoch total loss 1.36367714\n",
      "Trained batch 1385 batch loss 1.44007313 epoch total loss 1.36373222\n",
      "Trained batch 1386 batch loss 1.31066632 epoch total loss 1.36369395\n",
      "Trained batch 1387 batch loss 1.32189393 epoch total loss 1.36366379\n",
      "Trained batch 1388 batch loss 1.24879813 epoch total loss 1.36358106\n",
      "Epoch 2 train loss 1.3635810613632202\n",
      "Validated batch 1 batch loss 1.38259125\n",
      "Validated batch 2 batch loss 1.29708827\n",
      "Validated batch 3 batch loss 1.25250256\n",
      "Validated batch 4 batch loss 1.26216841\n",
      "Validated batch 5 batch loss 1.288432\n",
      "Validated batch 6 batch loss 1.35797274\n",
      "Validated batch 7 batch loss 1.31661248\n",
      "Validated batch 8 batch loss 1.26259089\n",
      "Validated batch 9 batch loss 1.3802979\n",
      "Validated batch 10 batch loss 1.33304739\n",
      "Validated batch 11 batch loss 1.25995135\n",
      "Validated batch 12 batch loss 1.21544492\n",
      "Validated batch 13 batch loss 1.33247101\n",
      "Validated batch 14 batch loss 1.36655354\n",
      "Validated batch 15 batch loss 1.48345077\n",
      "Validated batch 16 batch loss 1.41942823\n",
      "Validated batch 17 batch loss 1.31956792\n",
      "Validated batch 18 batch loss 1.43932736\n",
      "Validated batch 19 batch loss 1.32813728\n",
      "Validated batch 20 batch loss 1.34544683\n",
      "Validated batch 21 batch loss 1.38829279\n",
      "Validated batch 22 batch loss 1.11058688\n",
      "Validated batch 23 batch loss 1.3590281\n",
      "Validated batch 24 batch loss 1.37325668\n",
      "Validated batch 25 batch loss 1.33730602\n",
      "Validated batch 26 batch loss 1.27775812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 27 batch loss 1.29693651\n",
      "Validated batch 28 batch loss 1.37023735\n",
      "Validated batch 29 batch loss 1.4805584\n",
      "Validated batch 30 batch loss 1.25028872\n",
      "Validated batch 31 batch loss 1.35898674\n",
      "Validated batch 32 batch loss 1.34575582\n",
      "Validated batch 33 batch loss 1.35631454\n",
      "Validated batch 34 batch loss 1.35249472\n",
      "Validated batch 35 batch loss 1.21410513\n",
      "Validated batch 36 batch loss 1.46163094\n",
      "Validated batch 37 batch loss 1.23227584\n",
      "Validated batch 38 batch loss 1.36388898\n",
      "Validated batch 39 batch loss 1.35386157\n",
      "Validated batch 40 batch loss 1.42848265\n",
      "Validated batch 41 batch loss 1.2348516\n",
      "Validated batch 42 batch loss 1.33606684\n",
      "Validated batch 43 batch loss 1.2643981\n",
      "Validated batch 44 batch loss 1.35896158\n",
      "Validated batch 45 batch loss 1.3092072\n",
      "Validated batch 46 batch loss 1.34451592\n",
      "Validated batch 47 batch loss 1.46496534\n",
      "Validated batch 48 batch loss 1.36583304\n",
      "Validated batch 49 batch loss 1.28618789\n",
      "Validated batch 50 batch loss 1.27815318\n",
      "Validated batch 51 batch loss 1.28637242\n",
      "Validated batch 52 batch loss 1.33706856\n",
      "Validated batch 53 batch loss 1.36481977\n",
      "Validated batch 54 batch loss 1.26722097\n",
      "Validated batch 55 batch loss 1.35843766\n",
      "Validated batch 56 batch loss 1.32760811\n",
      "Validated batch 57 batch loss 1.3621161\n",
      "Validated batch 58 batch loss 1.35756218\n",
      "Validated batch 59 batch loss 1.36326218\n",
      "Validated batch 60 batch loss 1.26723242\n",
      "Validated batch 61 batch loss 1.31546247\n",
      "Validated batch 62 batch loss 1.33800805\n",
      "Validated batch 63 batch loss 1.31148958\n",
      "Validated batch 64 batch loss 1.355528\n",
      "Validated batch 65 batch loss 1.42582715\n",
      "Validated batch 66 batch loss 1.55461824\n",
      "Validated batch 67 batch loss 1.39806259\n",
      "Validated batch 68 batch loss 1.38712966\n",
      "Validated batch 69 batch loss 1.19811594\n",
      "Validated batch 70 batch loss 1.27241075\n",
      "Validated batch 71 batch loss 1.38092113\n",
      "Validated batch 72 batch loss 1.23895454\n",
      "Validated batch 73 batch loss 1.28277433\n",
      "Validated batch 74 batch loss 1.32295799\n",
      "Validated batch 75 batch loss 1.37066185\n",
      "Validated batch 76 batch loss 1.41463923\n",
      "Validated batch 77 batch loss 1.43866217\n",
      "Validated batch 78 batch loss 1.35581088\n",
      "Validated batch 79 batch loss 1.34679854\n",
      "Validated batch 80 batch loss 1.38384295\n",
      "Validated batch 81 batch loss 1.28168976\n",
      "Validated batch 82 batch loss 1.32167935\n",
      "Validated batch 83 batch loss 1.46352661\n",
      "Validated batch 84 batch loss 1.3924911\n",
      "Validated batch 85 batch loss 1.34740305\n",
      "Validated batch 86 batch loss 1.47826385\n",
      "Validated batch 87 batch loss 1.18250561\n",
      "Validated batch 88 batch loss 1.35515702\n",
      "Validated batch 89 batch loss 1.17449832\n",
      "Validated batch 90 batch loss 1.29044831\n",
      "Validated batch 91 batch loss 1.43169117\n",
      "Validated batch 92 batch loss 1.27420211\n",
      "Validated batch 93 batch loss 1.35460711\n",
      "Validated batch 94 batch loss 1.18129802\n",
      "Validated batch 95 batch loss 1.31806195\n",
      "Validated batch 96 batch loss 1.25061965\n",
      "Validated batch 97 batch loss 1.29163933\n",
      "Validated batch 98 batch loss 1.30142021\n",
      "Validated batch 99 batch loss 1.34382713\n",
      "Validated batch 100 batch loss 1.31375146\n",
      "Validated batch 101 batch loss 1.37047434\n",
      "Validated batch 102 batch loss 1.26924562\n",
      "Validated batch 103 batch loss 1.38195193\n",
      "Validated batch 104 batch loss 1.35554612\n",
      "Validated batch 105 batch loss 1.24242163\n",
      "Validated batch 106 batch loss 1.25266957\n",
      "Validated batch 107 batch loss 1.3483485\n",
      "Validated batch 108 batch loss 1.28522205\n",
      "Validated batch 109 batch loss 1.47262013\n",
      "Validated batch 110 batch loss 1.3634305\n",
      "Validated batch 111 batch loss 1.29745519\n",
      "Validated batch 112 batch loss 1.40095472\n",
      "Validated batch 113 batch loss 1.30836248\n",
      "Validated batch 114 batch loss 1.2515583\n",
      "Validated batch 115 batch loss 1.29944146\n",
      "Validated batch 116 batch loss 1.32457876\n",
      "Validated batch 117 batch loss 1.25509727\n",
      "Validated batch 118 batch loss 1.31553912\n",
      "Validated batch 119 batch loss 1.23629355\n",
      "Validated batch 120 batch loss 1.25236785\n",
      "Validated batch 121 batch loss 1.44711041\n",
      "Validated batch 122 batch loss 1.34973121\n",
      "Validated batch 123 batch loss 1.384776\n",
      "Validated batch 124 batch loss 1.41682959\n",
      "Validated batch 125 batch loss 1.37881231\n",
      "Validated batch 126 batch loss 1.34892583\n",
      "Validated batch 127 batch loss 1.44898343\n",
      "Validated batch 128 batch loss 1.35916018\n",
      "Validated batch 129 batch loss 1.40478277\n",
      "Validated batch 130 batch loss 1.40771067\n",
      "Validated batch 131 batch loss 1.45132565\n",
      "Validated batch 132 batch loss 1.40352917\n",
      "Validated batch 133 batch loss 1.26123118\n",
      "Validated batch 134 batch loss 1.33662164\n",
      "Validated batch 135 batch loss 1.36242795\n",
      "Validated batch 136 batch loss 1.4158144\n",
      "Validated batch 137 batch loss 1.29223442\n",
      "Validated batch 138 batch loss 1.35672283\n",
      "Validated batch 139 batch loss 1.28969216\n",
      "Validated batch 140 batch loss 1.30753326\n",
      "Validated batch 141 batch loss 1.45462561\n",
      "Validated batch 142 batch loss 1.28902221\n",
      "Validated batch 143 batch loss 1.38374031\n",
      "Validated batch 144 batch loss 1.50337982\n",
      "Validated batch 145 batch loss 1.20741868\n",
      "Validated batch 146 batch loss 1.33234406\n",
      "Validated batch 147 batch loss 1.32274413\n",
      "Validated batch 148 batch loss 1.3656162\n",
      "Validated batch 149 batch loss 1.37534547\n",
      "Validated batch 150 batch loss 1.31613624\n",
      "Validated batch 151 batch loss 1.10806048\n",
      "Validated batch 152 batch loss 1.31158888\n",
      "Validated batch 153 batch loss 1.26538467\n",
      "Validated batch 154 batch loss 1.32477856\n",
      "Validated batch 155 batch loss 1.35667539\n",
      "Validated batch 156 batch loss 1.22023439\n",
      "Validated batch 157 batch loss 1.33403277\n",
      "Validated batch 158 batch loss 1.40884447\n",
      "Validated batch 159 batch loss 1.34397018\n",
      "Validated batch 160 batch loss 1.30905\n",
      "Validated batch 161 batch loss 1.24714422\n",
      "Validated batch 162 batch loss 1.2822361\n",
      "Validated batch 163 batch loss 1.35468078\n",
      "Validated batch 164 batch loss 1.33885407\n",
      "Validated batch 165 batch loss 1.20500898\n",
      "Validated batch 166 batch loss 1.29032242\n",
      "Validated batch 167 batch loss 1.38617945\n",
      "Validated batch 168 batch loss 1.22975075\n",
      "Validated batch 169 batch loss 1.22466791\n",
      "Validated batch 170 batch loss 1.23726392\n",
      "Validated batch 171 batch loss 1.29647684\n",
      "Validated batch 172 batch loss 1.20223236\n",
      "Validated batch 173 batch loss 1.30559397\n",
      "Validated batch 174 batch loss 1.22481549\n",
      "Validated batch 175 batch loss 1.36940777\n",
      "Validated batch 176 batch loss 1.3768115\n",
      "Validated batch 177 batch loss 1.41831231\n",
      "Validated batch 178 batch loss 1.2785114\n",
      "Validated batch 179 batch loss 1.46169138\n",
      "Validated batch 180 batch loss 1.17271733\n",
      "Validated batch 181 batch loss 1.16138577\n",
      "Validated batch 182 batch loss 1.35130906\n",
      "Validated batch 183 batch loss 1.29400742\n",
      "Validated batch 184 batch loss 1.43345857\n",
      "Validated batch 185 batch loss 1.44761288\n",
      "Epoch 2 val loss 1.3306076526641846\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-2-loss-1.3306.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.42421699 epoch total loss 1.42421699\n",
      "Trained batch 2 batch loss 1.31995273 epoch total loss 1.37208486\n",
      "Trained batch 3 batch loss 1.33821881 epoch total loss 1.36079609\n",
      "Trained batch 4 batch loss 1.41609228 epoch total loss 1.3746202\n",
      "Trained batch 5 batch loss 1.35828328 epoch total loss 1.37135279\n",
      "Trained batch 6 batch loss 1.39261246 epoch total loss 1.37489605\n",
      "Trained batch 7 batch loss 1.27988029 epoch total loss 1.3613224\n",
      "Trained batch 8 batch loss 1.30288863 epoch total loss 1.35401821\n",
      "Trained batch 9 batch loss 1.36062193 epoch total loss 1.35475206\n",
      "Trained batch 10 batch loss 1.29637313 epoch total loss 1.34891415\n",
      "Trained batch 11 batch loss 1.31908274 epoch total loss 1.34620214\n",
      "Trained batch 12 batch loss 1.3330276 epoch total loss 1.34510422\n",
      "Trained batch 13 batch loss 1.36836338 epoch total loss 1.34689331\n",
      "Trained batch 14 batch loss 1.30452096 epoch total loss 1.34386671\n",
      "Trained batch 15 batch loss 1.21080554 epoch total loss 1.33499599\n",
      "Trained batch 16 batch loss 1.1376251 epoch total loss 1.32266033\n",
      "Trained batch 17 batch loss 1.12293983 epoch total loss 1.31091213\n",
      "Trained batch 18 batch loss 1.17060387 epoch total loss 1.30311728\n",
      "Trained batch 19 batch loss 1.27196908 epoch total loss 1.30147779\n",
      "Trained batch 20 batch loss 1.20314777 epoch total loss 1.29656136\n",
      "Trained batch 21 batch loss 1.16022217 epoch total loss 1.29006898\n",
      "Trained batch 22 batch loss 1.218261 epoch total loss 1.28680503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 23 batch loss 1.32672501 epoch total loss 1.28854072\n",
      "Trained batch 24 batch loss 1.08536768 epoch total loss 1.28007519\n",
      "Trained batch 25 batch loss 1.09729409 epoch total loss 1.27276385\n",
      "Trained batch 26 batch loss 1.04130232 epoch total loss 1.26386154\n",
      "Trained batch 27 batch loss 1.22990656 epoch total loss 1.262604\n",
      "Trained batch 28 batch loss 1.35456967 epoch total loss 1.26588845\n",
      "Trained batch 29 batch loss 1.25480068 epoch total loss 1.26550603\n",
      "Trained batch 30 batch loss 1.43643236 epoch total loss 1.27120364\n",
      "Trained batch 31 batch loss 1.32713699 epoch total loss 1.27300787\n",
      "Trained batch 32 batch loss 1.2073617 epoch total loss 1.27095652\n",
      "Trained batch 33 batch loss 1.28655994 epoch total loss 1.2714293\n",
      "Trained batch 34 batch loss 1.36021876 epoch total loss 1.27404082\n",
      "Trained batch 35 batch loss 1.2130959 epoch total loss 1.27229953\n",
      "Trained batch 36 batch loss 1.40482283 epoch total loss 1.27598071\n",
      "Trained batch 37 batch loss 1.39384365 epoch total loss 1.27916622\n",
      "Trained batch 38 batch loss 1.41346049 epoch total loss 1.2827003\n",
      "Trained batch 39 batch loss 1.38386035 epoch total loss 1.28529418\n",
      "Trained batch 40 batch loss 1.29998827 epoch total loss 1.28566146\n",
      "Trained batch 41 batch loss 1.33930564 epoch total loss 1.2869699\n",
      "Trained batch 42 batch loss 1.19582021 epoch total loss 1.2847997\n",
      "Trained batch 43 batch loss 1.19018745 epoch total loss 1.28259933\n",
      "Trained batch 44 batch loss 1.29952538 epoch total loss 1.28298402\n",
      "Trained batch 45 batch loss 1.21083117 epoch total loss 1.28138065\n",
      "Trained batch 46 batch loss 1.34975386 epoch total loss 1.28286707\n",
      "Trained batch 47 batch loss 1.30862737 epoch total loss 1.28341508\n",
      "Trained batch 48 batch loss 1.32309425 epoch total loss 1.2842418\n",
      "Trained batch 49 batch loss 1.19408882 epoch total loss 1.28240192\n",
      "Trained batch 50 batch loss 1.37628269 epoch total loss 1.28427947\n",
      "Trained batch 51 batch loss 1.36294484 epoch total loss 1.28582191\n",
      "Trained batch 52 batch loss 1.35523129 epoch total loss 1.28715682\n",
      "Trained batch 53 batch loss 1.26794875 epoch total loss 1.28679442\n",
      "Trained batch 54 batch loss 1.1329062 epoch total loss 1.28394461\n",
      "Trained batch 55 batch loss 1.21442282 epoch total loss 1.28268063\n",
      "Trained batch 56 batch loss 1.1649152 epoch total loss 1.28057766\n",
      "Trained batch 57 batch loss 1.15370345 epoch total loss 1.27835178\n",
      "Trained batch 58 batch loss 1.11465859 epoch total loss 1.2755295\n",
      "Trained batch 59 batch loss 1.15168214 epoch total loss 1.27343035\n",
      "Trained batch 60 batch loss 1.30289209 epoch total loss 1.27392149\n",
      "Trained batch 61 batch loss 1.21357942 epoch total loss 1.27293217\n",
      "Trained batch 62 batch loss 1.33716726 epoch total loss 1.27396822\n",
      "Trained batch 63 batch loss 1.34282148 epoch total loss 1.27506113\n",
      "Trained batch 64 batch loss 1.32985687 epoch total loss 1.27591729\n",
      "Trained batch 65 batch loss 1.3419311 epoch total loss 1.27693295\n",
      "Trained batch 66 batch loss 1.22177958 epoch total loss 1.2760973\n",
      "Trained batch 67 batch loss 1.30145657 epoch total loss 1.27647579\n",
      "Trained batch 68 batch loss 1.37910259 epoch total loss 1.2779851\n",
      "Trained batch 69 batch loss 1.40899658 epoch total loss 1.27988374\n",
      "Trained batch 70 batch loss 1.35619247 epoch total loss 1.28097391\n",
      "Trained batch 71 batch loss 1.34588647 epoch total loss 1.28188813\n",
      "Trained batch 72 batch loss 1.34451747 epoch total loss 1.28275812\n",
      "Trained batch 73 batch loss 1.4100641 epoch total loss 1.28450203\n",
      "Trained batch 74 batch loss 1.25603735 epoch total loss 1.28411734\n",
      "Trained batch 75 batch loss 1.3381331 epoch total loss 1.2848376\n",
      "Trained batch 76 batch loss 1.3869338 epoch total loss 1.28618085\n",
      "Trained batch 77 batch loss 1.38513219 epoch total loss 1.28746593\n",
      "Trained batch 78 batch loss 1.26228952 epoch total loss 1.28714323\n",
      "Trained batch 79 batch loss 1.35675 epoch total loss 1.28802431\n",
      "Trained batch 80 batch loss 1.34275138 epoch total loss 1.28870845\n",
      "Trained batch 81 batch loss 1.27130914 epoch total loss 1.28849363\n",
      "Trained batch 82 batch loss 1.27948964 epoch total loss 1.28838372\n",
      "Trained batch 83 batch loss 1.38706267 epoch total loss 1.28957272\n",
      "Trained batch 84 batch loss 1.25257206 epoch total loss 1.28913212\n",
      "Trained batch 85 batch loss 1.21850801 epoch total loss 1.28830123\n",
      "Trained batch 86 batch loss 1.22217488 epoch total loss 1.28753233\n",
      "Trained batch 87 batch loss 1.42594659 epoch total loss 1.28912342\n",
      "Trained batch 88 batch loss 1.49282658 epoch total loss 1.29143822\n",
      "Trained batch 89 batch loss 1.30026698 epoch total loss 1.2915374\n",
      "Trained batch 90 batch loss 1.30940318 epoch total loss 1.29173589\n",
      "Trained batch 91 batch loss 1.19325364 epoch total loss 1.29065371\n",
      "Trained batch 92 batch loss 1.20938087 epoch total loss 1.28977025\n",
      "Trained batch 93 batch loss 1.29225707 epoch total loss 1.28979707\n",
      "Trained batch 94 batch loss 1.22357965 epoch total loss 1.28909266\n",
      "Trained batch 95 batch loss 1.37555921 epoch total loss 1.2900027\n",
      "Trained batch 96 batch loss 1.33034968 epoch total loss 1.29042304\n",
      "Trained batch 97 batch loss 1.49960625 epoch total loss 1.29257953\n",
      "Trained batch 98 batch loss 1.38844252 epoch total loss 1.29355776\n",
      "Trained batch 99 batch loss 1.33744168 epoch total loss 1.2940011\n",
      "Trained batch 100 batch loss 1.33171582 epoch total loss 1.29437816\n",
      "Trained batch 101 batch loss 1.27792287 epoch total loss 1.29421532\n",
      "Trained batch 102 batch loss 1.23681986 epoch total loss 1.29365253\n",
      "Trained batch 103 batch loss 1.32759786 epoch total loss 1.29398203\n",
      "Trained batch 104 batch loss 1.31618619 epoch total loss 1.29419565\n",
      "Trained batch 105 batch loss 1.24151742 epoch total loss 1.2936939\n",
      "Trained batch 106 batch loss 1.26446927 epoch total loss 1.29341817\n",
      "Trained batch 107 batch loss 1.15889335 epoch total loss 1.29216087\n",
      "Trained batch 108 batch loss 1.25479913 epoch total loss 1.29181504\n",
      "Trained batch 109 batch loss 1.20587122 epoch total loss 1.29102659\n",
      "Trained batch 110 batch loss 1.26284611 epoch total loss 1.29077041\n",
      "Trained batch 111 batch loss 1.45999813 epoch total loss 1.29229486\n",
      "Trained batch 112 batch loss 1.58995712 epoch total loss 1.29495251\n",
      "Trained batch 113 batch loss 1.52295184 epoch total loss 1.29697025\n",
      "Trained batch 114 batch loss 1.37853551 epoch total loss 1.29768574\n",
      "Trained batch 115 batch loss 1.44515872 epoch total loss 1.29896808\n",
      "Trained batch 116 batch loss 1.3877269 epoch total loss 1.29973328\n",
      "Trained batch 117 batch loss 1.28510654 epoch total loss 1.29960823\n",
      "Trained batch 118 batch loss 1.18888342 epoch total loss 1.29866993\n",
      "Trained batch 119 batch loss 1.29989195 epoch total loss 1.29868031\n",
      "Trained batch 120 batch loss 1.38794589 epoch total loss 1.29942405\n",
      "Trained batch 121 batch loss 1.38197386 epoch total loss 1.30010629\n",
      "Trained batch 122 batch loss 1.30513453 epoch total loss 1.30014753\n",
      "Trained batch 123 batch loss 1.28543687 epoch total loss 1.30002785\n",
      "Trained batch 124 batch loss 1.20774281 epoch total loss 1.29928362\n",
      "Trained batch 125 batch loss 1.21633697 epoch total loss 1.2986201\n",
      "Trained batch 126 batch loss 1.2187525 epoch total loss 1.29798627\n",
      "Trained batch 127 batch loss 1.24748957 epoch total loss 1.29758859\n",
      "Trained batch 128 batch loss 1.21649885 epoch total loss 1.29695499\n",
      "Trained batch 129 batch loss 1.22846246 epoch total loss 1.29642415\n",
      "Trained batch 130 batch loss 1.25455427 epoch total loss 1.29610193\n",
      "Trained batch 131 batch loss 1.26283097 epoch total loss 1.29584801\n",
      "Trained batch 132 batch loss 1.20894933 epoch total loss 1.29518974\n",
      "Trained batch 133 batch loss 1.23562562 epoch total loss 1.29474187\n",
      "Trained batch 134 batch loss 1.2293427 epoch total loss 1.29425383\n",
      "Trained batch 135 batch loss 1.25878155 epoch total loss 1.29399109\n",
      "Trained batch 136 batch loss 1.31497788 epoch total loss 1.29414535\n",
      "Trained batch 137 batch loss 1.31194103 epoch total loss 1.29427516\n",
      "Trained batch 138 batch loss 1.32067275 epoch total loss 1.2944665\n",
      "Trained batch 139 batch loss 1.41642833 epoch total loss 1.295344\n",
      "Trained batch 140 batch loss 1.31332934 epoch total loss 1.29547238\n",
      "Trained batch 141 batch loss 1.4724797 epoch total loss 1.29672766\n",
      "Trained batch 142 batch loss 1.25064707 epoch total loss 1.29640317\n",
      "Trained batch 143 batch loss 1.17881489 epoch total loss 1.29558086\n",
      "Trained batch 144 batch loss 1.29581606 epoch total loss 1.29558253\n",
      "Trained batch 145 batch loss 1.34962618 epoch total loss 1.2959553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 146 batch loss 1.29817486 epoch total loss 1.29597044\n",
      "Trained batch 147 batch loss 1.41112232 epoch total loss 1.29675376\n",
      "Trained batch 148 batch loss 1.37044168 epoch total loss 1.29725158\n",
      "Trained batch 149 batch loss 1.35025692 epoch total loss 1.2976073\n",
      "Trained batch 150 batch loss 1.20088518 epoch total loss 1.2969625\n",
      "Trained batch 151 batch loss 1.38143432 epoch total loss 1.29752195\n",
      "Trained batch 152 batch loss 1.28072858 epoch total loss 1.29741144\n",
      "Trained batch 153 batch loss 1.21767259 epoch total loss 1.29689026\n",
      "Trained batch 154 batch loss 1.23635173 epoch total loss 1.29649723\n",
      "Trained batch 155 batch loss 1.24588633 epoch total loss 1.29617059\n",
      "Trained batch 156 batch loss 1.36521482 epoch total loss 1.29661322\n",
      "Trained batch 157 batch loss 1.3648417 epoch total loss 1.29704785\n",
      "Trained batch 158 batch loss 1.32645643 epoch total loss 1.29723394\n",
      "Trained batch 159 batch loss 1.29503274 epoch total loss 1.29722011\n",
      "Trained batch 160 batch loss 1.19023478 epoch total loss 1.29655147\n",
      "Trained batch 161 batch loss 1.21664906 epoch total loss 1.29605508\n",
      "Trained batch 162 batch loss 1.29439497 epoch total loss 1.29604483\n",
      "Trained batch 163 batch loss 1.23111117 epoch total loss 1.29564643\n",
      "Trained batch 164 batch loss 1.23006833 epoch total loss 1.2952466\n",
      "Trained batch 165 batch loss 1.44392359 epoch total loss 1.2961477\n",
      "Trained batch 166 batch loss 1.38990402 epoch total loss 1.29671252\n",
      "Trained batch 167 batch loss 1.34199119 epoch total loss 1.2969836\n",
      "Trained batch 168 batch loss 1.30862713 epoch total loss 1.29705298\n",
      "Trained batch 169 batch loss 1.24133706 epoch total loss 1.29672325\n",
      "Trained batch 170 batch loss 1.26010311 epoch total loss 1.29650784\n",
      "Trained batch 171 batch loss 1.27699518 epoch total loss 1.29639363\n",
      "Trained batch 172 batch loss 1.31722641 epoch total loss 1.29651487\n",
      "Trained batch 173 batch loss 1.42483401 epoch total loss 1.29725659\n",
      "Trained batch 174 batch loss 1.39937544 epoch total loss 1.29784346\n",
      "Trained batch 175 batch loss 1.34890294 epoch total loss 1.29813516\n",
      "Trained batch 176 batch loss 1.34365976 epoch total loss 1.29839385\n",
      "Trained batch 177 batch loss 1.34832275 epoch total loss 1.29867601\n",
      "Trained batch 178 batch loss 1.34622598 epoch total loss 1.29894304\n",
      "Trained batch 179 batch loss 1.32992494 epoch total loss 1.29911613\n",
      "Trained batch 180 batch loss 1.36649919 epoch total loss 1.29949057\n",
      "Trained batch 181 batch loss 1.35147607 epoch total loss 1.29977775\n",
      "Trained batch 182 batch loss 1.3527801 epoch total loss 1.30006897\n",
      "Trained batch 183 batch loss 1.2567091 epoch total loss 1.29983199\n",
      "Trained batch 184 batch loss 1.23427153 epoch total loss 1.29947567\n",
      "Trained batch 185 batch loss 1.26438451 epoch total loss 1.29928601\n",
      "Trained batch 186 batch loss 1.25140858 epoch total loss 1.29902864\n",
      "Trained batch 187 batch loss 1.28220606 epoch total loss 1.29893863\n",
      "Trained batch 188 batch loss 1.22530246 epoch total loss 1.29854703\n",
      "Trained batch 189 batch loss 1.11224711 epoch total loss 1.29756129\n",
      "Trained batch 190 batch loss 1.09631896 epoch total loss 1.29650199\n",
      "Trained batch 191 batch loss 1.32333 epoch total loss 1.29664254\n",
      "Trained batch 192 batch loss 1.28145218 epoch total loss 1.29656339\n",
      "Trained batch 193 batch loss 1.32013524 epoch total loss 1.29668546\n",
      "Trained batch 194 batch loss 1.34615552 epoch total loss 1.29694057\n",
      "Trained batch 195 batch loss 1.30147099 epoch total loss 1.29696369\n",
      "Trained batch 196 batch loss 1.32583523 epoch total loss 1.29711103\n",
      "Trained batch 197 batch loss 1.33831406 epoch total loss 1.29732025\n",
      "Trained batch 198 batch loss 1.2787292 epoch total loss 1.29722631\n",
      "Trained batch 199 batch loss 1.24675179 epoch total loss 1.29697263\n",
      "Trained batch 200 batch loss 1.19272053 epoch total loss 1.29645145\n",
      "Trained batch 201 batch loss 1.24255562 epoch total loss 1.29618323\n",
      "Trained batch 202 batch loss 1.12309313 epoch total loss 1.29532647\n",
      "Trained batch 203 batch loss 1.15835798 epoch total loss 1.29465175\n",
      "Trained batch 204 batch loss 1.23433542 epoch total loss 1.29435611\n",
      "Trained batch 205 batch loss 1.34031713 epoch total loss 1.29458034\n",
      "Trained batch 206 batch loss 1.2010932 epoch total loss 1.29412651\n",
      "Trained batch 207 batch loss 1.24112761 epoch total loss 1.29387045\n",
      "Trained batch 208 batch loss 1.3307941 epoch total loss 1.29404783\n",
      "Trained batch 209 batch loss 1.30973649 epoch total loss 1.29412282\n",
      "Trained batch 210 batch loss 1.32114232 epoch total loss 1.29425156\n",
      "Trained batch 211 batch loss 1.44036531 epoch total loss 1.29494405\n",
      "Trained batch 212 batch loss 1.31560302 epoch total loss 1.29504144\n",
      "Trained batch 213 batch loss 1.28352761 epoch total loss 1.29498744\n",
      "Trained batch 214 batch loss 1.25405073 epoch total loss 1.29479623\n",
      "Trained batch 215 batch loss 1.29359305 epoch total loss 1.29479063\n",
      "Trained batch 216 batch loss 1.29115748 epoch total loss 1.29477382\n",
      "Trained batch 217 batch loss 1.31103265 epoch total loss 1.2948488\n",
      "Trained batch 218 batch loss 1.30873036 epoch total loss 1.29491234\n",
      "Trained batch 219 batch loss 1.31045783 epoch total loss 1.29498339\n",
      "Trained batch 220 batch loss 1.16517103 epoch total loss 1.29439318\n",
      "Trained batch 221 batch loss 1.10997045 epoch total loss 1.29355884\n",
      "Trained batch 222 batch loss 1.19212294 epoch total loss 1.29310179\n",
      "Trained batch 223 batch loss 1.22841382 epoch total loss 1.29281175\n",
      "Trained batch 224 batch loss 1.23861587 epoch total loss 1.29256988\n",
      "Trained batch 225 batch loss 1.34638643 epoch total loss 1.29280901\n",
      "Trained batch 226 batch loss 1.20076144 epoch total loss 1.29240179\n",
      "Trained batch 227 batch loss 1.33808565 epoch total loss 1.2926029\n",
      "Trained batch 228 batch loss 1.25848854 epoch total loss 1.29245329\n",
      "Trained batch 229 batch loss 1.35505724 epoch total loss 1.29272676\n",
      "Trained batch 230 batch loss 1.39711511 epoch total loss 1.2931807\n",
      "Trained batch 231 batch loss 1.47282743 epoch total loss 1.29395843\n",
      "Trained batch 232 batch loss 1.56277847 epoch total loss 1.29511714\n",
      "Trained batch 233 batch loss 1.36816263 epoch total loss 1.29543054\n",
      "Trained batch 234 batch loss 1.23981023 epoch total loss 1.29519284\n",
      "Trained batch 235 batch loss 1.33037293 epoch total loss 1.29534268\n",
      "Trained batch 236 batch loss 1.43372178 epoch total loss 1.29592896\n",
      "Trained batch 237 batch loss 1.48135233 epoch total loss 1.29671133\n",
      "Trained batch 238 batch loss 1.44885373 epoch total loss 1.29735065\n",
      "Trained batch 239 batch loss 1.40283918 epoch total loss 1.29779196\n",
      "Trained batch 240 batch loss 1.33047104 epoch total loss 1.29792809\n",
      "Trained batch 241 batch loss 1.22723484 epoch total loss 1.29763472\n",
      "Trained batch 242 batch loss 1.20552301 epoch total loss 1.2972542\n",
      "Trained batch 243 batch loss 1.09743905 epoch total loss 1.2964319\n",
      "Trained batch 244 batch loss 1.21857285 epoch total loss 1.29611278\n",
      "Trained batch 245 batch loss 1.33473635 epoch total loss 1.29627049\n",
      "Trained batch 246 batch loss 1.29259777 epoch total loss 1.29625559\n",
      "Trained batch 247 batch loss 1.198125 epoch total loss 1.29585826\n",
      "Trained batch 248 batch loss 1.19867015 epoch total loss 1.29546642\n",
      "Trained batch 249 batch loss 1.34175766 epoch total loss 1.29565239\n",
      "Trained batch 250 batch loss 1.30613947 epoch total loss 1.29569435\n",
      "Trained batch 251 batch loss 1.50091505 epoch total loss 1.29651201\n",
      "Trained batch 252 batch loss 1.16791379 epoch total loss 1.29600167\n",
      "Trained batch 253 batch loss 1.16623116 epoch total loss 1.29548872\n",
      "Trained batch 254 batch loss 1.25520301 epoch total loss 1.29533\n",
      "Trained batch 255 batch loss 1.28065121 epoch total loss 1.29527235\n",
      "Trained batch 256 batch loss 1.45140886 epoch total loss 1.29588234\n",
      "Trained batch 257 batch loss 1.41534567 epoch total loss 1.29634714\n",
      "Trained batch 258 batch loss 1.30440402 epoch total loss 1.29637849\n",
      "Trained batch 259 batch loss 1.31848693 epoch total loss 1.29646373\n",
      "Trained batch 260 batch loss 1.19188023 epoch total loss 1.29606164\n",
      "Trained batch 261 batch loss 1.23196769 epoch total loss 1.29581606\n",
      "Trained batch 262 batch loss 1.2196691 epoch total loss 1.29552531\n",
      "Trained batch 263 batch loss 1.22279346 epoch total loss 1.29524875\n",
      "Trained batch 264 batch loss 1.19815862 epoch total loss 1.29488099\n",
      "Trained batch 265 batch loss 1.24413109 epoch total loss 1.29468954\n",
      "Trained batch 266 batch loss 1.20072961 epoch total loss 1.29433632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 267 batch loss 1.28852439 epoch total loss 1.2943145\n",
      "Trained batch 268 batch loss 1.23772502 epoch total loss 1.29410338\n",
      "Trained batch 269 batch loss 1.09431171 epoch total loss 1.29336059\n",
      "Trained batch 270 batch loss 1.05457735 epoch total loss 1.29247618\n",
      "Trained batch 271 batch loss 1.04198468 epoch total loss 1.29155183\n",
      "Trained batch 272 batch loss 1.23906755 epoch total loss 1.29135895\n",
      "Trained batch 273 batch loss 1.36327505 epoch total loss 1.2916224\n",
      "Trained batch 274 batch loss 1.34255 epoch total loss 1.29180825\n",
      "Trained batch 275 batch loss 1.38909483 epoch total loss 1.29216206\n",
      "Trained batch 276 batch loss 1.22165287 epoch total loss 1.2919066\n",
      "Trained batch 277 batch loss 1.11932445 epoch total loss 1.29128361\n",
      "Trained batch 278 batch loss 1.23395562 epoch total loss 1.29107738\n",
      "Trained batch 279 batch loss 1.15158677 epoch total loss 1.29057729\n",
      "Trained batch 280 batch loss 1.3264581 epoch total loss 1.29070544\n",
      "Trained batch 281 batch loss 1.22265613 epoch total loss 1.29046321\n",
      "Trained batch 282 batch loss 1.29483676 epoch total loss 1.29047871\n",
      "Trained batch 283 batch loss 1.34045696 epoch total loss 1.29065537\n",
      "Trained batch 284 batch loss 1.30768061 epoch total loss 1.29071534\n",
      "Trained batch 285 batch loss 1.17510724 epoch total loss 1.29030967\n",
      "Trained batch 286 batch loss 1.06849623 epoch total loss 1.28953397\n",
      "Trained batch 287 batch loss 1.08674634 epoch total loss 1.28882754\n",
      "Trained batch 288 batch loss 1.61011839 epoch total loss 1.2899431\n",
      "Trained batch 289 batch loss 1.3681612 epoch total loss 1.2902137\n",
      "Trained batch 290 batch loss 1.35606527 epoch total loss 1.2904408\n",
      "Trained batch 291 batch loss 1.32549834 epoch total loss 1.29056132\n",
      "Trained batch 292 batch loss 1.38961315 epoch total loss 1.29090059\n",
      "Trained batch 293 batch loss 1.20829058 epoch total loss 1.29061854\n",
      "Trained batch 294 batch loss 1.26788449 epoch total loss 1.29054129\n",
      "Trained batch 295 batch loss 1.29387355 epoch total loss 1.29055262\n",
      "Trained batch 296 batch loss 1.19702113 epoch total loss 1.29023659\n",
      "Trained batch 297 batch loss 1.32326174 epoch total loss 1.29034781\n",
      "Trained batch 298 batch loss 1.32820821 epoch total loss 1.29047489\n",
      "Trained batch 299 batch loss 1.24424732 epoch total loss 1.29032028\n",
      "Trained batch 300 batch loss 1.32379115 epoch total loss 1.29043186\n",
      "Trained batch 301 batch loss 1.26294565 epoch total loss 1.29034054\n",
      "Trained batch 302 batch loss 1.23988545 epoch total loss 1.29017341\n",
      "Trained batch 303 batch loss 1.23313069 epoch total loss 1.28998518\n",
      "Trained batch 304 batch loss 1.34493756 epoch total loss 1.2901659\n",
      "Trained batch 305 batch loss 1.32515442 epoch total loss 1.2902807\n",
      "Trained batch 306 batch loss 1.47228181 epoch total loss 1.29087555\n",
      "Trained batch 307 batch loss 1.2622298 epoch total loss 1.29078221\n",
      "Trained batch 308 batch loss 1.19078505 epoch total loss 1.29045761\n",
      "Trained batch 309 batch loss 1.28498721 epoch total loss 1.29043984\n",
      "Trained batch 310 batch loss 1.25739813 epoch total loss 1.29033327\n",
      "Trained batch 311 batch loss 1.29926908 epoch total loss 1.29036188\n",
      "Trained batch 312 batch loss 1.28000176 epoch total loss 1.29032874\n",
      "Trained batch 313 batch loss 1.32594824 epoch total loss 1.29044247\n",
      "Trained batch 314 batch loss 1.30199957 epoch total loss 1.2904793\n",
      "Trained batch 315 batch loss 1.240605 epoch total loss 1.29032099\n",
      "Trained batch 316 batch loss 1.25669062 epoch total loss 1.29021454\n",
      "Trained batch 317 batch loss 1.21149981 epoch total loss 1.28996623\n",
      "Trained batch 318 batch loss 1.31413877 epoch total loss 1.29004228\n",
      "Trained batch 319 batch loss 1.21067798 epoch total loss 1.28979337\n",
      "Trained batch 320 batch loss 1.29059 epoch total loss 1.28979588\n",
      "Trained batch 321 batch loss 1.32014072 epoch total loss 1.28989041\n",
      "Trained batch 322 batch loss 1.27145505 epoch total loss 1.28983307\n",
      "Trained batch 323 batch loss 1.26701581 epoch total loss 1.2897625\n",
      "Trained batch 324 batch loss 1.24996889 epoch total loss 1.28963971\n",
      "Trained batch 325 batch loss 1.16476011 epoch total loss 1.2892555\n",
      "Trained batch 326 batch loss 1.22508442 epoch total loss 1.28905869\n",
      "Trained batch 327 batch loss 1.25164664 epoch total loss 1.28894424\n",
      "Trained batch 328 batch loss 1.41763413 epoch total loss 1.28933656\n",
      "Trained batch 329 batch loss 1.38253558 epoch total loss 1.28961992\n",
      "Trained batch 330 batch loss 1.35042763 epoch total loss 1.28980422\n",
      "Trained batch 331 batch loss 1.29186153 epoch total loss 1.28981042\n",
      "Trained batch 332 batch loss 1.28929853 epoch total loss 1.28980887\n",
      "Trained batch 333 batch loss 1.36522961 epoch total loss 1.29003537\n",
      "Trained batch 334 batch loss 1.30889499 epoch total loss 1.29009187\n",
      "Trained batch 335 batch loss 1.21731949 epoch total loss 1.28987467\n",
      "Trained batch 336 batch loss 1.2922945 epoch total loss 1.28988183\n",
      "Trained batch 337 batch loss 1.35438526 epoch total loss 1.29007316\n",
      "Trained batch 338 batch loss 1.43428361 epoch total loss 1.29049993\n",
      "Trained batch 339 batch loss 1.30002987 epoch total loss 1.29052794\n",
      "Trained batch 340 batch loss 1.32642925 epoch total loss 1.29063356\n",
      "Trained batch 341 batch loss 1.10718691 epoch total loss 1.29009557\n",
      "Trained batch 342 batch loss 1.18035626 epoch total loss 1.28977466\n",
      "Trained batch 343 batch loss 1.2168678 epoch total loss 1.28956211\n",
      "Trained batch 344 batch loss 1.26659584 epoch total loss 1.28949535\n",
      "Trained batch 345 batch loss 1.31849289 epoch total loss 1.28957939\n",
      "Trained batch 346 batch loss 1.29650283 epoch total loss 1.28959942\n",
      "Trained batch 347 batch loss 1.34137464 epoch total loss 1.28974855\n",
      "Trained batch 348 batch loss 1.28521991 epoch total loss 1.28973556\n",
      "Trained batch 349 batch loss 1.28525901 epoch total loss 1.28972268\n",
      "Trained batch 350 batch loss 1.39034832 epoch total loss 1.29001021\n",
      "Trained batch 351 batch loss 1.26294923 epoch total loss 1.28993309\n",
      "Trained batch 352 batch loss 1.23601675 epoch total loss 1.2897799\n",
      "Trained batch 353 batch loss 1.26064813 epoch total loss 1.28969741\n",
      "Trained batch 354 batch loss 1.24949992 epoch total loss 1.28958392\n",
      "Trained batch 355 batch loss 1.37587357 epoch total loss 1.28982699\n",
      "Trained batch 356 batch loss 1.27391148 epoch total loss 1.28978229\n",
      "Trained batch 357 batch loss 1.24838209 epoch total loss 1.28966641\n",
      "Trained batch 358 batch loss 1.28352547 epoch total loss 1.28964925\n",
      "Trained batch 359 batch loss 1.19219136 epoch total loss 1.28937781\n",
      "Trained batch 360 batch loss 1.41633081 epoch total loss 1.28973043\n",
      "Trained batch 361 batch loss 1.25998163 epoch total loss 1.28964806\n",
      "Trained batch 362 batch loss 1.25193214 epoch total loss 1.28954375\n",
      "Trained batch 363 batch loss 1.45563233 epoch total loss 1.29000127\n",
      "Trained batch 364 batch loss 1.27001619 epoch total loss 1.28994644\n",
      "Trained batch 365 batch loss 1.31635571 epoch total loss 1.2900188\n",
      "Trained batch 366 batch loss 1.35372949 epoch total loss 1.29019284\n",
      "Trained batch 367 batch loss 1.20913327 epoch total loss 1.28997195\n",
      "Trained batch 368 batch loss 1.33689153 epoch total loss 1.29009938\n",
      "Trained batch 369 batch loss 1.24719155 epoch total loss 1.28998315\n",
      "Trained batch 370 batch loss 1.19905257 epoch total loss 1.28973746\n",
      "Trained batch 371 batch loss 1.33726335 epoch total loss 1.28986549\n",
      "Trained batch 372 batch loss 1.28739738 epoch total loss 1.28985882\n",
      "Trained batch 373 batch loss 1.28883886 epoch total loss 1.28985608\n",
      "Trained batch 374 batch loss 1.25312197 epoch total loss 1.28975785\n",
      "Trained batch 375 batch loss 1.2361958 epoch total loss 1.28961504\n",
      "Trained batch 376 batch loss 1.22827256 epoch total loss 1.28945196\n",
      "Trained batch 377 batch loss 1.29894137 epoch total loss 1.28947711\n",
      "Trained batch 378 batch loss 1.3059032 epoch total loss 1.28952062\n",
      "Trained batch 379 batch loss 1.39387989 epoch total loss 1.28979599\n",
      "Trained batch 380 batch loss 1.28277493 epoch total loss 1.28977752\n",
      "Trained batch 381 batch loss 1.24942279 epoch total loss 1.28967154\n",
      "Trained batch 382 batch loss 1.24862862 epoch total loss 1.28956413\n",
      "Trained batch 383 batch loss 1.33060336 epoch total loss 1.2896713\n",
      "Trained batch 384 batch loss 1.32236409 epoch total loss 1.28975642\n",
      "Trained batch 385 batch loss 1.3484242 epoch total loss 1.28990877\n",
      "Trained batch 386 batch loss 1.42536139 epoch total loss 1.2902596\n",
      "Trained batch 387 batch loss 1.40653265 epoch total loss 1.29056013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 388 batch loss 1.45411336 epoch total loss 1.29098153\n",
      "Trained batch 389 batch loss 1.25353384 epoch total loss 1.29088533\n",
      "Trained batch 390 batch loss 1.21105707 epoch total loss 1.29068065\n",
      "Trained batch 391 batch loss 1.22562838 epoch total loss 1.29051423\n",
      "Trained batch 392 batch loss 1.26477885 epoch total loss 1.29044855\n",
      "Trained batch 393 batch loss 1.19862843 epoch total loss 1.2902149\n",
      "Trained batch 394 batch loss 1.30083036 epoch total loss 1.29024196\n",
      "Trained batch 395 batch loss 1.32107449 epoch total loss 1.29031992\n",
      "Trained batch 396 batch loss 1.20201528 epoch total loss 1.290097\n",
      "Trained batch 397 batch loss 1.18140745 epoch total loss 1.28982317\n",
      "Trained batch 398 batch loss 1.20504093 epoch total loss 1.28961015\n",
      "Trained batch 399 batch loss 1.19678688 epoch total loss 1.28937745\n",
      "Trained batch 400 batch loss 1.33371949 epoch total loss 1.28948832\n",
      "Trained batch 401 batch loss 1.34112394 epoch total loss 1.28961718\n",
      "Trained batch 402 batch loss 1.14304006 epoch total loss 1.28925264\n",
      "Trained batch 403 batch loss 1.20749259 epoch total loss 1.28904974\n",
      "Trained batch 404 batch loss 1.25058484 epoch total loss 1.28895462\n",
      "Trained batch 405 batch loss 1.3439219 epoch total loss 1.28909039\n",
      "Trained batch 406 batch loss 1.27059829 epoch total loss 1.28904474\n",
      "Trained batch 407 batch loss 1.32083452 epoch total loss 1.28912294\n",
      "Trained batch 408 batch loss 1.2478348 epoch total loss 1.28902185\n",
      "Trained batch 409 batch loss 1.1991415 epoch total loss 1.28880215\n",
      "Trained batch 410 batch loss 1.22802842 epoch total loss 1.28865385\n",
      "Trained batch 411 batch loss 1.21986091 epoch total loss 1.28848648\n",
      "Trained batch 412 batch loss 1.18098235 epoch total loss 1.28822553\n",
      "Trained batch 413 batch loss 1.29144 epoch total loss 1.28823328\n",
      "Trained batch 414 batch loss 1.21857464 epoch total loss 1.28806496\n",
      "Trained batch 415 batch loss 1.22742248 epoch total loss 1.28791881\n",
      "Trained batch 416 batch loss 1.18603396 epoch total loss 1.28767395\n",
      "Trained batch 417 batch loss 1.37104654 epoch total loss 1.28787386\n",
      "Trained batch 418 batch loss 1.46333623 epoch total loss 1.2882936\n",
      "Trained batch 419 batch loss 1.38597131 epoch total loss 1.28852677\n",
      "Trained batch 420 batch loss 1.4374615 epoch total loss 1.2888813\n",
      "Trained batch 421 batch loss 1.35221148 epoch total loss 1.28903174\n",
      "Trained batch 422 batch loss 1.37339401 epoch total loss 1.28923178\n",
      "Trained batch 423 batch loss 1.43200874 epoch total loss 1.28956926\n",
      "Trained batch 424 batch loss 1.3058486 epoch total loss 1.28960764\n",
      "Trained batch 425 batch loss 1.28123951 epoch total loss 1.28958797\n",
      "Trained batch 426 batch loss 1.296435 epoch total loss 1.28960407\n",
      "Trained batch 427 batch loss 1.25316763 epoch total loss 1.28951871\n",
      "Trained batch 428 batch loss 1.22078276 epoch total loss 1.28935814\n",
      "Trained batch 429 batch loss 1.32184839 epoch total loss 1.28943384\n",
      "Trained batch 430 batch loss 1.29494536 epoch total loss 1.28944659\n",
      "Trained batch 431 batch loss 1.2461369 epoch total loss 1.2893461\n",
      "Trained batch 432 batch loss 1.26712739 epoch total loss 1.28929472\n",
      "Trained batch 433 batch loss 1.19789135 epoch total loss 1.2890836\n",
      "Trained batch 434 batch loss 1.26902318 epoch total loss 1.28903747\n",
      "Trained batch 435 batch loss 1.39017582 epoch total loss 1.28927\n",
      "Trained batch 436 batch loss 1.41103709 epoch total loss 1.28954923\n",
      "Trained batch 437 batch loss 1.40020323 epoch total loss 1.28980243\n",
      "Trained batch 438 batch loss 1.24482632 epoch total loss 1.28969979\n",
      "Trained batch 439 batch loss 1.28867245 epoch total loss 1.28969741\n",
      "Trained batch 440 batch loss 1.261163 epoch total loss 1.28963268\n",
      "Trained batch 441 batch loss 1.31767428 epoch total loss 1.28969622\n",
      "Trained batch 442 batch loss 1.23192143 epoch total loss 1.28956556\n",
      "Trained batch 443 batch loss 1.28927064 epoch total loss 1.28956485\n",
      "Trained batch 444 batch loss 1.42550826 epoch total loss 1.2898711\n",
      "Trained batch 445 batch loss 1.25869787 epoch total loss 1.28980112\n",
      "Trained batch 446 batch loss 1.27724195 epoch total loss 1.28977287\n",
      "Trained batch 447 batch loss 1.35023224 epoch total loss 1.28990817\n",
      "Trained batch 448 batch loss 1.15368128 epoch total loss 1.28960407\n",
      "Trained batch 449 batch loss 1.23125505 epoch total loss 1.28947413\n",
      "Trained batch 450 batch loss 1.22725821 epoch total loss 1.28933585\n",
      "Trained batch 451 batch loss 1.2533927 epoch total loss 1.28925622\n",
      "Trained batch 452 batch loss 1.18965685 epoch total loss 1.2890358\n",
      "Trained batch 453 batch loss 1.18132412 epoch total loss 1.28879797\n",
      "Trained batch 454 batch loss 1.22030723 epoch total loss 1.28864717\n",
      "Trained batch 455 batch loss 1.29851639 epoch total loss 1.28866887\n",
      "Trained batch 456 batch loss 1.32323337 epoch total loss 1.28874469\n",
      "Trained batch 457 batch loss 1.34380198 epoch total loss 1.28886521\n",
      "Trained batch 458 batch loss 1.37944329 epoch total loss 1.28906298\n",
      "Trained batch 459 batch loss 1.29137325 epoch total loss 1.2890681\n",
      "Trained batch 460 batch loss 1.4272387 epoch total loss 1.28936851\n",
      "Trained batch 461 batch loss 1.42032647 epoch total loss 1.28965259\n",
      "Trained batch 462 batch loss 1.53063798 epoch total loss 1.29017425\n",
      "Trained batch 463 batch loss 1.43044269 epoch total loss 1.29047716\n",
      "Trained batch 464 batch loss 1.21938074 epoch total loss 1.29032385\n",
      "Trained batch 465 batch loss 1.16646135 epoch total loss 1.29005742\n",
      "Trained batch 466 batch loss 1.26056314 epoch total loss 1.28999412\n",
      "Trained batch 467 batch loss 1.17423248 epoch total loss 1.28974628\n",
      "Trained batch 468 batch loss 1.15862453 epoch total loss 1.28946614\n",
      "Trained batch 469 batch loss 1.20451462 epoch total loss 1.28928506\n",
      "Trained batch 470 batch loss 1.1963079 epoch total loss 1.28908718\n",
      "Trained batch 471 batch loss 1.12832975 epoch total loss 1.28874588\n",
      "Trained batch 472 batch loss 1.13342285 epoch total loss 1.28841686\n",
      "Trained batch 473 batch loss 1.17271483 epoch total loss 1.28817225\n",
      "Trained batch 474 batch loss 1.2106092 epoch total loss 1.28800869\n",
      "Trained batch 475 batch loss 1.24211943 epoch total loss 1.28791213\n",
      "Trained batch 476 batch loss 1.16325736 epoch total loss 1.28765023\n",
      "Trained batch 477 batch loss 1.24247742 epoch total loss 1.28755558\n",
      "Trained batch 478 batch loss 1.16670811 epoch total loss 1.28730273\n",
      "Trained batch 479 batch loss 1.24959421 epoch total loss 1.28722394\n",
      "Trained batch 480 batch loss 1.38262451 epoch total loss 1.28742266\n",
      "Trained batch 481 batch loss 1.33693051 epoch total loss 1.28752553\n",
      "Trained batch 482 batch loss 1.47963643 epoch total loss 1.28792405\n",
      "Trained batch 483 batch loss 1.53029108 epoch total loss 1.28842592\n",
      "Trained batch 484 batch loss 1.36287808 epoch total loss 1.28857958\n",
      "Trained batch 485 batch loss 1.37697887 epoch total loss 1.28876185\n",
      "Trained batch 486 batch loss 1.32502151 epoch total loss 1.28883648\n",
      "Trained batch 487 batch loss 1.26952219 epoch total loss 1.28879678\n",
      "Trained batch 488 batch loss 1.39337325 epoch total loss 1.28901112\n",
      "Trained batch 489 batch loss 1.35855603 epoch total loss 1.28915334\n",
      "Trained batch 490 batch loss 1.37368464 epoch total loss 1.28932583\n",
      "Trained batch 491 batch loss 1.45618033 epoch total loss 1.2896657\n",
      "Trained batch 492 batch loss 1.41462255 epoch total loss 1.28991961\n",
      "Trained batch 493 batch loss 1.51216531 epoch total loss 1.29037035\n",
      "Trained batch 494 batch loss 1.48510766 epoch total loss 1.29076457\n",
      "Trained batch 495 batch loss 1.38870311 epoch total loss 1.29096246\n",
      "Trained batch 496 batch loss 1.45996189 epoch total loss 1.29130316\n",
      "Trained batch 497 batch loss 1.39525223 epoch total loss 1.29151237\n",
      "Trained batch 498 batch loss 1.41313171 epoch total loss 1.29175663\n",
      "Trained batch 499 batch loss 1.38659883 epoch total loss 1.29194665\n",
      "Trained batch 500 batch loss 1.3935504 epoch total loss 1.2921499\n",
      "Trained batch 501 batch loss 1.34826374 epoch total loss 1.29226196\n",
      "Trained batch 502 batch loss 1.26724923 epoch total loss 1.29221213\n",
      "Trained batch 503 batch loss 1.20834148 epoch total loss 1.29204535\n",
      "Trained batch 504 batch loss 1.2403301 epoch total loss 1.29194283\n",
      "Trained batch 505 batch loss 1.20607948 epoch total loss 1.29177272\n",
      "Trained batch 506 batch loss 1.33544874 epoch total loss 1.29185903\n",
      "Trained batch 507 batch loss 1.40106201 epoch total loss 1.29207444\n",
      "Trained batch 508 batch loss 1.41645646 epoch total loss 1.29231918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 509 batch loss 1.5103581 epoch total loss 1.29274762\n",
      "Trained batch 510 batch loss 1.3817544 epoch total loss 1.29292214\n",
      "Trained batch 511 batch loss 1.35255563 epoch total loss 1.29303885\n",
      "Trained batch 512 batch loss 1.24352872 epoch total loss 1.29294217\n",
      "Trained batch 513 batch loss 1.16426611 epoch total loss 1.29269135\n",
      "Trained batch 514 batch loss 1.20171058 epoch total loss 1.29251432\n",
      "Trained batch 515 batch loss 1.26404798 epoch total loss 1.29245901\n",
      "Trained batch 516 batch loss 1.24891341 epoch total loss 1.29237461\n",
      "Trained batch 517 batch loss 1.35635805 epoch total loss 1.29249847\n",
      "Trained batch 518 batch loss 1.32124627 epoch total loss 1.2925539\n",
      "Trained batch 519 batch loss 1.2755456 epoch total loss 1.29252112\n",
      "Trained batch 520 batch loss 1.33894849 epoch total loss 1.29261041\n",
      "Trained batch 521 batch loss 1.28013659 epoch total loss 1.29258645\n",
      "Trained batch 522 batch loss 1.16855586 epoch total loss 1.29234898\n",
      "Trained batch 523 batch loss 1.17973387 epoch total loss 1.29213369\n",
      "Trained batch 524 batch loss 1.17411959 epoch total loss 1.29190838\n",
      "Trained batch 525 batch loss 1.21105 epoch total loss 1.29175448\n",
      "Trained batch 526 batch loss 1.1026715 epoch total loss 1.29139495\n",
      "Trained batch 527 batch loss 1.15687227 epoch total loss 1.29113972\n",
      "Trained batch 528 batch loss 1.12132525 epoch total loss 1.2908181\n",
      "Trained batch 529 batch loss 1.14986956 epoch total loss 1.29055154\n",
      "Trained batch 530 batch loss 1.20091486 epoch total loss 1.2903825\n",
      "Trained batch 531 batch loss 1.18855751 epoch total loss 1.2901907\n",
      "Trained batch 532 batch loss 1.11361909 epoch total loss 1.28985882\n",
      "Trained batch 533 batch loss 1.18878365 epoch total loss 1.28966916\n",
      "Trained batch 534 batch loss 1.31652403 epoch total loss 1.28971946\n",
      "Trained batch 535 batch loss 1.41708112 epoch total loss 1.28995752\n",
      "Trained batch 536 batch loss 1.31711566 epoch total loss 1.29000819\n",
      "Trained batch 537 batch loss 1.3060962 epoch total loss 1.29003811\n",
      "Trained batch 538 batch loss 1.55695653 epoch total loss 1.29053426\n",
      "Trained batch 539 batch loss 1.4182297 epoch total loss 1.29077113\n",
      "Trained batch 540 batch loss 1.4075917 epoch total loss 1.29098749\n",
      "Trained batch 541 batch loss 1.17176902 epoch total loss 1.29076707\n",
      "Trained batch 542 batch loss 1.24116266 epoch total loss 1.29067552\n",
      "Trained batch 543 batch loss 1.42369556 epoch total loss 1.2909205\n",
      "Trained batch 544 batch loss 1.28641129 epoch total loss 1.29091227\n",
      "Trained batch 545 batch loss 1.26087976 epoch total loss 1.2908572\n",
      "Trained batch 546 batch loss 1.31973338 epoch total loss 1.29091012\n",
      "Trained batch 547 batch loss 1.30594826 epoch total loss 1.29093766\n",
      "Trained batch 548 batch loss 1.30116832 epoch total loss 1.29095626\n",
      "Trained batch 549 batch loss 1.3116281 epoch total loss 1.29099393\n",
      "Trained batch 550 batch loss 1.33695209 epoch total loss 1.29107749\n",
      "Trained batch 551 batch loss 1.41718948 epoch total loss 1.29130638\n",
      "Trained batch 552 batch loss 1.45084739 epoch total loss 1.29159546\n",
      "Trained batch 553 batch loss 1.31245613 epoch total loss 1.29163313\n",
      "Trained batch 554 batch loss 1.37273014 epoch total loss 1.29177952\n",
      "Trained batch 555 batch loss 1.29498982 epoch total loss 1.29178536\n",
      "Trained batch 556 batch loss 1.25236106 epoch total loss 1.29171443\n",
      "Trained batch 557 batch loss 1.28360868 epoch total loss 1.29169989\n",
      "Trained batch 558 batch loss 1.35484 epoch total loss 1.29181314\n",
      "Trained batch 559 batch loss 1.32281613 epoch total loss 1.29186857\n",
      "Trained batch 560 batch loss 1.30895603 epoch total loss 1.29189909\n",
      "Trained batch 561 batch loss 1.29906523 epoch total loss 1.29191184\n",
      "Trained batch 562 batch loss 1.34201288 epoch total loss 1.29200113\n",
      "Trained batch 563 batch loss 1.24760795 epoch total loss 1.29192221\n",
      "Trained batch 564 batch loss 1.25301504 epoch total loss 1.29185319\n",
      "Trained batch 565 batch loss 1.2799927 epoch total loss 1.29183221\n",
      "Trained batch 566 batch loss 1.225209 epoch total loss 1.29171455\n",
      "Trained batch 567 batch loss 1.19185925 epoch total loss 1.29153836\n",
      "Trained batch 568 batch loss 1.26609766 epoch total loss 1.29149354\n",
      "Trained batch 569 batch loss 1.23832989 epoch total loss 1.29140019\n",
      "Trained batch 570 batch loss 1.26904225 epoch total loss 1.29136097\n",
      "Trained batch 571 batch loss 1.3706162 epoch total loss 1.29149973\n",
      "Trained batch 572 batch loss 1.38856542 epoch total loss 1.29166937\n",
      "Trained batch 573 batch loss 1.37844467 epoch total loss 1.29182076\n",
      "Trained batch 574 batch loss 1.24850035 epoch total loss 1.29174531\n",
      "Trained batch 575 batch loss 1.27818584 epoch total loss 1.2917217\n",
      "Trained batch 576 batch loss 1.16374028 epoch total loss 1.29149961\n",
      "Trained batch 577 batch loss 1.16352451 epoch total loss 1.29127777\n",
      "Trained batch 578 batch loss 1.15400219 epoch total loss 1.29104018\n",
      "Trained batch 579 batch loss 1.32115495 epoch total loss 1.29109228\n",
      "Trained batch 580 batch loss 1.37672949 epoch total loss 1.29123986\n",
      "Trained batch 581 batch loss 1.37521362 epoch total loss 1.29138434\n",
      "Trained batch 582 batch loss 1.2171284 epoch total loss 1.29125667\n",
      "Trained batch 583 batch loss 1.24415457 epoch total loss 1.29117596\n",
      "Trained batch 584 batch loss 1.29172468 epoch total loss 1.29117692\n",
      "Trained batch 585 batch loss 1.15700078 epoch total loss 1.29094744\n",
      "Trained batch 586 batch loss 0.972441554 epoch total loss 1.29040396\n",
      "Trained batch 587 batch loss 1.09206951 epoch total loss 1.290066\n",
      "Trained batch 588 batch loss 1.16718245 epoch total loss 1.28985703\n",
      "Trained batch 589 batch loss 1.1916213 epoch total loss 1.28969026\n",
      "Trained batch 590 batch loss 1.22029555 epoch total loss 1.2895726\n",
      "Trained batch 591 batch loss 1.22469878 epoch total loss 1.2894628\n",
      "Trained batch 592 batch loss 1.1047703 epoch total loss 1.28915083\n",
      "Trained batch 593 batch loss 1.13563299 epoch total loss 1.28889191\n",
      "Trained batch 594 batch loss 1.33245182 epoch total loss 1.28896534\n",
      "Trained batch 595 batch loss 1.25523067 epoch total loss 1.2889086\n",
      "Trained batch 596 batch loss 1.3721348 epoch total loss 1.28904831\n",
      "Trained batch 597 batch loss 1.18873119 epoch total loss 1.28888023\n",
      "Trained batch 598 batch loss 1.07243049 epoch total loss 1.28851831\n",
      "Trained batch 599 batch loss 1.2099328 epoch total loss 1.28838718\n",
      "Trained batch 600 batch loss 1.24691939 epoch total loss 1.28831804\n",
      "Trained batch 601 batch loss 1.21803749 epoch total loss 1.28820109\n",
      "Trained batch 602 batch loss 1.53352141 epoch total loss 1.28860855\n",
      "Trained batch 603 batch loss 1.46789122 epoch total loss 1.28890586\n",
      "Trained batch 604 batch loss 1.42063808 epoch total loss 1.28912401\n",
      "Trained batch 605 batch loss 1.27506351 epoch total loss 1.28910089\n",
      "Trained batch 606 batch loss 1.23379564 epoch total loss 1.28900957\n",
      "Trained batch 607 batch loss 1.21516383 epoch total loss 1.28888798\n",
      "Trained batch 608 batch loss 1.27972639 epoch total loss 1.28887284\n",
      "Trained batch 609 batch loss 1.22718441 epoch total loss 1.28877151\n",
      "Trained batch 610 batch loss 1.31547475 epoch total loss 1.28881538\n",
      "Trained batch 611 batch loss 1.22604203 epoch total loss 1.28871262\n",
      "Trained batch 612 batch loss 1.25812972 epoch total loss 1.28866255\n",
      "Trained batch 613 batch loss 1.26505244 epoch total loss 1.28862405\n",
      "Trained batch 614 batch loss 1.4072988 epoch total loss 1.28881741\n",
      "Trained batch 615 batch loss 1.2627362 epoch total loss 1.28877497\n",
      "Trained batch 616 batch loss 1.27761841 epoch total loss 1.28875697\n",
      "Trained batch 617 batch loss 1.36210382 epoch total loss 1.28887582\n",
      "Trained batch 618 batch loss 1.22264528 epoch total loss 1.28876865\n",
      "Trained batch 619 batch loss 1.33368397 epoch total loss 1.28884125\n",
      "Trained batch 620 batch loss 1.30955863 epoch total loss 1.28887463\n",
      "Trained batch 621 batch loss 1.17420363 epoch total loss 1.28869\n",
      "Trained batch 622 batch loss 1.35260034 epoch total loss 1.28879273\n",
      "Trained batch 623 batch loss 1.2492516 epoch total loss 1.28872931\n",
      "Trained batch 624 batch loss 1.33808517 epoch total loss 1.28880835\n",
      "Trained batch 625 batch loss 1.37089324 epoch total loss 1.28893971\n",
      "Trained batch 626 batch loss 1.46764469 epoch total loss 1.28922522\n",
      "Trained batch 627 batch loss 1.33190715 epoch total loss 1.28929329\n",
      "Trained batch 628 batch loss 1.26734042 epoch total loss 1.28925836\n",
      "Trained batch 629 batch loss 1.18504572 epoch total loss 1.28909266\n",
      "Trained batch 630 batch loss 1.29330933 epoch total loss 1.28909945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 631 batch loss 1.33972764 epoch total loss 1.28917968\n",
      "Trained batch 632 batch loss 1.28559041 epoch total loss 1.28917396\n",
      "Trained batch 633 batch loss 1.31077576 epoch total loss 1.28920805\n",
      "Trained batch 634 batch loss 1.52150404 epoch total loss 1.2895745\n",
      "Trained batch 635 batch loss 1.36618304 epoch total loss 1.28969514\n",
      "Trained batch 636 batch loss 1.34084129 epoch total loss 1.28977549\n",
      "Trained batch 637 batch loss 1.40306866 epoch total loss 1.28995335\n",
      "Trained batch 638 batch loss 1.11014509 epoch total loss 1.28967166\n",
      "Trained batch 639 batch loss 0.995395064 epoch total loss 1.28921115\n",
      "Trained batch 640 batch loss 1.27705407 epoch total loss 1.28919208\n",
      "Trained batch 641 batch loss 1.12811351 epoch total loss 1.28894079\n",
      "Trained batch 642 batch loss 1.07567203 epoch total loss 1.28860867\n",
      "Trained batch 643 batch loss 1.03496838 epoch total loss 1.28821421\n",
      "Trained batch 644 batch loss 1.08721161 epoch total loss 1.28790212\n",
      "Trained batch 645 batch loss 1.11947012 epoch total loss 1.28764093\n",
      "Trained batch 646 batch loss 1.19933033 epoch total loss 1.2875042\n",
      "Trained batch 647 batch loss 1.25132823 epoch total loss 1.28744829\n",
      "Trained batch 648 batch loss 1.31275034 epoch total loss 1.28748739\n",
      "Trained batch 649 batch loss 1.28373063 epoch total loss 1.28748155\n",
      "Trained batch 650 batch loss 1.35653806 epoch total loss 1.28758788\n",
      "Trained batch 651 batch loss 1.28796685 epoch total loss 1.28758848\n",
      "Trained batch 652 batch loss 1.22036958 epoch total loss 1.28748536\n",
      "Trained batch 653 batch loss 1.34193373 epoch total loss 1.28756881\n",
      "Trained batch 654 batch loss 1.24689007 epoch total loss 1.28750658\n",
      "Trained batch 655 batch loss 1.34187841 epoch total loss 1.28758955\n",
      "Trained batch 656 batch loss 1.34299767 epoch total loss 1.28767407\n",
      "Trained batch 657 batch loss 1.26629508 epoch total loss 1.28764153\n",
      "Trained batch 658 batch loss 1.34969604 epoch total loss 1.28773582\n",
      "Trained batch 659 batch loss 1.36911416 epoch total loss 1.28785932\n",
      "Trained batch 660 batch loss 1.38455343 epoch total loss 1.28800583\n",
      "Trained batch 661 batch loss 1.21473849 epoch total loss 1.28789496\n",
      "Trained batch 662 batch loss 1.22073328 epoch total loss 1.28779352\n",
      "Trained batch 663 batch loss 1.35382771 epoch total loss 1.28789306\n",
      "Trained batch 664 batch loss 1.23940861 epoch total loss 1.28782\n",
      "Trained batch 665 batch loss 1.2571075 epoch total loss 1.28777373\n",
      "Trained batch 666 batch loss 1.22343898 epoch total loss 1.28767717\n",
      "Trained batch 667 batch loss 1.33587229 epoch total loss 1.28774941\n",
      "Trained batch 668 batch loss 1.24805117 epoch total loss 1.28769\n",
      "Trained batch 669 batch loss 1.29786301 epoch total loss 1.28770518\n",
      "Trained batch 670 batch loss 1.38927913 epoch total loss 1.28785682\n",
      "Trained batch 671 batch loss 1.26542544 epoch total loss 1.28782344\n",
      "Trained batch 672 batch loss 1.18611526 epoch total loss 1.28767204\n",
      "Trained batch 673 batch loss 1.0619812 epoch total loss 1.28733659\n",
      "Trained batch 674 batch loss 1.02365494 epoch total loss 1.28694546\n",
      "Trained batch 675 batch loss 1.17331958 epoch total loss 1.28677714\n",
      "Trained batch 676 batch loss 1.25680113 epoch total loss 1.28673279\n",
      "Trained batch 677 batch loss 1.52797246 epoch total loss 1.28708911\n",
      "Trained batch 678 batch loss 1.43814576 epoch total loss 1.28731191\n",
      "Trained batch 679 batch loss 1.23304236 epoch total loss 1.28723204\n",
      "Trained batch 680 batch loss 1.31078076 epoch total loss 1.28726661\n",
      "Trained batch 681 batch loss 1.23201156 epoch total loss 1.28718543\n",
      "Trained batch 682 batch loss 1.34693611 epoch total loss 1.28727305\n",
      "Trained batch 683 batch loss 1.36423898 epoch total loss 1.28738582\n",
      "Trained batch 684 batch loss 1.30015993 epoch total loss 1.28740442\n",
      "Trained batch 685 batch loss 1.28146684 epoch total loss 1.28739583\n",
      "Trained batch 686 batch loss 1.38343954 epoch total loss 1.28753579\n",
      "Trained batch 687 batch loss 1.34935617 epoch total loss 1.28762579\n",
      "Trained batch 688 batch loss 1.24418235 epoch total loss 1.28756273\n",
      "Trained batch 689 batch loss 1.17114496 epoch total loss 1.28739369\n",
      "Trained batch 690 batch loss 1.22807062 epoch total loss 1.28730774\n",
      "Trained batch 691 batch loss 1.20758569 epoch total loss 1.28719234\n",
      "Trained batch 692 batch loss 1.30488443 epoch total loss 1.28721797\n",
      "Trained batch 693 batch loss 1.1058315 epoch total loss 1.28695619\n",
      "Trained batch 694 batch loss 1.19914055 epoch total loss 1.28682971\n",
      "Trained batch 695 batch loss 1.23617756 epoch total loss 1.28675687\n",
      "Trained batch 696 batch loss 1.30218542 epoch total loss 1.28677905\n",
      "Trained batch 697 batch loss 1.3559221 epoch total loss 1.28687823\n",
      "Trained batch 698 batch loss 1.29209161 epoch total loss 1.28688574\n",
      "Trained batch 699 batch loss 1.1941278 epoch total loss 1.28675306\n",
      "Trained batch 700 batch loss 1.24859941 epoch total loss 1.28669858\n",
      "Trained batch 701 batch loss 1.22522008 epoch total loss 1.28661084\n",
      "Trained batch 702 batch loss 1.1252315 epoch total loss 1.28638101\n",
      "Trained batch 703 batch loss 1.20020962 epoch total loss 1.28625834\n",
      "Trained batch 704 batch loss 1.24056458 epoch total loss 1.28619337\n",
      "Trained batch 705 batch loss 1.266922 epoch total loss 1.28616607\n",
      "Trained batch 706 batch loss 1.14908409 epoch total loss 1.28597188\n",
      "Trained batch 707 batch loss 1.20325208 epoch total loss 1.28585494\n",
      "Trained batch 708 batch loss 1.21633613 epoch total loss 1.28575671\n",
      "Trained batch 709 batch loss 1.38000107 epoch total loss 1.28588963\n",
      "Trained batch 710 batch loss 1.33847129 epoch total loss 1.28596377\n",
      "Trained batch 711 batch loss 1.23396361 epoch total loss 1.28589058\n",
      "Trained batch 712 batch loss 1.21428919 epoch total loss 1.28579\n",
      "Trained batch 713 batch loss 1.02753758 epoch total loss 1.28542781\n",
      "Trained batch 714 batch loss 0.97207737 epoch total loss 1.284989\n",
      "Trained batch 715 batch loss 1.09054267 epoch total loss 1.28471696\n",
      "Trained batch 716 batch loss 1.18659163 epoch total loss 1.28457987\n",
      "Trained batch 717 batch loss 1.18691993 epoch total loss 1.28444362\n",
      "Trained batch 718 batch loss 1.36609817 epoch total loss 1.28455734\n",
      "Trained batch 719 batch loss 1.27589202 epoch total loss 1.2845453\n",
      "Trained batch 720 batch loss 1.31796217 epoch total loss 1.28459167\n",
      "Trained batch 721 batch loss 1.34081578 epoch total loss 1.28466964\n",
      "Trained batch 722 batch loss 1.27520752 epoch total loss 1.28465652\n",
      "Trained batch 723 batch loss 1.35690713 epoch total loss 1.28475654\n",
      "Trained batch 724 batch loss 1.25882745 epoch total loss 1.28472078\n",
      "Trained batch 725 batch loss 1.29518509 epoch total loss 1.2847352\n",
      "Trained batch 726 batch loss 1.43792796 epoch total loss 1.2849462\n",
      "Trained batch 727 batch loss 1.34970927 epoch total loss 1.28503525\n",
      "Trained batch 728 batch loss 1.24295926 epoch total loss 1.28497756\n",
      "Trained batch 729 batch loss 1.28467321 epoch total loss 1.28497708\n",
      "Trained batch 730 batch loss 1.28842354 epoch total loss 1.28498185\n",
      "Trained batch 731 batch loss 1.32758665 epoch total loss 1.28504014\n",
      "Trained batch 732 batch loss 1.35766065 epoch total loss 1.28513932\n",
      "Trained batch 733 batch loss 1.46334589 epoch total loss 1.28538239\n",
      "Trained batch 734 batch loss 1.40461886 epoch total loss 1.28554487\n",
      "Trained batch 735 batch loss 1.19975078 epoch total loss 1.28542817\n",
      "Trained batch 736 batch loss 1.21537042 epoch total loss 1.28533292\n",
      "Trained batch 737 batch loss 1.14908278 epoch total loss 1.28514814\n",
      "Trained batch 738 batch loss 0.98172 epoch total loss 1.28473699\n",
      "Trained batch 739 batch loss 1.03912234 epoch total loss 1.28440464\n",
      "Trained batch 740 batch loss 1.35425758 epoch total loss 1.28449893\n",
      "Trained batch 741 batch loss 1.33890414 epoch total loss 1.28457236\n",
      "Trained batch 742 batch loss 1.41744328 epoch total loss 1.28475142\n",
      "Trained batch 743 batch loss 1.32154393 epoch total loss 1.28480101\n",
      "Trained batch 744 batch loss 1.42051113 epoch total loss 1.2849834\n",
      "Trained batch 745 batch loss 1.37165761 epoch total loss 1.28509974\n",
      "Trained batch 746 batch loss 1.32538521 epoch total loss 1.28515375\n",
      "Trained batch 747 batch loss 1.30487251 epoch total loss 1.28518009\n",
      "Trained batch 748 batch loss 1.43480158 epoch total loss 1.28538013\n",
      "Trained batch 749 batch loss 1.26168084 epoch total loss 1.28534853\n",
      "Trained batch 750 batch loss 1.26916361 epoch total loss 1.28532696\n",
      "Trained batch 751 batch loss 1.22315431 epoch total loss 1.28524411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 752 batch loss 1.10330117 epoch total loss 1.28500211\n",
      "Trained batch 753 batch loss 1.17249799 epoch total loss 1.28485274\n",
      "Trained batch 754 batch loss 1.24099874 epoch total loss 1.28479457\n",
      "Trained batch 755 batch loss 1.2992934 epoch total loss 1.28481376\n",
      "Trained batch 756 batch loss 1.35857701 epoch total loss 1.28491139\n",
      "Trained batch 757 batch loss 1.41967344 epoch total loss 1.28508937\n",
      "Trained batch 758 batch loss 1.33588028 epoch total loss 1.28515637\n",
      "Trained batch 759 batch loss 1.3431437 epoch total loss 1.28523278\n",
      "Trained batch 760 batch loss 1.28377271 epoch total loss 1.28523088\n",
      "Trained batch 761 batch loss 1.27839661 epoch total loss 1.28522182\n",
      "Trained batch 762 batch loss 1.23428237 epoch total loss 1.28515494\n",
      "Trained batch 763 batch loss 1.22678483 epoch total loss 1.28507853\n",
      "Trained batch 764 batch loss 1.14320946 epoch total loss 1.2848928\n",
      "Trained batch 765 batch loss 1.37759352 epoch total loss 1.28501391\n",
      "Trained batch 766 batch loss 1.30238986 epoch total loss 1.28503656\n",
      "Trained batch 767 batch loss 1.23450541 epoch total loss 1.28497064\n",
      "Trained batch 768 batch loss 1.29514194 epoch total loss 1.28498399\n",
      "Trained batch 769 batch loss 1.35939312 epoch total loss 1.28508067\n",
      "Trained batch 770 batch loss 1.38972592 epoch total loss 1.28521657\n",
      "Trained batch 771 batch loss 1.38203728 epoch total loss 1.2853421\n",
      "Trained batch 772 batch loss 1.42180789 epoch total loss 1.28551888\n",
      "Trained batch 773 batch loss 1.37762666 epoch total loss 1.28563809\n",
      "Trained batch 774 batch loss 1.31675911 epoch total loss 1.28567827\n",
      "Trained batch 775 batch loss 1.18044603 epoch total loss 1.28554249\n",
      "Trained batch 776 batch loss 1.26722181 epoch total loss 1.28551888\n",
      "Trained batch 777 batch loss 1.20427275 epoch total loss 1.28541434\n",
      "Trained batch 778 batch loss 1.14780629 epoch total loss 1.28523743\n",
      "Trained batch 779 batch loss 1.26279974 epoch total loss 1.2852087\n",
      "Trained batch 780 batch loss 1.21998084 epoch total loss 1.28512502\n",
      "Trained batch 781 batch loss 1.14521492 epoch total loss 1.28494585\n",
      "Trained batch 782 batch loss 1.00853562 epoch total loss 1.28459239\n",
      "Trained batch 783 batch loss 1.42641652 epoch total loss 1.28477347\n",
      "Trained batch 784 batch loss 1.35935545 epoch total loss 1.28486872\n",
      "Trained batch 785 batch loss 1.39763188 epoch total loss 1.28501236\n",
      "Trained batch 786 batch loss 1.28300047 epoch total loss 1.28500974\n",
      "Trained batch 787 batch loss 1.39347124 epoch total loss 1.28514767\n",
      "Trained batch 788 batch loss 1.279814 epoch total loss 1.28514087\n",
      "Trained batch 789 batch loss 1.1587733 epoch total loss 1.28498065\n",
      "Trained batch 790 batch loss 1.15413356 epoch total loss 1.28481495\n",
      "Trained batch 791 batch loss 1.26907229 epoch total loss 1.28479505\n",
      "Trained batch 792 batch loss 1.31523609 epoch total loss 1.28483355\n",
      "Trained batch 793 batch loss 1.25678992 epoch total loss 1.28479815\n",
      "Trained batch 794 batch loss 1.38747215 epoch total loss 1.28492737\n",
      "Trained batch 795 batch loss 1.29443645 epoch total loss 1.28493941\n",
      "Trained batch 796 batch loss 1.33562183 epoch total loss 1.28500307\n",
      "Trained batch 797 batch loss 1.28691578 epoch total loss 1.28500533\n",
      "Trained batch 798 batch loss 1.43671918 epoch total loss 1.28519559\n",
      "Trained batch 799 batch loss 1.44862795 epoch total loss 1.2854\n",
      "Trained batch 800 batch loss 1.33439839 epoch total loss 1.28546131\n",
      "Trained batch 801 batch loss 1.41972911 epoch total loss 1.2856288\n",
      "Trained batch 802 batch loss 1.3464179 epoch total loss 1.28570461\n",
      "Trained batch 803 batch loss 1.44864678 epoch total loss 1.28590751\n",
      "Trained batch 804 batch loss 1.35527563 epoch total loss 1.2859937\n",
      "Trained batch 805 batch loss 1.25822783 epoch total loss 1.28595924\n",
      "Trained batch 806 batch loss 1.12634647 epoch total loss 1.28576112\n",
      "Trained batch 807 batch loss 1.14144778 epoch total loss 1.2855823\n",
      "Trained batch 808 batch loss 1.39285755 epoch total loss 1.2857151\n",
      "Trained batch 809 batch loss 1.35864067 epoch total loss 1.28580523\n",
      "Trained batch 810 batch loss 1.41129255 epoch total loss 1.28596008\n",
      "Trained batch 811 batch loss 1.3237834 epoch total loss 1.28600669\n",
      "Trained batch 812 batch loss 1.39008725 epoch total loss 1.28613496\n",
      "Trained batch 813 batch loss 1.27868772 epoch total loss 1.28612578\n",
      "Trained batch 814 batch loss 1.25748265 epoch total loss 1.28609049\n",
      "Trained batch 815 batch loss 1.32349789 epoch total loss 1.28613639\n",
      "Trained batch 816 batch loss 1.2543658 epoch total loss 1.28609753\n",
      "Trained batch 817 batch loss 1.26041913 epoch total loss 1.28606606\n",
      "Trained batch 818 batch loss 1.21054339 epoch total loss 1.28597379\n",
      "Trained batch 819 batch loss 1.31383312 epoch total loss 1.28600776\n",
      "Trained batch 820 batch loss 1.29587674 epoch total loss 1.2860198\n",
      "Trained batch 821 batch loss 1.24048734 epoch total loss 1.28596437\n",
      "Trained batch 822 batch loss 1.33052063 epoch total loss 1.28601861\n",
      "Trained batch 823 batch loss 1.29810047 epoch total loss 1.28603327\n",
      "Trained batch 824 batch loss 1.36463428 epoch total loss 1.28612864\n",
      "Trained batch 825 batch loss 1.20962834 epoch total loss 1.2860359\n",
      "Trained batch 826 batch loss 1.20632505 epoch total loss 1.28593934\n",
      "Trained batch 827 batch loss 1.22591233 epoch total loss 1.28586686\n",
      "Trained batch 828 batch loss 1.22225845 epoch total loss 1.28579009\n",
      "Trained batch 829 batch loss 1.30531502 epoch total loss 1.28581357\n",
      "Trained batch 830 batch loss 1.40813577 epoch total loss 1.28596091\n",
      "Trained batch 831 batch loss 1.23697245 epoch total loss 1.2859019\n",
      "Trained batch 832 batch loss 1.24670768 epoch total loss 1.28585482\n",
      "Trained batch 833 batch loss 1.29615068 epoch total loss 1.28586709\n",
      "Trained batch 834 batch loss 1.36012137 epoch total loss 1.28595614\n",
      "Trained batch 835 batch loss 1.36095738 epoch total loss 1.28604603\n",
      "Trained batch 836 batch loss 1.29959011 epoch total loss 1.28606212\n",
      "Trained batch 837 batch loss 1.22999084 epoch total loss 1.28599513\n",
      "Trained batch 838 batch loss 1.27595282 epoch total loss 1.2859832\n",
      "Trained batch 839 batch loss 1.1749903 epoch total loss 1.285851\n",
      "Trained batch 840 batch loss 1.27105045 epoch total loss 1.28583336\n",
      "Trained batch 841 batch loss 1.29411745 epoch total loss 1.28584313\n",
      "Trained batch 842 batch loss 1.22421622 epoch total loss 1.28576994\n",
      "Trained batch 843 batch loss 1.22868717 epoch total loss 1.28570211\n",
      "Trained batch 844 batch loss 1.15739536 epoch total loss 1.28555012\n",
      "Trained batch 845 batch loss 1.23829484 epoch total loss 1.28549409\n",
      "Trained batch 846 batch loss 1.29312527 epoch total loss 1.28550315\n",
      "Trained batch 847 batch loss 1.29444933 epoch total loss 1.28551364\n",
      "Trained batch 848 batch loss 1.25671256 epoch total loss 1.28547966\n",
      "Trained batch 849 batch loss 1.34979641 epoch total loss 1.28555548\n",
      "Trained batch 850 batch loss 1.41064978 epoch total loss 1.28570271\n",
      "Trained batch 851 batch loss 1.18466449 epoch total loss 1.28558397\n",
      "Trained batch 852 batch loss 1.04716957 epoch total loss 1.28530407\n",
      "Trained batch 853 batch loss 1.25606585 epoch total loss 1.28526986\n",
      "Trained batch 854 batch loss 1.25726902 epoch total loss 1.28523719\n",
      "Trained batch 855 batch loss 1.39686513 epoch total loss 1.28536773\n",
      "Trained batch 856 batch loss 1.39743817 epoch total loss 1.28549862\n",
      "Trained batch 857 batch loss 1.45783639 epoch total loss 1.28569984\n",
      "Trained batch 858 batch loss 1.32928228 epoch total loss 1.28575051\n",
      "Trained batch 859 batch loss 1.32045138 epoch total loss 1.28579092\n",
      "Trained batch 860 batch loss 1.36254764 epoch total loss 1.28588021\n",
      "Trained batch 861 batch loss 1.28281 epoch total loss 1.28587663\n",
      "Trained batch 862 batch loss 1.37262154 epoch total loss 1.28597736\n",
      "Trained batch 863 batch loss 1.27224267 epoch total loss 1.28596139\n",
      "Trained batch 864 batch loss 1.3162744 epoch total loss 1.28599644\n",
      "Trained batch 865 batch loss 1.34139311 epoch total loss 1.28606057\n",
      "Trained batch 866 batch loss 1.34624505 epoch total loss 1.28613\n",
      "Trained batch 867 batch loss 1.36861527 epoch total loss 1.2862252\n",
      "Trained batch 868 batch loss 1.22333992 epoch total loss 1.28615284\n",
      "Trained batch 869 batch loss 1.30864561 epoch total loss 1.28617859\n",
      "Trained batch 870 batch loss 1.32101262 epoch total loss 1.28621864\n",
      "Trained batch 871 batch loss 1.21426296 epoch total loss 1.28613603\n",
      "Trained batch 872 batch loss 1.22021246 epoch total loss 1.28606045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 873 batch loss 1.26583242 epoch total loss 1.28603733\n",
      "Trained batch 874 batch loss 1.20770299 epoch total loss 1.2859478\n",
      "Trained batch 875 batch loss 1.14107358 epoch total loss 1.28578222\n",
      "Trained batch 876 batch loss 1.33276772 epoch total loss 1.28583586\n",
      "Trained batch 877 batch loss 1.24987745 epoch total loss 1.28579485\n",
      "Trained batch 878 batch loss 1.22571707 epoch total loss 1.28572643\n",
      "Trained batch 879 batch loss 1.22343206 epoch total loss 1.2856555\n",
      "Trained batch 880 batch loss 1.23757339 epoch total loss 1.28560078\n",
      "Trained batch 881 batch loss 1.21055818 epoch total loss 1.28551567\n",
      "Trained batch 882 batch loss 1.23802614 epoch total loss 1.28546178\n",
      "Trained batch 883 batch loss 1.13451171 epoch total loss 1.28529096\n",
      "Trained batch 884 batch loss 1.18286157 epoch total loss 1.28517509\n",
      "Trained batch 885 batch loss 1.27901232 epoch total loss 1.28516817\n",
      "Trained batch 886 batch loss 1.17802358 epoch total loss 1.28504717\n",
      "Trained batch 887 batch loss 1.18576932 epoch total loss 1.28493524\n",
      "Trained batch 888 batch loss 1.40803337 epoch total loss 1.28507388\n",
      "Trained batch 889 batch loss 1.2364049 epoch total loss 1.28501916\n",
      "Trained batch 890 batch loss 1.19454396 epoch total loss 1.28491759\n",
      "Trained batch 891 batch loss 1.21882653 epoch total loss 1.28484344\n",
      "Trained batch 892 batch loss 1.27142227 epoch total loss 1.28482831\n",
      "Trained batch 893 batch loss 1.31732118 epoch total loss 1.28486466\n",
      "Trained batch 894 batch loss 1.30776858 epoch total loss 1.28489029\n",
      "Trained batch 895 batch loss 1.30341935 epoch total loss 1.28491104\n",
      "Trained batch 896 batch loss 1.20760536 epoch total loss 1.28482473\n",
      "Trained batch 897 batch loss 1.20772827 epoch total loss 1.2847389\n",
      "Trained batch 898 batch loss 1.21610177 epoch total loss 1.28466237\n",
      "Trained batch 899 batch loss 1.28939521 epoch total loss 1.28466773\n",
      "Trained batch 900 batch loss 1.16556716 epoch total loss 1.28453529\n",
      "Trained batch 901 batch loss 1.16456044 epoch total loss 1.28440213\n",
      "Trained batch 902 batch loss 1.2084868 epoch total loss 1.28431797\n",
      "Trained batch 903 batch loss 1.26249778 epoch total loss 1.28429377\n",
      "Trained batch 904 batch loss 1.20757365 epoch total loss 1.28420889\n",
      "Trained batch 905 batch loss 1.33233929 epoch total loss 1.28426206\n",
      "Trained batch 906 batch loss 1.27145839 epoch total loss 1.28424799\n",
      "Trained batch 907 batch loss 1.0915122 epoch total loss 1.28403556\n",
      "Trained batch 908 batch loss 1.03919506 epoch total loss 1.28376591\n",
      "Trained batch 909 batch loss 1.01810431 epoch total loss 1.28347361\n",
      "Trained batch 910 batch loss 1.18777263 epoch total loss 1.28336835\n",
      "Trained batch 911 batch loss 1.147753 epoch total loss 1.28321946\n",
      "Trained batch 912 batch loss 1.21071279 epoch total loss 1.28314\n",
      "Trained batch 913 batch loss 1.20080948 epoch total loss 1.28304982\n",
      "Trained batch 914 batch loss 1.22955704 epoch total loss 1.28299129\n",
      "Trained batch 915 batch loss 1.31392717 epoch total loss 1.28302515\n",
      "Trained batch 916 batch loss 1.40553904 epoch total loss 1.2831589\n",
      "Trained batch 917 batch loss 1.39427078 epoch total loss 1.28328\n",
      "Trained batch 918 batch loss 1.34892488 epoch total loss 1.28335154\n",
      "Trained batch 919 batch loss 1.26274419 epoch total loss 1.28332901\n",
      "Trained batch 920 batch loss 1.32493412 epoch total loss 1.28337431\n",
      "Trained batch 921 batch loss 1.19488215 epoch total loss 1.28327811\n",
      "Trained batch 922 batch loss 1.35652089 epoch total loss 1.28335762\n",
      "Trained batch 923 batch loss 1.35968208 epoch total loss 1.28344035\n",
      "Trained batch 924 batch loss 1.51836264 epoch total loss 1.28369462\n",
      "Trained batch 925 batch loss 1.41310167 epoch total loss 1.28383446\n",
      "Trained batch 926 batch loss 1.32121563 epoch total loss 1.28387475\n",
      "Trained batch 927 batch loss 1.36569536 epoch total loss 1.28396308\n",
      "Trained batch 928 batch loss 1.39508212 epoch total loss 1.28408289\n",
      "Trained batch 929 batch loss 1.42870259 epoch total loss 1.28423858\n",
      "Trained batch 930 batch loss 1.32760572 epoch total loss 1.28428519\n",
      "Trained batch 931 batch loss 1.31792283 epoch total loss 1.28432131\n",
      "Trained batch 932 batch loss 1.14616454 epoch total loss 1.28417301\n",
      "Trained batch 933 batch loss 1.32718229 epoch total loss 1.28421903\n",
      "Trained batch 934 batch loss 1.21395755 epoch total loss 1.28414392\n",
      "Trained batch 935 batch loss 1.24581087 epoch total loss 1.28410292\n",
      "Trained batch 936 batch loss 1.37659442 epoch total loss 1.28420174\n",
      "Trained batch 937 batch loss 1.37636375 epoch total loss 1.28430009\n",
      "Trained batch 938 batch loss 1.28166974 epoch total loss 1.28429723\n",
      "Trained batch 939 batch loss 1.24020267 epoch total loss 1.28425026\n",
      "Trained batch 940 batch loss 1.23517513 epoch total loss 1.28419816\n",
      "Trained batch 941 batch loss 1.26854908 epoch total loss 1.28418148\n",
      "Trained batch 942 batch loss 1.22487044 epoch total loss 1.28411853\n",
      "Trained batch 943 batch loss 1.27430093 epoch total loss 1.28410804\n",
      "Trained batch 944 batch loss 1.37215209 epoch total loss 1.28420138\n",
      "Trained batch 945 batch loss 1.33906507 epoch total loss 1.28425956\n",
      "Trained batch 946 batch loss 1.33595681 epoch total loss 1.28431416\n",
      "Trained batch 947 batch loss 1.31280279 epoch total loss 1.2843442\n",
      "Trained batch 948 batch loss 1.2885747 epoch total loss 1.28434861\n",
      "Trained batch 949 batch loss 1.36391652 epoch total loss 1.28443241\n",
      "Trained batch 950 batch loss 1.22777438 epoch total loss 1.28437281\n",
      "Trained batch 951 batch loss 1.32279384 epoch total loss 1.28441322\n",
      "Trained batch 952 batch loss 1.22448397 epoch total loss 1.28435028\n",
      "Trained batch 953 batch loss 1.14012122 epoch total loss 1.28419888\n",
      "Trained batch 954 batch loss 1.27484286 epoch total loss 1.28418911\n",
      "Trained batch 955 batch loss 1.2317816 epoch total loss 1.28413427\n",
      "Trained batch 956 batch loss 1.24426651 epoch total loss 1.28409255\n",
      "Trained batch 957 batch loss 1.05573726 epoch total loss 1.28385401\n",
      "Trained batch 958 batch loss 1.11161685 epoch total loss 1.28367424\n",
      "Trained batch 959 batch loss 1.1304543 epoch total loss 1.2835145\n",
      "Trained batch 960 batch loss 1.27601552 epoch total loss 1.28350663\n",
      "Trained batch 961 batch loss 1.21134925 epoch total loss 1.28343153\n",
      "Trained batch 962 batch loss 1.30479038 epoch total loss 1.2834537\n",
      "Trained batch 963 batch loss 1.29855144 epoch total loss 1.28346944\n",
      "Trained batch 964 batch loss 1.23478663 epoch total loss 1.28341889\n",
      "Trained batch 965 batch loss 1.32855964 epoch total loss 1.28346574\n",
      "Trained batch 966 batch loss 1.23602891 epoch total loss 1.28341663\n",
      "Trained batch 967 batch loss 1.23196912 epoch total loss 1.28336346\n",
      "Trained batch 968 batch loss 1.32350504 epoch total loss 1.28340495\n",
      "Trained batch 969 batch loss 1.11263108 epoch total loss 1.28322875\n",
      "Trained batch 970 batch loss 0.936802149 epoch total loss 1.28287148\n",
      "Trained batch 971 batch loss 0.981937051 epoch total loss 1.28256154\n",
      "Trained batch 972 batch loss 1.27570701 epoch total loss 1.28255463\n",
      "Trained batch 973 batch loss 1.35053933 epoch total loss 1.28262448\n",
      "Trained batch 974 batch loss 1.52653098 epoch total loss 1.28287494\n",
      "Trained batch 975 batch loss 1.40556026 epoch total loss 1.28300071\n",
      "Trained batch 976 batch loss 1.32196736 epoch total loss 1.28304064\n",
      "Trained batch 977 batch loss 1.3829124 epoch total loss 1.28314292\n",
      "Trained batch 978 batch loss 1.30074644 epoch total loss 1.28316092\n",
      "Trained batch 979 batch loss 1.31588411 epoch total loss 1.28319442\n",
      "Trained batch 980 batch loss 1.28112447 epoch total loss 1.28319228\n",
      "Trained batch 981 batch loss 1.36625302 epoch total loss 1.28327692\n",
      "Trained batch 982 batch loss 1.41195178 epoch total loss 1.28340793\n",
      "Trained batch 983 batch loss 1.22593141 epoch total loss 1.28334951\n",
      "Trained batch 984 batch loss 1.2929883 epoch total loss 1.28335929\n",
      "Trained batch 985 batch loss 1.49611413 epoch total loss 1.2835753\n",
      "Trained batch 986 batch loss 1.36547 epoch total loss 1.28365839\n",
      "Trained batch 987 batch loss 1.24582195 epoch total loss 1.28362\n",
      "Trained batch 988 batch loss 1.27828765 epoch total loss 1.28361464\n",
      "Trained batch 989 batch loss 1.16939831 epoch total loss 1.28349924\n",
      "Trained batch 990 batch loss 1.26079035 epoch total loss 1.28347623\n",
      "Trained batch 991 batch loss 1.29701102 epoch total loss 1.28349\n",
      "Trained batch 992 batch loss 1.33704233 epoch total loss 1.28354383\n",
      "Trained batch 993 batch loss 1.26806378 epoch total loss 1.28352833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 994 batch loss 1.18024564 epoch total loss 1.28342438\n",
      "Trained batch 995 batch loss 1.20684767 epoch total loss 1.28334737\n",
      "Trained batch 996 batch loss 1.20435309 epoch total loss 1.28326809\n",
      "Trained batch 997 batch loss 1.18119216 epoch total loss 1.28316569\n",
      "Trained batch 998 batch loss 1.32349253 epoch total loss 1.28320611\n",
      "Trained batch 999 batch loss 1.29634976 epoch total loss 1.28321922\n",
      "Trained batch 1000 batch loss 1.26783955 epoch total loss 1.28320384\n",
      "Trained batch 1001 batch loss 1.27240348 epoch total loss 1.28319311\n",
      "Trained batch 1002 batch loss 1.29284418 epoch total loss 1.28320277\n",
      "Trained batch 1003 batch loss 1.24523 epoch total loss 1.28316486\n",
      "Trained batch 1004 batch loss 1.24457848 epoch total loss 1.28312647\n",
      "Trained batch 1005 batch loss 1.34646821 epoch total loss 1.28318954\n",
      "Trained batch 1006 batch loss 1.19759488 epoch total loss 1.28310442\n",
      "Trained batch 1007 batch loss 1.18511033 epoch total loss 1.28300714\n",
      "Trained batch 1008 batch loss 1.21056533 epoch total loss 1.28293526\n",
      "Trained batch 1009 batch loss 1.19682884 epoch total loss 1.28284991\n",
      "Trained batch 1010 batch loss 1.29062772 epoch total loss 1.28285754\n",
      "Trained batch 1011 batch loss 1.40594721 epoch total loss 1.28297937\n",
      "Trained batch 1012 batch loss 1.49674261 epoch total loss 1.28319061\n",
      "Trained batch 1013 batch loss 1.45331764 epoch total loss 1.28335857\n",
      "Trained batch 1014 batch loss 1.37459242 epoch total loss 1.28344858\n",
      "Trained batch 1015 batch loss 1.26266885 epoch total loss 1.28342819\n",
      "Trained batch 1016 batch loss 1.15921986 epoch total loss 1.28330588\n",
      "Trained batch 1017 batch loss 1.03688884 epoch total loss 1.28306353\n",
      "Trained batch 1018 batch loss 1.02871192 epoch total loss 1.28281367\n",
      "Trained batch 1019 batch loss 1.09604204 epoch total loss 1.28263044\n",
      "Trained batch 1020 batch loss 1.32110929 epoch total loss 1.28266811\n",
      "Trained batch 1021 batch loss 1.19578528 epoch total loss 1.28258312\n",
      "Trained batch 1022 batch loss 1.24167156 epoch total loss 1.28254306\n",
      "Trained batch 1023 batch loss 1.33595812 epoch total loss 1.28259528\n",
      "Trained batch 1024 batch loss 1.19475257 epoch total loss 1.28250945\n",
      "Trained batch 1025 batch loss 1.22926557 epoch total loss 1.28245747\n",
      "Trained batch 1026 batch loss 1.23094058 epoch total loss 1.28240728\n",
      "Trained batch 1027 batch loss 1.18064213 epoch total loss 1.28230822\n",
      "Trained batch 1028 batch loss 1.13871 epoch total loss 1.28216851\n",
      "Trained batch 1029 batch loss 1.03747189 epoch total loss 1.28193069\n",
      "Trained batch 1030 batch loss 1.07158017 epoch total loss 1.28172648\n",
      "Trained batch 1031 batch loss 1.16493738 epoch total loss 1.28161311\n",
      "Trained batch 1032 batch loss 1.20384371 epoch total loss 1.28153777\n",
      "Trained batch 1033 batch loss 1.11471128 epoch total loss 1.28137636\n",
      "Trained batch 1034 batch loss 1.20976937 epoch total loss 1.28130698\n",
      "Trained batch 1035 batch loss 1.20034444 epoch total loss 1.28122878\n",
      "Trained batch 1036 batch loss 1.25924528 epoch total loss 1.28120756\n",
      "Trained batch 1037 batch loss 1.31454957 epoch total loss 1.28123975\n",
      "Trained batch 1038 batch loss 1.31169105 epoch total loss 1.28126907\n",
      "Trained batch 1039 batch loss 1.34633076 epoch total loss 1.28133166\n",
      "Trained batch 1040 batch loss 1.34908199 epoch total loss 1.28139687\n",
      "Trained batch 1041 batch loss 1.25000525 epoch total loss 1.28136671\n",
      "Trained batch 1042 batch loss 1.18343806 epoch total loss 1.28127277\n",
      "Trained batch 1043 batch loss 1.05567312 epoch total loss 1.2810564\n",
      "Trained batch 1044 batch loss 1.23846471 epoch total loss 1.28101563\n",
      "Trained batch 1045 batch loss 1.17798805 epoch total loss 1.28091705\n",
      "Trained batch 1046 batch loss 1.21929681 epoch total loss 1.28085816\n",
      "Trained batch 1047 batch loss 1.31338739 epoch total loss 1.28088915\n",
      "Trained batch 1048 batch loss 1.35866308 epoch total loss 1.2809633\n",
      "Trained batch 1049 batch loss 1.34517384 epoch total loss 1.28102458\n",
      "Trained batch 1050 batch loss 1.36324453 epoch total loss 1.2811029\n",
      "Trained batch 1051 batch loss 1.42679679 epoch total loss 1.28124154\n",
      "Trained batch 1052 batch loss 1.28169227 epoch total loss 1.28124201\n",
      "Trained batch 1053 batch loss 1.21526206 epoch total loss 1.28117931\n",
      "Trained batch 1054 batch loss 1.24799907 epoch total loss 1.28114784\n",
      "Trained batch 1055 batch loss 1.37760079 epoch total loss 1.28123927\n",
      "Trained batch 1056 batch loss 1.24534535 epoch total loss 1.2812053\n",
      "Trained batch 1057 batch loss 1.16487813 epoch total loss 1.28109527\n",
      "Trained batch 1058 batch loss 1.24416089 epoch total loss 1.28106034\n",
      "Trained batch 1059 batch loss 1.1907959 epoch total loss 1.2809751\n",
      "Trained batch 1060 batch loss 1.10816371 epoch total loss 1.28081203\n",
      "Trained batch 1061 batch loss 1.21458697 epoch total loss 1.28074968\n",
      "Trained batch 1062 batch loss 1.27573693 epoch total loss 1.28074491\n",
      "Trained batch 1063 batch loss 1.30735481 epoch total loss 1.28077\n",
      "Trained batch 1064 batch loss 1.38838124 epoch total loss 1.28087115\n",
      "Trained batch 1065 batch loss 1.38829803 epoch total loss 1.280972\n",
      "Trained batch 1066 batch loss 1.31695485 epoch total loss 1.28100574\n",
      "Trained batch 1067 batch loss 1.37077105 epoch total loss 1.28108978\n",
      "Trained batch 1068 batch loss 1.36830318 epoch total loss 1.28117144\n",
      "Trained batch 1069 batch loss 1.26713824 epoch total loss 1.28115833\n",
      "Trained batch 1070 batch loss 1.30339229 epoch total loss 1.28117907\n",
      "Trained batch 1071 batch loss 1.14776111 epoch total loss 1.28105438\n",
      "Trained batch 1072 batch loss 1.04596734 epoch total loss 1.28083515\n",
      "Trained batch 1073 batch loss 1.10300016 epoch total loss 1.28066945\n",
      "Trained batch 1074 batch loss 1.09640801 epoch total loss 1.28049791\n",
      "Trained batch 1075 batch loss 1.20227396 epoch total loss 1.28042519\n",
      "Trained batch 1076 batch loss 0.988546729 epoch total loss 1.28015387\n",
      "Trained batch 1077 batch loss 1.00182629 epoch total loss 1.27989542\n",
      "Trained batch 1078 batch loss 0.948829651 epoch total loss 1.27958834\n",
      "Trained batch 1079 batch loss 1.03764212 epoch total loss 1.27936411\n",
      "Trained batch 1080 batch loss 1.34805417 epoch total loss 1.27942765\n",
      "Trained batch 1081 batch loss 1.17650986 epoch total loss 1.2793324\n",
      "Trained batch 1082 batch loss 1.18931901 epoch total loss 1.27924931\n",
      "Trained batch 1083 batch loss 1.12188017 epoch total loss 1.27910388\n",
      "Trained batch 1084 batch loss 1.3112185 epoch total loss 1.27913356\n",
      "Trained batch 1085 batch loss 1.48715651 epoch total loss 1.27932537\n",
      "Trained batch 1086 batch loss 1.30716395 epoch total loss 1.279351\n",
      "Trained batch 1087 batch loss 1.22896457 epoch total loss 1.27930462\n",
      "Trained batch 1088 batch loss 1.41643167 epoch total loss 1.27943063\n",
      "Trained batch 1089 batch loss 1.20823407 epoch total loss 1.2793653\n",
      "Trained batch 1090 batch loss 1.14440274 epoch total loss 1.27924144\n",
      "Trained batch 1091 batch loss 1.31766629 epoch total loss 1.27927661\n",
      "Trained batch 1092 batch loss 1.27884912 epoch total loss 1.27927625\n",
      "Trained batch 1093 batch loss 1.2013582 epoch total loss 1.27920496\n",
      "Trained batch 1094 batch loss 1.34949815 epoch total loss 1.27926922\n",
      "Trained batch 1095 batch loss 1.23730588 epoch total loss 1.27923083\n",
      "Trained batch 1096 batch loss 1.28311086 epoch total loss 1.27923441\n",
      "Trained batch 1097 batch loss 1.16036415 epoch total loss 1.27912605\n",
      "Trained batch 1098 batch loss 1.16118944 epoch total loss 1.27901864\n",
      "Trained batch 1099 batch loss 1.31130314 epoch total loss 1.27904797\n",
      "Trained batch 1100 batch loss 1.22787321 epoch total loss 1.27900147\n",
      "Trained batch 1101 batch loss 1.32592511 epoch total loss 1.27904403\n",
      "Trained batch 1102 batch loss 1.26323092 epoch total loss 1.27902973\n",
      "Trained batch 1103 batch loss 1.229954 epoch total loss 1.27898526\n",
      "Trained batch 1104 batch loss 1.15393305 epoch total loss 1.27887189\n",
      "Trained batch 1105 batch loss 1.25644267 epoch total loss 1.27885163\n",
      "Trained batch 1106 batch loss 1.3812983 epoch total loss 1.27894437\n",
      "Trained batch 1107 batch loss 1.22189164 epoch total loss 1.27889287\n",
      "Trained batch 1108 batch loss 1.22843671 epoch total loss 1.27884722\n",
      "Trained batch 1109 batch loss 1.30636466 epoch total loss 1.27887213\n",
      "Trained batch 1110 batch loss 1.15614879 epoch total loss 1.27876151\n",
      "Trained batch 1111 batch loss 1.20608521 epoch total loss 1.27869606\n",
      "Trained batch 1112 batch loss 1.29816723 epoch total loss 1.27871358\n",
      "Trained batch 1113 batch loss 1.1582346 epoch total loss 1.27860534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1114 batch loss 1.20049584 epoch total loss 1.27853525\n",
      "Trained batch 1115 batch loss 1.22754 epoch total loss 1.27848947\n",
      "Trained batch 1116 batch loss 1.20123792 epoch total loss 1.27842033\n",
      "Trained batch 1117 batch loss 1.10105789 epoch total loss 1.27826154\n",
      "Trained batch 1118 batch loss 1.12938166 epoch total loss 1.27812839\n",
      "Trained batch 1119 batch loss 1.14902627 epoch total loss 1.27801299\n",
      "Trained batch 1120 batch loss 1.18362463 epoch total loss 1.27792871\n",
      "Trained batch 1121 batch loss 1.18755484 epoch total loss 1.27784801\n",
      "Trained batch 1122 batch loss 1.22632742 epoch total loss 1.27780211\n",
      "Trained batch 1123 batch loss 1.2328248 epoch total loss 1.27776206\n",
      "Trained batch 1124 batch loss 1.2701509 epoch total loss 1.27775526\n",
      "Trained batch 1125 batch loss 1.3477093 epoch total loss 1.27781737\n",
      "Trained batch 1126 batch loss 1.40628314 epoch total loss 1.27793145\n",
      "Trained batch 1127 batch loss 1.30087698 epoch total loss 1.27795184\n",
      "Trained batch 1128 batch loss 1.35415208 epoch total loss 1.27801931\n",
      "Trained batch 1129 batch loss 1.19366348 epoch total loss 1.27794456\n",
      "Trained batch 1130 batch loss 1.22577 epoch total loss 1.27789843\n",
      "Trained batch 1131 batch loss 1.36900985 epoch total loss 1.27797902\n",
      "Trained batch 1132 batch loss 1.18650734 epoch total loss 1.27789819\n",
      "Trained batch 1133 batch loss 1.20628285 epoch total loss 1.27783501\n",
      "Trained batch 1134 batch loss 1.23644805 epoch total loss 1.27779853\n",
      "Trained batch 1135 batch loss 1.21623957 epoch total loss 1.27774429\n",
      "Trained batch 1136 batch loss 1.19594646 epoch total loss 1.27767229\n",
      "Trained batch 1137 batch loss 1.23786211 epoch total loss 1.27763724\n",
      "Trained batch 1138 batch loss 1.36549807 epoch total loss 1.27771449\n",
      "Trained batch 1139 batch loss 1.41640675 epoch total loss 1.2778362\n",
      "Trained batch 1140 batch loss 1.39582253 epoch total loss 1.2779398\n",
      "Trained batch 1141 batch loss 1.36628675 epoch total loss 1.27801728\n",
      "Trained batch 1142 batch loss 1.2786504 epoch total loss 1.27801776\n",
      "Trained batch 1143 batch loss 1.28403759 epoch total loss 1.27802312\n",
      "Trained batch 1144 batch loss 1.28974259 epoch total loss 1.27803338\n",
      "Trained batch 1145 batch loss 1.2977457 epoch total loss 1.27805054\n",
      "Trained batch 1146 batch loss 1.15907431 epoch total loss 1.27794671\n",
      "Trained batch 1147 batch loss 1.1954 epoch total loss 1.27787483\n",
      "Trained batch 1148 batch loss 1.14484739 epoch total loss 1.27775896\n",
      "Trained batch 1149 batch loss 1.17397702 epoch total loss 1.2776686\n",
      "Trained batch 1150 batch loss 1.17400181 epoch total loss 1.27757847\n",
      "Trained batch 1151 batch loss 1.10228932 epoch total loss 1.27742612\n",
      "Trained batch 1152 batch loss 1.11501789 epoch total loss 1.2772851\n",
      "Trained batch 1153 batch loss 1.17156887 epoch total loss 1.27719343\n",
      "Trained batch 1154 batch loss 1.20138764 epoch total loss 1.27712774\n",
      "Trained batch 1155 batch loss 1.24828267 epoch total loss 1.27710283\n",
      "Trained batch 1156 batch loss 1.40315485 epoch total loss 1.2772119\n",
      "Trained batch 1157 batch loss 1.36450982 epoch total loss 1.27728724\n",
      "Trained batch 1158 batch loss 1.22894263 epoch total loss 1.27724552\n",
      "Trained batch 1159 batch loss 1.25637639 epoch total loss 1.27722752\n",
      "Trained batch 1160 batch loss 1.32119775 epoch total loss 1.27726531\n",
      "Trained batch 1161 batch loss 1.22264278 epoch total loss 1.27721834\n",
      "Trained batch 1162 batch loss 1.24833977 epoch total loss 1.27719343\n",
      "Trained batch 1163 batch loss 1.34828937 epoch total loss 1.27725458\n",
      "Trained batch 1164 batch loss 1.28719139 epoch total loss 1.27726305\n",
      "Trained batch 1165 batch loss 1.29738474 epoch total loss 1.27728033\n",
      "Trained batch 1166 batch loss 1.26437247 epoch total loss 1.27726936\n",
      "Trained batch 1167 batch loss 1.39029503 epoch total loss 1.27736616\n",
      "Trained batch 1168 batch loss 1.37373018 epoch total loss 1.27744865\n",
      "Trained batch 1169 batch loss 1.46670151 epoch total loss 1.27761054\n",
      "Trained batch 1170 batch loss 1.23069108 epoch total loss 1.27757049\n",
      "Trained batch 1171 batch loss 1.24883819 epoch total loss 1.27754593\n",
      "Trained batch 1172 batch loss 1.19443917 epoch total loss 1.277475\n",
      "Trained batch 1173 batch loss 1.17533326 epoch total loss 1.27738786\n",
      "Trained batch 1174 batch loss 1.29882336 epoch total loss 1.2774061\n",
      "Trained batch 1175 batch loss 1.28989244 epoch total loss 1.27741683\n",
      "Trained batch 1176 batch loss 1.34712029 epoch total loss 1.27747607\n",
      "Trained batch 1177 batch loss 1.22552013 epoch total loss 1.27743196\n",
      "Trained batch 1178 batch loss 1.1694572 epoch total loss 1.27734017\n",
      "Trained batch 1179 batch loss 1.19757867 epoch total loss 1.27727258\n",
      "Trained batch 1180 batch loss 1.0973829 epoch total loss 1.27712023\n",
      "Trained batch 1181 batch loss 1.19342113 epoch total loss 1.27704942\n",
      "Trained batch 1182 batch loss 1.09335196 epoch total loss 1.27689397\n",
      "Trained batch 1183 batch loss 1.17320251 epoch total loss 1.27680635\n",
      "Trained batch 1184 batch loss 1.28707337 epoch total loss 1.27681506\n",
      "Trained batch 1185 batch loss 1.09498572 epoch total loss 1.27666163\n",
      "Trained batch 1186 batch loss 1.19948959 epoch total loss 1.27659655\n",
      "Trained batch 1187 batch loss 1.17594862 epoch total loss 1.27651167\n",
      "Trained batch 1188 batch loss 1.15123761 epoch total loss 1.27640629\n",
      "Trained batch 1189 batch loss 1.24887848 epoch total loss 1.27638304\n",
      "Trained batch 1190 batch loss 1.30278707 epoch total loss 1.27640522\n",
      "Trained batch 1191 batch loss 1.24028111 epoch total loss 1.27637482\n",
      "Trained batch 1192 batch loss 1.24799919 epoch total loss 1.27635109\n",
      "Trained batch 1193 batch loss 1.25950027 epoch total loss 1.27633703\n",
      "Trained batch 1194 batch loss 1.26047063 epoch total loss 1.2763238\n",
      "Trained batch 1195 batch loss 1.32527173 epoch total loss 1.27636468\n",
      "Trained batch 1196 batch loss 1.38795626 epoch total loss 1.27645802\n",
      "Trained batch 1197 batch loss 1.22594988 epoch total loss 1.27641582\n",
      "Trained batch 1198 batch loss 1.12137973 epoch total loss 1.27628636\n",
      "Trained batch 1199 batch loss 1.05296183 epoch total loss 1.27610016\n",
      "Trained batch 1200 batch loss 1.21602619 epoch total loss 1.27605009\n",
      "Trained batch 1201 batch loss 1.32991195 epoch total loss 1.27609503\n",
      "Trained batch 1202 batch loss 1.33976185 epoch total loss 1.27614796\n",
      "Trained batch 1203 batch loss 1.240309 epoch total loss 1.27611816\n",
      "Trained batch 1204 batch loss 1.24109745 epoch total loss 1.27608907\n",
      "Trained batch 1205 batch loss 1.27319324 epoch total loss 1.27608669\n",
      "Trained batch 1206 batch loss 1.22965395 epoch total loss 1.27604818\n",
      "Trained batch 1207 batch loss 1.25731969 epoch total loss 1.27603269\n",
      "Trained batch 1208 batch loss 1.35696077 epoch total loss 1.27609956\n",
      "Trained batch 1209 batch loss 1.40177345 epoch total loss 1.27620351\n",
      "Trained batch 1210 batch loss 1.32849944 epoch total loss 1.27624679\n",
      "Trained batch 1211 batch loss 1.1555444 epoch total loss 1.27614701\n",
      "Trained batch 1212 batch loss 1.1986692 epoch total loss 1.27608311\n",
      "Trained batch 1213 batch loss 1.13966858 epoch total loss 1.27597058\n",
      "Trained batch 1214 batch loss 1.29274845 epoch total loss 1.27598441\n",
      "Trained batch 1215 batch loss 1.32173848 epoch total loss 1.27602208\n",
      "Trained batch 1216 batch loss 1.38692451 epoch total loss 1.27611327\n",
      "Trained batch 1217 batch loss 1.37968612 epoch total loss 1.27619839\n",
      "Trained batch 1218 batch loss 1.48129559 epoch total loss 1.27636683\n",
      "Trained batch 1219 batch loss 1.45298958 epoch total loss 1.27651167\n",
      "Trained batch 1220 batch loss 1.23181129 epoch total loss 1.27647507\n",
      "Trained batch 1221 batch loss 1.2522887 epoch total loss 1.27645528\n",
      "Trained batch 1222 batch loss 1.28541136 epoch total loss 1.27646255\n",
      "Trained batch 1223 batch loss 1.18698084 epoch total loss 1.27638948\n",
      "Trained batch 1224 batch loss 1.31132603 epoch total loss 1.27641797\n",
      "Trained batch 1225 batch loss 1.28009558 epoch total loss 1.27642095\n",
      "Trained batch 1226 batch loss 1.12367976 epoch total loss 1.27629638\n",
      "Trained batch 1227 batch loss 1.13387132 epoch total loss 1.27618039\n",
      "Trained batch 1228 batch loss 1.16650403 epoch total loss 1.27609098\n",
      "Trained batch 1229 batch loss 1.03388727 epoch total loss 1.27589405\n",
      "Trained batch 1230 batch loss 1.29163361 epoch total loss 1.2759068\n",
      "Trained batch 1231 batch loss 1.51720369 epoch total loss 1.27610278\n",
      "Trained batch 1232 batch loss 1.5749588 epoch total loss 1.27634537\n",
      "Trained batch 1233 batch loss 1.48178601 epoch total loss 1.27651203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1234 batch loss 1.34497118 epoch total loss 1.27656746\n",
      "Trained batch 1235 batch loss 1.37384892 epoch total loss 1.27664638\n",
      "Trained batch 1236 batch loss 1.41813529 epoch total loss 1.2767607\n",
      "Trained batch 1237 batch loss 1.32038856 epoch total loss 1.2767961\n",
      "Trained batch 1238 batch loss 1.2581408 epoch total loss 1.27678108\n",
      "Trained batch 1239 batch loss 1.25883186 epoch total loss 1.27676654\n",
      "Trained batch 1240 batch loss 1.12133551 epoch total loss 1.27664113\n",
      "Trained batch 1241 batch loss 1.15154743 epoch total loss 1.27654028\n",
      "Trained batch 1242 batch loss 1.20709729 epoch total loss 1.27648449\n",
      "Trained batch 1243 batch loss 1.29178798 epoch total loss 1.27649677\n",
      "Trained batch 1244 batch loss 1.34545362 epoch total loss 1.2765522\n",
      "Trained batch 1245 batch loss 1.23099065 epoch total loss 1.27651548\n",
      "Trained batch 1246 batch loss 1.21393371 epoch total loss 1.2764653\n",
      "Trained batch 1247 batch loss 1.22634721 epoch total loss 1.27642512\n",
      "Trained batch 1248 batch loss 1.26356113 epoch total loss 1.27641487\n",
      "Trained batch 1249 batch loss 1.32459664 epoch total loss 1.27645338\n",
      "Trained batch 1250 batch loss 1.20661402 epoch total loss 1.27639759\n",
      "Trained batch 1251 batch loss 1.29602516 epoch total loss 1.2764132\n",
      "Trained batch 1252 batch loss 1.28165412 epoch total loss 1.27641737\n",
      "Trained batch 1253 batch loss 1.23482323 epoch total loss 1.27638423\n",
      "Trained batch 1254 batch loss 1.15778255 epoch total loss 1.2762897\n",
      "Trained batch 1255 batch loss 1.23067307 epoch total loss 1.27625334\n",
      "Trained batch 1256 batch loss 1.22317493 epoch total loss 1.27621114\n",
      "Trained batch 1257 batch loss 1.22889113 epoch total loss 1.27617347\n",
      "Trained batch 1258 batch loss 1.41328 epoch total loss 1.27628243\n",
      "Trained batch 1259 batch loss 1.48973048 epoch total loss 1.27645206\n",
      "Trained batch 1260 batch loss 1.30497575 epoch total loss 1.2764746\n",
      "Trained batch 1261 batch loss 1.26940525 epoch total loss 1.27646899\n",
      "Trained batch 1262 batch loss 1.44138193 epoch total loss 1.27659976\n",
      "Trained batch 1263 batch loss 1.40359747 epoch total loss 1.27670026\n",
      "Trained batch 1264 batch loss 1.28020239 epoch total loss 1.276703\n",
      "Trained batch 1265 batch loss 1.24543619 epoch total loss 1.27667832\n",
      "Trained batch 1266 batch loss 1.12143385 epoch total loss 1.27655566\n",
      "Trained batch 1267 batch loss 1.29087508 epoch total loss 1.27656698\n",
      "Trained batch 1268 batch loss 1.3245213 epoch total loss 1.27660477\n",
      "Trained batch 1269 batch loss 1.28617525 epoch total loss 1.27661228\n",
      "Trained batch 1270 batch loss 1.33345592 epoch total loss 1.2766571\n",
      "Trained batch 1271 batch loss 1.31721389 epoch total loss 1.27668905\n",
      "Trained batch 1272 batch loss 1.34674263 epoch total loss 1.27674413\n",
      "Trained batch 1273 batch loss 1.38225627 epoch total loss 1.27682698\n",
      "Trained batch 1274 batch loss 1.23212028 epoch total loss 1.27679193\n",
      "Trained batch 1275 batch loss 1.14735615 epoch total loss 1.27669036\n",
      "Trained batch 1276 batch loss 1.16985357 epoch total loss 1.27660668\n",
      "Trained batch 1277 batch loss 1.28341448 epoch total loss 1.27661204\n",
      "Trained batch 1278 batch loss 1.36874068 epoch total loss 1.27668417\n",
      "Trained batch 1279 batch loss 1.1761874 epoch total loss 1.27660549\n",
      "Trained batch 1280 batch loss 1.44706511 epoch total loss 1.27673864\n",
      "Trained batch 1281 batch loss 1.27583659 epoch total loss 1.27673793\n",
      "Trained batch 1282 batch loss 1.33011436 epoch total loss 1.27677953\n",
      "Trained batch 1283 batch loss 1.21724689 epoch total loss 1.27673316\n",
      "Trained batch 1284 batch loss 1.21914744 epoch total loss 1.27668834\n",
      "Trained batch 1285 batch loss 1.26186216 epoch total loss 1.27667677\n",
      "Trained batch 1286 batch loss 1.27365065 epoch total loss 1.27667451\n",
      "Trained batch 1287 batch loss 1.06273353 epoch total loss 1.27650821\n",
      "Trained batch 1288 batch loss 1.13467681 epoch total loss 1.27639806\n",
      "Trained batch 1289 batch loss 1.04225302 epoch total loss 1.27621639\n",
      "Trained batch 1290 batch loss 1.15737367 epoch total loss 1.27612424\n",
      "Trained batch 1291 batch loss 1.18349767 epoch total loss 1.27605247\n",
      "Trained batch 1292 batch loss 1.14319539 epoch total loss 1.27594972\n",
      "Trained batch 1293 batch loss 1.18203604 epoch total loss 1.275877\n",
      "Trained batch 1294 batch loss 1.18613946 epoch total loss 1.27580774\n",
      "Trained batch 1295 batch loss 1.27054095 epoch total loss 1.27580357\n",
      "Trained batch 1296 batch loss 1.16636169 epoch total loss 1.27571917\n",
      "Trained batch 1297 batch loss 1.17600965 epoch total loss 1.27564228\n",
      "Trained batch 1298 batch loss 1.25748968 epoch total loss 1.27562833\n",
      "Trained batch 1299 batch loss 1.31705713 epoch total loss 1.27566016\n",
      "Trained batch 1300 batch loss 1.29475367 epoch total loss 1.27567482\n",
      "Trained batch 1301 batch loss 1.26704133 epoch total loss 1.27566826\n",
      "Trained batch 1302 batch loss 1.35062015 epoch total loss 1.27572584\n",
      "Trained batch 1303 batch loss 1.2325536 epoch total loss 1.2756927\n",
      "Trained batch 1304 batch loss 1.23202777 epoch total loss 1.2756592\n",
      "Trained batch 1305 batch loss 1.20807385 epoch total loss 1.27560747\n",
      "Trained batch 1306 batch loss 1.17888379 epoch total loss 1.27553332\n",
      "Trained batch 1307 batch loss 1.13437629 epoch total loss 1.27542531\n",
      "Trained batch 1308 batch loss 1.15011358 epoch total loss 1.27532959\n",
      "Trained batch 1309 batch loss 1.13130951 epoch total loss 1.27521956\n",
      "Trained batch 1310 batch loss 1.10544968 epoch total loss 1.27509\n",
      "Trained batch 1311 batch loss 1.1102463 epoch total loss 1.27496421\n",
      "Trained batch 1312 batch loss 1.19254553 epoch total loss 1.27490139\n",
      "Trained batch 1313 batch loss 1.05980301 epoch total loss 1.2747376\n",
      "Trained batch 1314 batch loss 1.30416775 epoch total loss 1.27476\n",
      "Trained batch 1315 batch loss 1.19717479 epoch total loss 1.274701\n",
      "Trained batch 1316 batch loss 1.30999291 epoch total loss 1.27472782\n",
      "Trained batch 1317 batch loss 1.25119686 epoch total loss 1.27470994\n",
      "Trained batch 1318 batch loss 1.32499146 epoch total loss 1.27474809\n",
      "Trained batch 1319 batch loss 1.27584851 epoch total loss 1.27474892\n",
      "Trained batch 1320 batch loss 1.16311979 epoch total loss 1.27466428\n",
      "Trained batch 1321 batch loss 1.21639371 epoch total loss 1.27462018\n",
      "Trained batch 1322 batch loss 1.34936869 epoch total loss 1.2746768\n",
      "Trained batch 1323 batch loss 1.33101332 epoch total loss 1.27471936\n",
      "Trained batch 1324 batch loss 1.19535708 epoch total loss 1.2746594\n",
      "Trained batch 1325 batch loss 1.23824954 epoch total loss 1.27463198\n",
      "Trained batch 1326 batch loss 1.37647283 epoch total loss 1.27470875\n",
      "Trained batch 1327 batch loss 1.28615522 epoch total loss 1.27471733\n",
      "Trained batch 1328 batch loss 1.24098325 epoch total loss 1.27469194\n",
      "Trained batch 1329 batch loss 1.20467293 epoch total loss 1.27463925\n",
      "Trained batch 1330 batch loss 1.25410426 epoch total loss 1.27462387\n",
      "Trained batch 1331 batch loss 1.28614736 epoch total loss 1.27463257\n",
      "Trained batch 1332 batch loss 1.21496582 epoch total loss 1.27458775\n",
      "Trained batch 1333 batch loss 1.07229674 epoch total loss 1.274436\n",
      "Trained batch 1334 batch loss 1.10041738 epoch total loss 1.27430558\n",
      "Trained batch 1335 batch loss 1.28623891 epoch total loss 1.27431452\n",
      "Trained batch 1336 batch loss 1.24166679 epoch total loss 1.27429008\n",
      "Trained batch 1337 batch loss 1.16527605 epoch total loss 1.27420855\n",
      "Trained batch 1338 batch loss 1.21247447 epoch total loss 1.27416241\n",
      "Trained batch 1339 batch loss 1.35243893 epoch total loss 1.27422082\n",
      "Trained batch 1340 batch loss 1.29348207 epoch total loss 1.27423525\n",
      "Trained batch 1341 batch loss 1.32560921 epoch total loss 1.27427351\n",
      "Trained batch 1342 batch loss 1.30785477 epoch total loss 1.27429855\n",
      "Trained batch 1343 batch loss 1.42100883 epoch total loss 1.27440774\n",
      "Trained batch 1344 batch loss 1.47046018 epoch total loss 1.27455366\n",
      "Trained batch 1345 batch loss 1.21425223 epoch total loss 1.27450883\n",
      "Trained batch 1346 batch loss 1.48775125 epoch total loss 1.27466726\n",
      "Trained batch 1347 batch loss 1.40435576 epoch total loss 1.27476346\n",
      "Trained batch 1348 batch loss 1.33294392 epoch total loss 1.27480662\n",
      "Trained batch 1349 batch loss 1.42910576 epoch total loss 1.27492094\n",
      "Trained batch 1350 batch loss 1.29245567 epoch total loss 1.27493393\n",
      "Trained batch 1351 batch loss 1.2923696 epoch total loss 1.27494693\n",
      "Trained batch 1352 batch loss 1.16913736 epoch total loss 1.27486861\n",
      "Trained batch 1353 batch loss 1.29913473 epoch total loss 1.27488661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1354 batch loss 1.26409864 epoch total loss 1.27487862\n",
      "Trained batch 1355 batch loss 1.24972546 epoch total loss 1.27486014\n",
      "Trained batch 1356 batch loss 1.22061682 epoch total loss 1.27482009\n",
      "Trained batch 1357 batch loss 1.30317545 epoch total loss 1.27484095\n",
      "Trained batch 1358 batch loss 1.21167886 epoch total loss 1.27479446\n",
      "Trained batch 1359 batch loss 1.21926486 epoch total loss 1.27475357\n",
      "Trained batch 1360 batch loss 1.1197648 epoch total loss 1.27463961\n",
      "Trained batch 1361 batch loss 1.06721687 epoch total loss 1.27448726\n",
      "Trained batch 1362 batch loss 1.13699675 epoch total loss 1.27438629\n",
      "Trained batch 1363 batch loss 1.25284374 epoch total loss 1.27437043\n",
      "Trained batch 1364 batch loss 1.44707036 epoch total loss 1.27449703\n",
      "Trained batch 1365 batch loss 1.36271346 epoch total loss 1.27456164\n",
      "Trained batch 1366 batch loss 1.31908369 epoch total loss 1.27459419\n",
      "Trained batch 1367 batch loss 1.35694671 epoch total loss 1.27465451\n",
      "Trained batch 1368 batch loss 1.31242883 epoch total loss 1.27468204\n",
      "Trained batch 1369 batch loss 1.28102815 epoch total loss 1.27468669\n",
      "Trained batch 1370 batch loss 1.25561154 epoch total loss 1.27467275\n",
      "Trained batch 1371 batch loss 1.25668883 epoch total loss 1.27465963\n",
      "Trained batch 1372 batch loss 1.2867806 epoch total loss 1.27466846\n",
      "Trained batch 1373 batch loss 1.30590892 epoch total loss 1.27469122\n",
      "Trained batch 1374 batch loss 1.19089031 epoch total loss 1.27463019\n",
      "Trained batch 1375 batch loss 1.19854569 epoch total loss 1.27457488\n",
      "Trained batch 1376 batch loss 1.20507097 epoch total loss 1.27452433\n",
      "Trained batch 1377 batch loss 1.24890363 epoch total loss 1.27450573\n",
      "Trained batch 1378 batch loss 1.36933231 epoch total loss 1.27457464\n",
      "Trained batch 1379 batch loss 1.28523159 epoch total loss 1.27458239\n",
      "Trained batch 1380 batch loss 1.21210337 epoch total loss 1.27453709\n",
      "Trained batch 1381 batch loss 1.32971406 epoch total loss 1.27457702\n",
      "Trained batch 1382 batch loss 1.27426493 epoch total loss 1.2745769\n",
      "Trained batch 1383 batch loss 1.11858988 epoch total loss 1.27446401\n",
      "Trained batch 1384 batch loss 1.09529221 epoch total loss 1.27433455\n",
      "Trained batch 1385 batch loss 1.24282074 epoch total loss 1.27431178\n",
      "Trained batch 1386 batch loss 1.3843739 epoch total loss 1.27439129\n",
      "Trained batch 1387 batch loss 1.31088626 epoch total loss 1.27441764\n",
      "Trained batch 1388 batch loss 1.301337 epoch total loss 1.27443707\n",
      "Epoch 3 train loss 1.2744370698928833\n",
      "Validated batch 1 batch loss 1.3039242\n",
      "Validated batch 2 batch loss 1.19542718\n",
      "Validated batch 3 batch loss 1.20365393\n",
      "Validated batch 4 batch loss 1.21649075\n",
      "Validated batch 5 batch loss 1.3416431\n",
      "Validated batch 6 batch loss 1.34944534\n",
      "Validated batch 7 batch loss 1.20743501\n",
      "Validated batch 8 batch loss 1.24598849\n",
      "Validated batch 9 batch loss 1.26609159\n",
      "Validated batch 10 batch loss 1.2502141\n",
      "Validated batch 11 batch loss 1.27576637\n",
      "Validated batch 12 batch loss 1.14678144\n",
      "Validated batch 13 batch loss 1.41608787\n",
      "Validated batch 14 batch loss 1.1608758\n",
      "Validated batch 15 batch loss 1.26184571\n",
      "Validated batch 16 batch loss 1.2961297\n",
      "Validated batch 17 batch loss 1.33743525\n",
      "Validated batch 18 batch loss 1.08838522\n",
      "Validated batch 19 batch loss 1.30123317\n",
      "Validated batch 20 batch loss 1.16377544\n",
      "Validated batch 21 batch loss 1.29688275\n",
      "Validated batch 22 batch loss 1.24341321\n",
      "Validated batch 23 batch loss 1.26252019\n",
      "Validated batch 24 batch loss 1.23960555\n",
      "Validated batch 25 batch loss 1.21079743\n",
      "Validated batch 26 batch loss 1.18922412\n",
      "Validated batch 27 batch loss 1.21434343\n",
      "Validated batch 28 batch loss 1.17510557\n",
      "Validated batch 29 batch loss 1.3391819\n",
      "Validated batch 30 batch loss 1.22231\n",
      "Validated batch 31 batch loss 1.11958706\n",
      "Validated batch 32 batch loss 1.17046893\n",
      "Validated batch 33 batch loss 1.20028543\n",
      "Validated batch 34 batch loss 1.18641698\n",
      "Validated batch 35 batch loss 1.20027351\n",
      "Validated batch 36 batch loss 1.22100186\n",
      "Validated batch 37 batch loss 1.19557798\n",
      "Validated batch 38 batch loss 1.330428\n",
      "Validated batch 39 batch loss 1.29851151\n",
      "Validated batch 40 batch loss 1.23764539\n",
      "Validated batch 41 batch loss 1.30533719\n",
      "Validated batch 42 batch loss 1.06420159\n",
      "Validated batch 43 batch loss 1.17199981\n",
      "Validated batch 44 batch loss 1.11503541\n",
      "Validated batch 45 batch loss 1.27399564\n",
      "Validated batch 46 batch loss 1.41145229\n",
      "Validated batch 47 batch loss 1.15390825\n",
      "Validated batch 48 batch loss 1.26762545\n",
      "Validated batch 49 batch loss 1.28198373\n",
      "Validated batch 50 batch loss 1.17293632\n",
      "Validated batch 51 batch loss 1.29898143\n",
      "Validated batch 52 batch loss 1.39875197\n",
      "Validated batch 53 batch loss 1.07980883\n",
      "Validated batch 54 batch loss 1.24725342\n",
      "Validated batch 55 batch loss 1.22465038\n",
      "Validated batch 56 batch loss 1.2800827\n",
      "Validated batch 57 batch loss 1.2784152\n",
      "Validated batch 58 batch loss 1.0986073\n",
      "Validated batch 59 batch loss 1.10244226\n",
      "Validated batch 60 batch loss 1.2080884\n",
      "Validated batch 61 batch loss 1.1831888\n",
      "Validated batch 62 batch loss 1.17183638\n",
      "Validated batch 63 batch loss 1.20681465\n",
      "Validated batch 64 batch loss 1.14210296\n",
      "Validated batch 65 batch loss 1.26598704\n",
      "Validated batch 66 batch loss 1.28852248\n",
      "Validated batch 67 batch loss 1.2732687\n",
      "Validated batch 68 batch loss 1.25104952\n",
      "Validated batch 69 batch loss 1.10916615\n",
      "Validated batch 70 batch loss 1.20546675\n",
      "Validated batch 71 batch loss 1.1447264\n",
      "Validated batch 72 batch loss 1.24275\n",
      "Validated batch 73 batch loss 1.18652248\n",
      "Validated batch 74 batch loss 1.19657135\n",
      "Validated batch 75 batch loss 1.24566209\n",
      "Validated batch 76 batch loss 1.2481612\n",
      "Validated batch 77 batch loss 1.20465326\n",
      "Validated batch 78 batch loss 1.27913249\n",
      "Validated batch 79 batch loss 1.19275546\n",
      "Validated batch 80 batch loss 1.2784611\n",
      "Validated batch 81 batch loss 1.30035555\n",
      "Validated batch 82 batch loss 1.20537591\n",
      "Validated batch 83 batch loss 1.19950271\n",
      "Validated batch 84 batch loss 1.28096259\n",
      "Validated batch 85 batch loss 1.20502591\n",
      "Validated batch 86 batch loss 1.39734364\n",
      "Validated batch 87 batch loss 1.27019954\n",
      "Validated batch 88 batch loss 1.16725802\n",
      "Validated batch 89 batch loss 1.29841983\n",
      "Validated batch 90 batch loss 1.25240111\n",
      "Validated batch 91 batch loss 1.16210938\n",
      "Validated batch 92 batch loss 1.25595629\n",
      "Validated batch 93 batch loss 1.38490844\n",
      "Validated batch 94 batch loss 1.24206758\n",
      "Validated batch 95 batch loss 1.22883964\n",
      "Validated batch 96 batch loss 1.25105751\n",
      "Validated batch 97 batch loss 1.18095791\n",
      "Validated batch 98 batch loss 1.24621928\n",
      "Validated batch 99 batch loss 1.27752244\n",
      "Validated batch 100 batch loss 1.20849538\n",
      "Validated batch 101 batch loss 1.18375802\n",
      "Validated batch 102 batch loss 1.24345982\n",
      "Validated batch 103 batch loss 1.26938272\n",
      "Validated batch 104 batch loss 1.34303331\n",
      "Validated batch 105 batch loss 1.21959698\n",
      "Validated batch 106 batch loss 1.14323962\n",
      "Validated batch 107 batch loss 1.1735599\n",
      "Validated batch 108 batch loss 1.26251256\n",
      "Validated batch 109 batch loss 1.17800951\n",
      "Validated batch 110 batch loss 1.31292272\n",
      "Validated batch 111 batch loss 1.29833698\n",
      "Validated batch 112 batch loss 1.47578311\n",
      "Validated batch 113 batch loss 1.38298476\n",
      "Validated batch 114 batch loss 1.23317921\n",
      "Validated batch 115 batch loss 1.14624012\n",
      "Validated batch 116 batch loss 1.13191259\n",
      "Validated batch 117 batch loss 1.17733502\n",
      "Validated batch 118 batch loss 1.20782208\n",
      "Validated batch 119 batch loss 1.1549263\n",
      "Validated batch 120 batch loss 1.17151546\n",
      "Validated batch 121 batch loss 1.3368876\n",
      "Validated batch 122 batch loss 1.22602069\n",
      "Validated batch 123 batch loss 1.33306766\n",
      "Validated batch 124 batch loss 1.31084943\n",
      "Validated batch 125 batch loss 1.31244397\n",
      "Validated batch 126 batch loss 1.26074636\n",
      "Validated batch 127 batch loss 1.40204954\n",
      "Validated batch 128 batch loss 1.28621221\n",
      "Validated batch 129 batch loss 1.32923222\n",
      "Validated batch 130 batch loss 1.33165383\n",
      "Validated batch 131 batch loss 1.36794221\n",
      "Validated batch 132 batch loss 1.29693246\n",
      "Validated batch 133 batch loss 1.18455195\n",
      "Validated batch 134 batch loss 1.23161256\n",
      "Validated batch 135 batch loss 1.28400278\n",
      "Validated batch 136 batch loss 1.32364368\n",
      "Validated batch 137 batch loss 1.25197315\n",
      "Validated batch 138 batch loss 1.26574111\n",
      "Validated batch 139 batch loss 1.22706974\n",
      "Validated batch 140 batch loss 1.31365705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 141 batch loss 1.24761927\n",
      "Validated batch 142 batch loss 1.18477893\n",
      "Validated batch 143 batch loss 1.26913571\n",
      "Validated batch 144 batch loss 1.3007344\n",
      "Validated batch 145 batch loss 1.33356309\n",
      "Validated batch 146 batch loss 1.35008574\n",
      "Validated batch 147 batch loss 1.29265785\n",
      "Validated batch 148 batch loss 1.23579645\n",
      "Validated batch 149 batch loss 1.28033221\n",
      "Validated batch 150 batch loss 1.277089\n",
      "Validated batch 151 batch loss 1.2447598\n",
      "Validated batch 152 batch loss 1.29672873\n",
      "Validated batch 153 batch loss 1.3121177\n",
      "Validated batch 154 batch loss 1.26212287\n",
      "Validated batch 155 batch loss 1.36787713\n",
      "Validated batch 156 batch loss 1.21930194\n",
      "Validated batch 157 batch loss 1.23328722\n",
      "Validated batch 158 batch loss 1.19533837\n",
      "Validated batch 159 batch loss 1.1032213\n",
      "Validated batch 160 batch loss 1.32577682\n",
      "Validated batch 161 batch loss 1.19933975\n",
      "Validated batch 162 batch loss 1.25981379\n",
      "Validated batch 163 batch loss 1.28117788\n",
      "Validated batch 164 batch loss 1.17515993\n",
      "Validated batch 165 batch loss 1.24686646\n",
      "Validated batch 166 batch loss 1.2424053\n",
      "Validated batch 167 batch loss 1.23384261\n",
      "Validated batch 168 batch loss 1.28356695\n",
      "Validated batch 169 batch loss 1.21585882\n",
      "Validated batch 170 batch loss 1.16179609\n",
      "Validated batch 171 batch loss 1.39057529\n",
      "Validated batch 172 batch loss 1.20822167\n",
      "Validated batch 173 batch loss 1.13623345\n",
      "Validated batch 174 batch loss 1.1976825\n",
      "Validated batch 175 batch loss 1.40190911\n",
      "Validated batch 176 batch loss 1.35928428\n",
      "Validated batch 177 batch loss 1.35146642\n",
      "Validated batch 178 batch loss 1.24042344\n",
      "Validated batch 179 batch loss 1.38696575\n",
      "Validated batch 180 batch loss 1.24177814\n",
      "Validated batch 181 batch loss 1.35082865\n",
      "Validated batch 182 batch loss 1.31154025\n",
      "Validated batch 183 batch loss 1.05966449\n",
      "Validated batch 184 batch loss 1.17337346\n",
      "Validated batch 185 batch loss 1.42303252\n",
      "Epoch 3 val loss 1.2475699186325073\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-3-loss-1.2476.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.21886659 epoch total loss 1.21886659\n",
      "Trained batch 2 batch loss 1.17469513 epoch total loss 1.19678092\n",
      "Trained batch 3 batch loss 1.14505613 epoch total loss 1.17953932\n",
      "Trained batch 4 batch loss 1.20636761 epoch total loss 1.1862464\n",
      "Trained batch 5 batch loss 1.1640048 epoch total loss 1.1817981\n",
      "Trained batch 6 batch loss 1.1498636 epoch total loss 1.17647564\n",
      "Trained batch 7 batch loss 1.15434337 epoch total loss 1.17331398\n",
      "Trained batch 8 batch loss 1.27501678 epoch total loss 1.18602681\n",
      "Trained batch 9 batch loss 1.23184061 epoch total loss 1.19111717\n",
      "Trained batch 10 batch loss 1.17263031 epoch total loss 1.18926847\n",
      "Trained batch 11 batch loss 1.24709499 epoch total loss 1.19452548\n",
      "Trained batch 12 batch loss 1.25363946 epoch total loss 1.19945157\n",
      "Trained batch 13 batch loss 1.27145171 epoch total loss 1.20499015\n",
      "Trained batch 14 batch loss 1.22380447 epoch total loss 1.20633399\n",
      "Trained batch 15 batch loss 1.2410363 epoch total loss 1.20864737\n",
      "Trained batch 16 batch loss 1.17045474 epoch total loss 1.20626032\n",
      "Trained batch 17 batch loss 1.18385315 epoch total loss 1.20494223\n",
      "Trained batch 18 batch loss 1.1655947 epoch total loss 1.20275629\n",
      "Trained batch 19 batch loss 1.21853328 epoch total loss 1.20358658\n",
      "Trained batch 20 batch loss 1.13238454 epoch total loss 1.20002651\n",
      "Trained batch 21 batch loss 1.15958953 epoch total loss 1.19810092\n",
      "Trained batch 22 batch loss 1.20035279 epoch total loss 1.19820333\n",
      "Trained batch 23 batch loss 1.161569 epoch total loss 1.19661057\n",
      "Trained batch 24 batch loss 1.09634423 epoch total loss 1.19243276\n",
      "Trained batch 25 batch loss 1.02705836 epoch total loss 1.18581784\n",
      "Trained batch 26 batch loss 1.06514359 epoch total loss 1.18117642\n",
      "Trained batch 27 batch loss 1.21931672 epoch total loss 1.18258905\n",
      "Trained batch 28 batch loss 1.22451162 epoch total loss 1.1840862\n",
      "Trained batch 29 batch loss 1.26962674 epoch total loss 1.18703592\n",
      "Trained batch 30 batch loss 1.29538441 epoch total loss 1.19064748\n",
      "Trained batch 31 batch loss 1.34995985 epoch total loss 1.1957866\n",
      "Trained batch 32 batch loss 1.11367893 epoch total loss 1.19322073\n",
      "Trained batch 33 batch loss 1.19768023 epoch total loss 1.19335592\n",
      "Trained batch 34 batch loss 1.20977283 epoch total loss 1.19383883\n",
      "Trained batch 35 batch loss 1.16264796 epoch total loss 1.19294763\n",
      "Trained batch 36 batch loss 1.22792423 epoch total loss 1.19391918\n",
      "Trained batch 37 batch loss 1.04565978 epoch total loss 1.18991208\n",
      "Trained batch 38 batch loss 0.930142522 epoch total loss 1.18307602\n",
      "Trained batch 39 batch loss 1.14604616 epoch total loss 1.18212652\n",
      "Trained batch 40 batch loss 1.22475469 epoch total loss 1.18319225\n",
      "Trained batch 41 batch loss 1.46183205 epoch total loss 1.18998826\n",
      "Trained batch 42 batch loss 1.37318504 epoch total loss 1.19435012\n",
      "Trained batch 43 batch loss 1.34340608 epoch total loss 1.19781649\n",
      "Trained batch 44 batch loss 1.42487466 epoch total loss 1.20297694\n",
      "Trained batch 45 batch loss 1.23554969 epoch total loss 1.20370078\n",
      "Trained batch 46 batch loss 1.38646972 epoch total loss 1.20767403\n",
      "Trained batch 47 batch loss 1.22674847 epoch total loss 1.20807993\n",
      "Trained batch 48 batch loss 1.11830389 epoch total loss 1.20620954\n",
      "Trained batch 49 batch loss 1.21710443 epoch total loss 1.20643198\n",
      "Trained batch 50 batch loss 1.26666188 epoch total loss 1.20763659\n",
      "Trained batch 51 batch loss 1.08613551 epoch total loss 1.2052542\n",
      "Trained batch 52 batch loss 1.04593778 epoch total loss 1.2021904\n",
      "Trained batch 53 batch loss 1.01531947 epoch total loss 1.19866455\n",
      "Trained batch 54 batch loss 1.15029168 epoch total loss 1.19776869\n",
      "Trained batch 55 batch loss 1.1372999 epoch total loss 1.19666934\n",
      "Trained batch 56 batch loss 1.01954436 epoch total loss 1.19350636\n",
      "Trained batch 57 batch loss 1.15375781 epoch total loss 1.19280899\n",
      "Trained batch 58 batch loss 1.25855553 epoch total loss 1.19394255\n",
      "Trained batch 59 batch loss 1.19550538 epoch total loss 1.19396889\n",
      "Trained batch 60 batch loss 1.09428 epoch total loss 1.19230735\n",
      "Trained batch 61 batch loss 1.11841536 epoch total loss 1.19109607\n",
      "Trained batch 62 batch loss 1.26796854 epoch total loss 1.19233596\n",
      "Trained batch 63 batch loss 1.27031589 epoch total loss 1.19357371\n",
      "Trained batch 64 batch loss 1.22934461 epoch total loss 1.19413269\n",
      "Trained batch 65 batch loss 1.06936014 epoch total loss 1.19221306\n",
      "Trained batch 66 batch loss 1.03386533 epoch total loss 1.18981385\n",
      "Trained batch 67 batch loss 1.08700478 epoch total loss 1.18827951\n",
      "Trained batch 68 batch loss 1.14936042 epoch total loss 1.18770707\n",
      "Trained batch 69 batch loss 1.1158607 epoch total loss 1.18666589\n",
      "Trained batch 70 batch loss 1.21811104 epoch total loss 1.18711507\n",
      "Trained batch 71 batch loss 1.06400275 epoch total loss 1.18538105\n",
      "Trained batch 72 batch loss 1.03396463 epoch total loss 1.18327808\n",
      "Trained batch 73 batch loss 1.22277057 epoch total loss 1.18381906\n",
      "Trained batch 74 batch loss 1.27496326 epoch total loss 1.18505073\n",
      "Trained batch 75 batch loss 1.41142535 epoch total loss 1.18806911\n",
      "Trained batch 76 batch loss 1.06831324 epoch total loss 1.18649328\n",
      "Trained batch 77 batch loss 1.06823957 epoch total loss 1.1849575\n",
      "Trained batch 78 batch loss 1.18811166 epoch total loss 1.18499792\n",
      "Trained batch 79 batch loss 1.14030254 epoch total loss 1.18443227\n",
      "Trained batch 80 batch loss 1.24964738 epoch total loss 1.18524742\n",
      "Trained batch 81 batch loss 1.24231386 epoch total loss 1.18595195\n",
      "Trained batch 82 batch loss 1.43718505 epoch total loss 1.18901587\n",
      "Trained batch 83 batch loss 1.43472552 epoch total loss 1.19197619\n",
      "Trained batch 84 batch loss 1.2590661 epoch total loss 1.19277477\n",
      "Trained batch 85 batch loss 1.15924716 epoch total loss 1.19238043\n",
      "Trained batch 86 batch loss 1.12334538 epoch total loss 1.19157767\n",
      "Trained batch 87 batch loss 1.25555253 epoch total loss 1.19231296\n",
      "Trained batch 88 batch loss 1.17562199 epoch total loss 1.19212329\n",
      "Trained batch 89 batch loss 1.2323575 epoch total loss 1.19257545\n",
      "Trained batch 90 batch loss 1.19763684 epoch total loss 1.19263172\n",
      "Trained batch 91 batch loss 1.2380079 epoch total loss 1.19313037\n",
      "Trained batch 92 batch loss 1.12441456 epoch total loss 1.19238341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 93 batch loss 1.18968809 epoch total loss 1.19235444\n",
      "Trained batch 94 batch loss 1.21773756 epoch total loss 1.19262445\n",
      "Trained batch 95 batch loss 1.22124684 epoch total loss 1.19292569\n",
      "Trained batch 96 batch loss 1.19430602 epoch total loss 1.19294012\n",
      "Trained batch 97 batch loss 1.29819417 epoch total loss 1.19402516\n",
      "Trained batch 98 batch loss 1.25624776 epoch total loss 1.19466007\n",
      "Trained batch 99 batch loss 1.18291664 epoch total loss 1.19454145\n",
      "Trained batch 100 batch loss 1.29545581 epoch total loss 1.19555056\n",
      "Trained batch 101 batch loss 1.14197993 epoch total loss 1.1950202\n",
      "Trained batch 102 batch loss 1.30609274 epoch total loss 1.19610918\n",
      "Trained batch 103 batch loss 1.28618097 epoch total loss 1.19698358\n",
      "Trained batch 104 batch loss 1.25880814 epoch total loss 1.19757819\n",
      "Trained batch 105 batch loss 1.28306603 epoch total loss 1.19839227\n",
      "Trained batch 106 batch loss 1.16252744 epoch total loss 1.19805396\n",
      "Trained batch 107 batch loss 1.15148103 epoch total loss 1.19761872\n",
      "Trained batch 108 batch loss 1.25364041 epoch total loss 1.19813752\n",
      "Trained batch 109 batch loss 1.24732065 epoch total loss 1.19858861\n",
      "Trained batch 110 batch loss 1.11066222 epoch total loss 1.19778931\n",
      "Trained batch 111 batch loss 1.20184469 epoch total loss 1.19782579\n",
      "Trained batch 112 batch loss 1.29959238 epoch total loss 1.1987344\n",
      "Trained batch 113 batch loss 1.13235879 epoch total loss 1.19814694\n",
      "Trained batch 114 batch loss 1.34239793 epoch total loss 1.19941235\n",
      "Trained batch 115 batch loss 1.22747266 epoch total loss 1.19965637\n",
      "Trained batch 116 batch loss 1.16231871 epoch total loss 1.1993345\n",
      "Trained batch 117 batch loss 1.21282816 epoch total loss 1.1994499\n",
      "Trained batch 118 batch loss 1.24859428 epoch total loss 1.19986629\n",
      "Trained batch 119 batch loss 1.17950475 epoch total loss 1.19969523\n",
      "Trained batch 120 batch loss 1.10597777 epoch total loss 1.19891417\n",
      "Trained batch 121 batch loss 1.11731625 epoch total loss 1.1982398\n",
      "Trained batch 122 batch loss 1.26653 epoch total loss 1.19879949\n",
      "Trained batch 123 batch loss 1.30175197 epoch total loss 1.19963658\n",
      "Trained batch 124 batch loss 1.23197174 epoch total loss 1.19989729\n",
      "Trained batch 125 batch loss 1.20208931 epoch total loss 1.19991481\n",
      "Trained batch 126 batch loss 1.04394734 epoch total loss 1.19867694\n",
      "Trained batch 127 batch loss 1.02336204 epoch total loss 1.1972965\n",
      "Trained batch 128 batch loss 1.05250549 epoch total loss 1.19616532\n",
      "Trained batch 129 batch loss 0.988401115 epoch total loss 1.19455481\n",
      "Trained batch 130 batch loss 1.13304174 epoch total loss 1.19408154\n",
      "Trained batch 131 batch loss 1.1553036 epoch total loss 1.19378555\n",
      "Trained batch 132 batch loss 1.30856884 epoch total loss 1.19465506\n",
      "Trained batch 133 batch loss 1.23308921 epoch total loss 1.19494414\n",
      "Trained batch 134 batch loss 1.22463942 epoch total loss 1.19516575\n",
      "Trained batch 135 batch loss 1.199543 epoch total loss 1.19519806\n",
      "Trained batch 136 batch loss 1.13572216 epoch total loss 1.1947608\n",
      "Trained batch 137 batch loss 1.09509706 epoch total loss 1.19403327\n",
      "Trained batch 138 batch loss 1.19194973 epoch total loss 1.19401824\n",
      "Trained batch 139 batch loss 1.26472473 epoch total loss 1.19452691\n",
      "Trained batch 140 batch loss 1.21964419 epoch total loss 1.19470644\n",
      "Trained batch 141 batch loss 1.21151316 epoch total loss 1.19482565\n",
      "Trained batch 142 batch loss 1.14008212 epoch total loss 1.19444\n",
      "Trained batch 143 batch loss 1.15294766 epoch total loss 1.19415\n",
      "Trained batch 144 batch loss 1.1235739 epoch total loss 1.1936599\n",
      "Trained batch 145 batch loss 1.24011159 epoch total loss 1.19398022\n",
      "Trained batch 146 batch loss 1.15122223 epoch total loss 1.19368732\n",
      "Trained batch 147 batch loss 1.19117737 epoch total loss 1.19367027\n",
      "Trained batch 148 batch loss 1.22672856 epoch total loss 1.19389367\n",
      "Trained batch 149 batch loss 1.17567956 epoch total loss 1.19377136\n",
      "Trained batch 150 batch loss 1.1464138 epoch total loss 1.19345558\n",
      "Trained batch 151 batch loss 1.2583046 epoch total loss 1.19388509\n",
      "Trained batch 152 batch loss 1.3431859 epoch total loss 1.19486725\n",
      "Trained batch 153 batch loss 1.39598215 epoch total loss 1.19618177\n",
      "Trained batch 154 batch loss 1.23979199 epoch total loss 1.1964649\n",
      "Trained batch 155 batch loss 1.18527162 epoch total loss 1.19639277\n",
      "Trained batch 156 batch loss 1.11167538 epoch total loss 1.19584966\n",
      "Trained batch 157 batch loss 1.18166184 epoch total loss 1.1957593\n",
      "Trained batch 158 batch loss 1.32688189 epoch total loss 1.19658923\n",
      "Trained batch 159 batch loss 1.3042388 epoch total loss 1.19726634\n",
      "Trained batch 160 batch loss 1.23358119 epoch total loss 1.19749331\n",
      "Trained batch 161 batch loss 1.32848537 epoch total loss 1.19830692\n",
      "Trained batch 162 batch loss 1.30405378 epoch total loss 1.19895959\n",
      "Trained batch 163 batch loss 1.26528168 epoch total loss 1.19936645\n",
      "Trained batch 164 batch loss 1.32079458 epoch total loss 1.20010698\n",
      "Trained batch 165 batch loss 1.21913528 epoch total loss 1.20022225\n",
      "Trained batch 166 batch loss 1.13204241 epoch total loss 1.19981158\n",
      "Trained batch 167 batch loss 1.21008921 epoch total loss 1.19987309\n",
      "Trained batch 168 batch loss 1.21588016 epoch total loss 1.19996834\n",
      "Trained batch 169 batch loss 1.22713625 epoch total loss 1.20012915\n",
      "Trained batch 170 batch loss 1.25798416 epoch total loss 1.20046937\n",
      "Trained batch 171 batch loss 1.28475034 epoch total loss 1.20096231\n",
      "Trained batch 172 batch loss 1.33219755 epoch total loss 1.20172524\n",
      "Trained batch 173 batch loss 1.25273776 epoch total loss 1.20202\n",
      "Trained batch 174 batch loss 1.27212059 epoch total loss 1.20242298\n",
      "Trained batch 175 batch loss 1.23307371 epoch total loss 1.20259821\n",
      "Trained batch 176 batch loss 1.33726847 epoch total loss 1.2033633\n",
      "Trained batch 177 batch loss 1.29282463 epoch total loss 1.20386875\n",
      "Trained batch 178 batch loss 1.32088506 epoch total loss 1.20452619\n",
      "Trained batch 179 batch loss 1.40541697 epoch total loss 1.20564854\n",
      "Trained batch 180 batch loss 1.21117806 epoch total loss 1.20567918\n",
      "Trained batch 181 batch loss 1.13424766 epoch total loss 1.2052846\n",
      "Trained batch 182 batch loss 1.24925268 epoch total loss 1.20552611\n",
      "Trained batch 183 batch loss 1.36969614 epoch total loss 1.20642328\n",
      "Trained batch 184 batch loss 1.39633286 epoch total loss 1.2074554\n",
      "Trained batch 185 batch loss 1.30956328 epoch total loss 1.20800734\n",
      "Trained batch 186 batch loss 1.27852225 epoch total loss 1.20838642\n",
      "Trained batch 187 batch loss 1.21276045 epoch total loss 1.20840979\n",
      "Trained batch 188 batch loss 1.22675717 epoch total loss 1.20850742\n",
      "Trained batch 189 batch loss 1.26579714 epoch total loss 1.20881045\n",
      "Trained batch 190 batch loss 1.41127539 epoch total loss 1.20987606\n",
      "Trained batch 191 batch loss 1.43200612 epoch total loss 1.21103907\n",
      "Trained batch 192 batch loss 1.31875324 epoch total loss 1.21160007\n",
      "Trained batch 193 batch loss 1.1948657 epoch total loss 1.2115134\n",
      "Trained batch 194 batch loss 1.09777248 epoch total loss 1.21092713\n",
      "Trained batch 195 batch loss 1.18421352 epoch total loss 1.21079016\n",
      "Trained batch 196 batch loss 1.2324903 epoch total loss 1.21090078\n",
      "Trained batch 197 batch loss 1.24164128 epoch total loss 1.21105683\n",
      "Trained batch 198 batch loss 1.09799027 epoch total loss 1.21048582\n",
      "Trained batch 199 batch loss 1.08900476 epoch total loss 1.20987535\n",
      "Trained batch 200 batch loss 1.05001569 epoch total loss 1.20907605\n",
      "Trained batch 201 batch loss 1.28062904 epoch total loss 1.20943201\n",
      "Trained batch 202 batch loss 1.5315578 epoch total loss 1.21102667\n",
      "Trained batch 203 batch loss 1.31136775 epoch total loss 1.21152103\n",
      "Trained batch 204 batch loss 1.31641793 epoch total loss 1.21203518\n",
      "Trained batch 205 batch loss 1.17378831 epoch total loss 1.21184862\n",
      "Trained batch 206 batch loss 1.21227014 epoch total loss 1.21185064\n",
      "Trained batch 207 batch loss 1.18376386 epoch total loss 1.21171498\n",
      "Trained batch 208 batch loss 1.20033431 epoch total loss 1.21166027\n",
      "Trained batch 209 batch loss 1.11513829 epoch total loss 1.21119845\n",
      "Trained batch 210 batch loss 1.25883031 epoch total loss 1.2114253\n",
      "Trained batch 211 batch loss 1.31239653 epoch total loss 1.21190381\n",
      "Trained batch 212 batch loss 1.22902703 epoch total loss 1.21198463\n",
      "Trained batch 213 batch loss 1.27350807 epoch total loss 1.21227336\n",
      "Trained batch 214 batch loss 1.23734784 epoch total loss 1.21239054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 215 batch loss 1.18342948 epoch total loss 1.21225584\n",
      "Trained batch 216 batch loss 1.08503306 epoch total loss 1.21166682\n",
      "Trained batch 217 batch loss 1.17241716 epoch total loss 1.21148598\n",
      "Trained batch 218 batch loss 1.29145813 epoch total loss 1.21185279\n",
      "Trained batch 219 batch loss 1.31988013 epoch total loss 1.21234608\n",
      "Trained batch 220 batch loss 1.20988929 epoch total loss 1.21233487\n",
      "Trained batch 221 batch loss 1.08988833 epoch total loss 1.21178079\n",
      "Trained batch 222 batch loss 1.18898082 epoch total loss 1.21167815\n",
      "Trained batch 223 batch loss 1.31339526 epoch total loss 1.21213424\n",
      "Trained batch 224 batch loss 1.19221544 epoch total loss 1.21204543\n",
      "Trained batch 225 batch loss 1.2614435 epoch total loss 1.2122649\n",
      "Trained batch 226 batch loss 1.3769418 epoch total loss 1.21299362\n",
      "Trained batch 227 batch loss 1.33585787 epoch total loss 1.21353483\n",
      "Trained batch 228 batch loss 1.38042665 epoch total loss 1.2142669\n",
      "Trained batch 229 batch loss 1.32072592 epoch total loss 1.21473181\n",
      "Trained batch 230 batch loss 1.29662752 epoch total loss 1.21508789\n",
      "Trained batch 231 batch loss 1.41621375 epoch total loss 1.21595848\n",
      "Trained batch 232 batch loss 1.21101 epoch total loss 1.21593714\n",
      "Trained batch 233 batch loss 1.19570971 epoch total loss 1.21585035\n",
      "Trained batch 234 batch loss 1.21709144 epoch total loss 1.2158556\n",
      "Trained batch 235 batch loss 1.17488492 epoch total loss 1.21568131\n",
      "Trained batch 236 batch loss 1.05948305 epoch total loss 1.21501946\n",
      "Trained batch 237 batch loss 1.21157336 epoch total loss 1.21500492\n",
      "Trained batch 238 batch loss 1.19972205 epoch total loss 1.21494067\n",
      "Trained batch 239 batch loss 1.2603085 epoch total loss 1.21513057\n",
      "Trained batch 240 batch loss 1.2759763 epoch total loss 1.21538401\n",
      "Trained batch 241 batch loss 1.31987929 epoch total loss 1.21581769\n",
      "Trained batch 242 batch loss 1.27620244 epoch total loss 1.21606719\n",
      "Trained batch 243 batch loss 1.29365158 epoch total loss 1.21638644\n",
      "Trained batch 244 batch loss 1.46798098 epoch total loss 1.2174176\n",
      "Trained batch 245 batch loss 1.48141801 epoch total loss 1.21849513\n",
      "Trained batch 246 batch loss 1.21450377 epoch total loss 1.21847892\n",
      "Trained batch 247 batch loss 1.15599751 epoch total loss 1.21822596\n",
      "Trained batch 248 batch loss 1.00025821 epoch total loss 1.21734703\n",
      "Trained batch 249 batch loss 1.05044556 epoch total loss 1.21667671\n",
      "Trained batch 250 batch loss 1.2095083 epoch total loss 1.2166481\n",
      "Trained batch 251 batch loss 1.10256743 epoch total loss 1.21619356\n",
      "Trained batch 252 batch loss 0.946616411 epoch total loss 1.21512389\n",
      "Trained batch 253 batch loss 0.92251128 epoch total loss 1.21396732\n",
      "Trained batch 254 batch loss 1.04394543 epoch total loss 1.21329796\n",
      "Trained batch 255 batch loss 1.08417356 epoch total loss 1.21279156\n",
      "Trained batch 256 batch loss 1.184677 epoch total loss 1.21268165\n",
      "Trained batch 257 batch loss 1.25481296 epoch total loss 1.21284556\n",
      "Trained batch 258 batch loss 1.2124908 epoch total loss 1.21284425\n",
      "Trained batch 259 batch loss 1.2935946 epoch total loss 1.2131561\n",
      "Trained batch 260 batch loss 1.37622964 epoch total loss 1.21378326\n",
      "Trained batch 261 batch loss 1.23460329 epoch total loss 1.21386302\n",
      "Trained batch 262 batch loss 1.26087224 epoch total loss 1.21404243\n",
      "Trained batch 263 batch loss 1.21248341 epoch total loss 1.21403646\n",
      "Trained batch 264 batch loss 1.26600575 epoch total loss 1.21423328\n",
      "Trained batch 265 batch loss 1.38483822 epoch total loss 1.21487701\n",
      "Trained batch 266 batch loss 1.18419826 epoch total loss 1.21476173\n",
      "Trained batch 267 batch loss 1.30799985 epoch total loss 1.21511102\n",
      "Trained batch 268 batch loss 1.14935637 epoch total loss 1.21486557\n",
      "Trained batch 269 batch loss 1.17087984 epoch total loss 1.21470201\n",
      "Trained batch 270 batch loss 1.12422299 epoch total loss 1.21436703\n",
      "Trained batch 271 batch loss 1.244982 epoch total loss 1.21448\n",
      "Trained batch 272 batch loss 1.1514709 epoch total loss 1.2142483\n",
      "Trained batch 273 batch loss 1.26204515 epoch total loss 1.21442342\n",
      "Trained batch 274 batch loss 1.44495392 epoch total loss 1.2152648\n",
      "Trained batch 275 batch loss 1.4110285 epoch total loss 1.21597672\n",
      "Trained batch 276 batch loss 1.46004176 epoch total loss 1.21686101\n",
      "Trained batch 277 batch loss 1.27439201 epoch total loss 1.21706867\n",
      "Trained batch 278 batch loss 1.24495268 epoch total loss 1.21716905\n",
      "Trained batch 279 batch loss 1.05932367 epoch total loss 1.21660328\n",
      "Trained batch 280 batch loss 0.983481 epoch total loss 1.21577072\n",
      "Trained batch 281 batch loss 0.981782138 epoch total loss 1.21493804\n",
      "Trained batch 282 batch loss 1.22911942 epoch total loss 1.21498835\n",
      "Trained batch 283 batch loss 1.42204273 epoch total loss 1.21571994\n",
      "Trained batch 284 batch loss 1.39968264 epoch total loss 1.21636772\n",
      "Trained batch 285 batch loss 1.19911993 epoch total loss 1.21630716\n",
      "Trained batch 286 batch loss 1.15054321 epoch total loss 1.21607721\n",
      "Trained batch 287 batch loss 1.20877111 epoch total loss 1.21605182\n",
      "Trained batch 288 batch loss 1.23114896 epoch total loss 1.21610415\n",
      "Trained batch 289 batch loss 1.25474596 epoch total loss 1.2162379\n",
      "Trained batch 290 batch loss 1.2192 epoch total loss 1.21624815\n",
      "Trained batch 291 batch loss 1.29081202 epoch total loss 1.21650434\n",
      "Trained batch 292 batch loss 1.37739134 epoch total loss 1.21705532\n",
      "Trained batch 293 batch loss 1.35347033 epoch total loss 1.21752095\n",
      "Trained batch 294 batch loss 1.39364874 epoch total loss 1.21812\n",
      "Trained batch 295 batch loss 1.3018434 epoch total loss 1.21840382\n",
      "Trained batch 296 batch loss 1.22549462 epoch total loss 1.21842778\n",
      "Trained batch 297 batch loss 1.18157148 epoch total loss 1.2183038\n",
      "Trained batch 298 batch loss 1.25924718 epoch total loss 1.21844113\n",
      "Trained batch 299 batch loss 1.18772006 epoch total loss 1.21833837\n",
      "Trained batch 300 batch loss 1.22776127 epoch total loss 1.21836972\n",
      "Trained batch 301 batch loss 1.21811306 epoch total loss 1.21836889\n",
      "Trained batch 302 batch loss 1.16617656 epoch total loss 1.21819603\n",
      "Trained batch 303 batch loss 1.2096343 epoch total loss 1.21816778\n",
      "Trained batch 304 batch loss 1.22610402 epoch total loss 1.21819389\n",
      "Trained batch 305 batch loss 1.22777927 epoch total loss 1.21822524\n",
      "Trained batch 306 batch loss 1.056247 epoch total loss 1.21769595\n",
      "Trained batch 307 batch loss 1.19466281 epoch total loss 1.21762097\n",
      "Trained batch 308 batch loss 1.3273797 epoch total loss 1.2179774\n",
      "Trained batch 309 batch loss 1.16666377 epoch total loss 1.21781123\n",
      "Trained batch 310 batch loss 1.13834393 epoch total loss 1.21755493\n",
      "Trained batch 311 batch loss 1.18325496 epoch total loss 1.21744466\n",
      "Trained batch 312 batch loss 1.16304731 epoch total loss 1.21727026\n",
      "Trained batch 313 batch loss 1.29178262 epoch total loss 1.21750832\n",
      "Trained batch 314 batch loss 1.22471571 epoch total loss 1.2175312\n",
      "Trained batch 315 batch loss 1.17175126 epoch total loss 1.21738589\n",
      "Trained batch 316 batch loss 1.07785892 epoch total loss 1.21694434\n",
      "Trained batch 317 batch loss 1.18719 epoch total loss 1.21685052\n",
      "Trained batch 318 batch loss 1.25643706 epoch total loss 1.21697497\n",
      "Trained batch 319 batch loss 1.20373058 epoch total loss 1.21693349\n",
      "Trained batch 320 batch loss 1.2063545 epoch total loss 1.21690047\n",
      "Trained batch 321 batch loss 1.21293366 epoch total loss 1.21688807\n",
      "Trained batch 322 batch loss 1.21125221 epoch total loss 1.21687055\n",
      "Trained batch 323 batch loss 1.07905543 epoch total loss 1.21644378\n",
      "Trained batch 324 batch loss 1.13269615 epoch total loss 1.21618533\n",
      "Trained batch 325 batch loss 1.06661987 epoch total loss 1.21572506\n",
      "Trained batch 326 batch loss 1.3463819 epoch total loss 1.21612585\n",
      "Trained batch 327 batch loss 1.1132561 epoch total loss 1.21581125\n",
      "Trained batch 328 batch loss 1.26696074 epoch total loss 1.21596718\n",
      "Trained batch 329 batch loss 1.26756406 epoch total loss 1.21612406\n",
      "Trained batch 330 batch loss 1.26638484 epoch total loss 1.21627641\n",
      "Trained batch 331 batch loss 1.31821728 epoch total loss 1.21658432\n",
      "Trained batch 332 batch loss 1.25684738 epoch total loss 1.21670556\n",
      "Trained batch 333 batch loss 1.16074288 epoch total loss 1.21653748\n",
      "Trained batch 334 batch loss 1.11512339 epoch total loss 1.21623385\n",
      "Trained batch 335 batch loss 1.14595509 epoch total loss 1.21602404\n",
      "Trained batch 336 batch loss 1.18733072 epoch total loss 1.21593869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 337 batch loss 1.33158064 epoch total loss 1.21628177\n",
      "Trained batch 338 batch loss 1.28432536 epoch total loss 1.21648312\n",
      "Trained batch 339 batch loss 1.28133464 epoch total loss 1.21667445\n",
      "Trained batch 340 batch loss 1.25664937 epoch total loss 1.21679199\n",
      "Trained batch 341 batch loss 1.25399423 epoch total loss 1.21690118\n",
      "Trained batch 342 batch loss 1.23910642 epoch total loss 1.21696603\n",
      "Trained batch 343 batch loss 1.04032648 epoch total loss 1.21645105\n",
      "Trained batch 344 batch loss 1.14952767 epoch total loss 1.2162565\n",
      "Trained batch 345 batch loss 1.32372022 epoch total loss 1.21656799\n",
      "Trained batch 346 batch loss 1.30308807 epoch total loss 1.21681809\n",
      "Trained batch 347 batch loss 1.15985858 epoch total loss 1.21665394\n",
      "Trained batch 348 batch loss 1.18920219 epoch total loss 1.21657515\n",
      "Trained batch 349 batch loss 1.05196595 epoch total loss 1.21610343\n",
      "Trained batch 350 batch loss 1.15791583 epoch total loss 1.21593726\n",
      "Trained batch 351 batch loss 1.13260365 epoch total loss 1.21569979\n",
      "Trained batch 352 batch loss 1.30945063 epoch total loss 1.21596611\n",
      "Trained batch 353 batch loss 1.29479909 epoch total loss 1.2161895\n",
      "Trained batch 354 batch loss 1.21545982 epoch total loss 1.21618736\n",
      "Trained batch 355 batch loss 1.16752398 epoch total loss 1.21605027\n",
      "Trained batch 356 batch loss 1.11032355 epoch total loss 1.21575332\n",
      "Trained batch 357 batch loss 1.16387165 epoch total loss 1.215608\n",
      "Trained batch 358 batch loss 1.13426387 epoch total loss 1.21538079\n",
      "Trained batch 359 batch loss 1.2231462 epoch total loss 1.21540236\n",
      "Trained batch 360 batch loss 1.29166341 epoch total loss 1.2156142\n",
      "Trained batch 361 batch loss 1.26480126 epoch total loss 1.21575046\n",
      "Trained batch 362 batch loss 1.23522282 epoch total loss 1.21580434\n",
      "Trained batch 363 batch loss 1.27651215 epoch total loss 1.21597159\n",
      "Trained batch 364 batch loss 1.18723667 epoch total loss 1.21589255\n",
      "Trained batch 365 batch loss 1.14176702 epoch total loss 1.21568942\n",
      "Trained batch 366 batch loss 1.31805444 epoch total loss 1.2159692\n",
      "Trained batch 367 batch loss 1.22477126 epoch total loss 1.21599317\n",
      "Trained batch 368 batch loss 1.32237887 epoch total loss 1.21628225\n",
      "Trained batch 369 batch loss 1.28707564 epoch total loss 1.21647406\n",
      "Trained batch 370 batch loss 1.27305675 epoch total loss 1.216627\n",
      "Trained batch 371 batch loss 1.07388592 epoch total loss 1.21624231\n",
      "Trained batch 372 batch loss 1.01620662 epoch total loss 1.21570456\n",
      "Trained batch 373 batch loss 1.01562107 epoch total loss 1.21516812\n",
      "Trained batch 374 batch loss 1.16837561 epoch total loss 1.21504307\n",
      "Trained batch 375 batch loss 1.18986189 epoch total loss 1.21497583\n",
      "Trained batch 376 batch loss 1.3120544 epoch total loss 1.21523404\n",
      "Trained batch 377 batch loss 1.33768964 epoch total loss 1.21555877\n",
      "Trained batch 378 batch loss 1.28418672 epoch total loss 1.21574032\n",
      "Trained batch 379 batch loss 1.22712493 epoch total loss 1.21577036\n",
      "Trained batch 380 batch loss 1.2293781 epoch total loss 1.21580613\n",
      "Trained batch 381 batch loss 1.36142457 epoch total loss 1.21618831\n",
      "Trained batch 382 batch loss 1.15902448 epoch total loss 1.2160387\n",
      "Trained batch 383 batch loss 1.29330766 epoch total loss 1.21624041\n",
      "Trained batch 384 batch loss 1.23700118 epoch total loss 1.21629441\n",
      "Trained batch 385 batch loss 1.38113558 epoch total loss 1.21672261\n",
      "Trained batch 386 batch loss 1.31216621 epoch total loss 1.21696985\n",
      "Trained batch 387 batch loss 1.25434232 epoch total loss 1.21706641\n",
      "Trained batch 388 batch loss 1.21365225 epoch total loss 1.21705759\n",
      "Trained batch 389 batch loss 1.20089293 epoch total loss 1.2170161\n",
      "Trained batch 390 batch loss 1.13629019 epoch total loss 1.21680903\n",
      "Trained batch 391 batch loss 1.251019 epoch total loss 1.21689653\n",
      "Trained batch 392 batch loss 1.31978607 epoch total loss 1.21715903\n",
      "Trained batch 393 batch loss 1.09603631 epoch total loss 1.21685088\n",
      "Trained batch 394 batch loss 1.0528419 epoch total loss 1.2164346\n",
      "Trained batch 395 batch loss 1.03752959 epoch total loss 1.21598172\n",
      "Trained batch 396 batch loss 1.13706064 epoch total loss 1.2157824\n",
      "Trained batch 397 batch loss 1.19840503 epoch total loss 1.21573865\n",
      "Trained batch 398 batch loss 1.18622184 epoch total loss 1.21566439\n",
      "Trained batch 399 batch loss 1.13591111 epoch total loss 1.21546459\n",
      "Trained batch 400 batch loss 1.28301382 epoch total loss 1.21563351\n",
      "Trained batch 401 batch loss 1.21385086 epoch total loss 1.21562898\n",
      "Trained batch 402 batch loss 1.35710692 epoch total loss 1.21598101\n",
      "Trained batch 403 batch loss 1.34795713 epoch total loss 1.21630847\n",
      "Trained batch 404 batch loss 1.13775206 epoch total loss 1.21611404\n",
      "Trained batch 405 batch loss 1.12032104 epoch total loss 1.21587753\n",
      "Trained batch 406 batch loss 1.18907213 epoch total loss 1.21581149\n",
      "Trained batch 407 batch loss 1.27838969 epoch total loss 1.21596527\n",
      "Trained batch 408 batch loss 1.34290028 epoch total loss 1.21627641\n",
      "Trained batch 409 batch loss 1.38322055 epoch total loss 1.21668446\n",
      "Trained batch 410 batch loss 1.2394582 epoch total loss 1.21674013\n",
      "Trained batch 411 batch loss 1.28634429 epoch total loss 1.21690941\n",
      "Trained batch 412 batch loss 1.18979406 epoch total loss 1.21684361\n",
      "Trained batch 413 batch loss 1.24771786 epoch total loss 1.21691835\n",
      "Trained batch 414 batch loss 1.23086321 epoch total loss 1.21695209\n",
      "Trained batch 415 batch loss 1.1997534 epoch total loss 1.2169106\n",
      "Trained batch 416 batch loss 1.30084336 epoch total loss 1.21711242\n",
      "Trained batch 417 batch loss 1.11246109 epoch total loss 1.21686137\n",
      "Trained batch 418 batch loss 1.20867503 epoch total loss 1.21684182\n",
      "Trained batch 419 batch loss 1.19673586 epoch total loss 1.21679389\n",
      "Trained batch 420 batch loss 1.1440742 epoch total loss 1.2166208\n",
      "Trained batch 421 batch loss 1.10223031 epoch total loss 1.21634901\n",
      "Trained batch 422 batch loss 1.03365147 epoch total loss 1.21591604\n",
      "Trained batch 423 batch loss 1.06880391 epoch total loss 1.21556818\n",
      "Trained batch 424 batch loss 1.16780472 epoch total loss 1.21545553\n",
      "Trained batch 425 batch loss 1.23403192 epoch total loss 1.21549916\n",
      "Trained batch 426 batch loss 1.07409024 epoch total loss 1.21516728\n",
      "Trained batch 427 batch loss 1.08139789 epoch total loss 1.214854\n",
      "Trained batch 428 batch loss 1.16670287 epoch total loss 1.21474147\n",
      "Trained batch 429 batch loss 1.21081424 epoch total loss 1.21473241\n",
      "Trained batch 430 batch loss 1.26104021 epoch total loss 1.21484\n",
      "Trained batch 431 batch loss 1.33105111 epoch total loss 1.21510971\n",
      "Trained batch 432 batch loss 1.36624765 epoch total loss 1.21545959\n",
      "Trained batch 433 batch loss 1.27115655 epoch total loss 1.21558833\n",
      "Trained batch 434 batch loss 1.11511517 epoch total loss 1.21535683\n",
      "Trained batch 435 batch loss 1.08973122 epoch total loss 1.21506798\n",
      "Trained batch 436 batch loss 1.09975028 epoch total loss 1.21480346\n",
      "Trained batch 437 batch loss 1.14490986 epoch total loss 1.21464348\n",
      "Trained batch 438 batch loss 1.26475596 epoch total loss 1.21475792\n",
      "Trained batch 439 batch loss 1.2249105 epoch total loss 1.21478105\n",
      "Trained batch 440 batch loss 1.25596642 epoch total loss 1.21487463\n",
      "Trained batch 441 batch loss 1.37247336 epoch total loss 1.21523213\n",
      "Trained batch 442 batch loss 1.26423824 epoch total loss 1.215343\n",
      "Trained batch 443 batch loss 1.17733 epoch total loss 1.21525705\n",
      "Trained batch 444 batch loss 1.39405143 epoch total loss 1.21565974\n",
      "Trained batch 445 batch loss 1.21837354 epoch total loss 1.21566582\n",
      "Trained batch 446 batch loss 1.14570498 epoch total loss 1.21550894\n",
      "Trained batch 447 batch loss 1.20491683 epoch total loss 1.21548522\n",
      "Trained batch 448 batch loss 1.35388231 epoch total loss 1.21579421\n",
      "Trained batch 449 batch loss 1.31612206 epoch total loss 1.2160176\n",
      "Trained batch 450 batch loss 1.17619693 epoch total loss 1.21592903\n",
      "Trained batch 451 batch loss 1.34588397 epoch total loss 1.21621728\n",
      "Trained batch 452 batch loss 1.35977173 epoch total loss 1.21653485\n",
      "Trained batch 453 batch loss 1.39331281 epoch total loss 1.21692514\n",
      "Trained batch 454 batch loss 1.1772275 epoch total loss 1.21683776\n",
      "Trained batch 455 batch loss 1.24211013 epoch total loss 1.21689332\n",
      "Trained batch 456 batch loss 1.25350034 epoch total loss 1.21697354\n",
      "Trained batch 457 batch loss 1.20253587 epoch total loss 1.21694195\n",
      "Trained batch 458 batch loss 1.21649182 epoch total loss 1.21694088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 459 batch loss 1.18718064 epoch total loss 1.21687615\n",
      "Trained batch 460 batch loss 1.1818428 epoch total loss 1.2168\n",
      "Trained batch 461 batch loss 1.11809516 epoch total loss 1.21658587\n",
      "Trained batch 462 batch loss 1.09998631 epoch total loss 1.21633339\n",
      "Trained batch 463 batch loss 1.0612694 epoch total loss 1.21599853\n",
      "Trained batch 464 batch loss 1.09369147 epoch total loss 1.21573496\n",
      "Trained batch 465 batch loss 1.15034747 epoch total loss 1.21559429\n",
      "Trained batch 466 batch loss 1.05842495 epoch total loss 1.21525693\n",
      "Trained batch 467 batch loss 1.16057277 epoch total loss 1.21513987\n",
      "Trained batch 468 batch loss 1.11432254 epoch total loss 1.21492445\n",
      "Trained batch 469 batch loss 1.12453532 epoch total loss 1.21473169\n",
      "Trained batch 470 batch loss 1.32312655 epoch total loss 1.21496236\n",
      "Trained batch 471 batch loss 1.14888489 epoch total loss 1.21482193\n",
      "Trained batch 472 batch loss 1.26307201 epoch total loss 1.21492422\n",
      "Trained batch 473 batch loss 1.17518497 epoch total loss 1.21484017\n",
      "Trained batch 474 batch loss 1.28479135 epoch total loss 1.21498775\n",
      "Trained batch 475 batch loss 1.24981427 epoch total loss 1.21506107\n",
      "Trained batch 476 batch loss 1.27266669 epoch total loss 1.21518207\n",
      "Trained batch 477 batch loss 1.06864715 epoch total loss 1.21487486\n",
      "Trained batch 478 batch loss 1.05913901 epoch total loss 1.21454906\n",
      "Trained batch 479 batch loss 1.0903405 epoch total loss 1.21428967\n",
      "Trained batch 480 batch loss 1.19343805 epoch total loss 1.21424627\n",
      "Trained batch 481 batch loss 1.38496923 epoch total loss 1.21460116\n",
      "Trained batch 482 batch loss 1.36870241 epoch total loss 1.21492088\n",
      "Trained batch 483 batch loss 1.24671102 epoch total loss 1.21498668\n",
      "Trained batch 484 batch loss 1.35359287 epoch total loss 1.21527302\n",
      "Trained batch 485 batch loss 1.31257343 epoch total loss 1.21547365\n",
      "Trained batch 486 batch loss 1.24641454 epoch total loss 1.21553719\n",
      "Trained batch 487 batch loss 1.30280817 epoch total loss 1.21571636\n",
      "Trained batch 488 batch loss 1.31284642 epoch total loss 1.21591544\n",
      "Trained batch 489 batch loss 1.20012772 epoch total loss 1.21588326\n",
      "Trained batch 490 batch loss 1.20411515 epoch total loss 1.21585917\n",
      "Trained batch 491 batch loss 1.16312814 epoch total loss 1.21575177\n",
      "Trained batch 492 batch loss 1.04274857 epoch total loss 1.2154001\n",
      "Trained batch 493 batch loss 1.04420733 epoch total loss 1.21505284\n",
      "Trained batch 494 batch loss 1.22457159 epoch total loss 1.21507204\n",
      "Trained batch 495 batch loss 1.19743 epoch total loss 1.21503651\n",
      "Trained batch 496 batch loss 1.26864374 epoch total loss 1.21514452\n",
      "Trained batch 497 batch loss 1.29463351 epoch total loss 1.21530437\n",
      "Trained batch 498 batch loss 1.4075048 epoch total loss 1.21569037\n",
      "Trained batch 499 batch loss 1.27242124 epoch total loss 1.2158041\n",
      "Trained batch 500 batch loss 1.3047893 epoch total loss 1.21598208\n",
      "Trained batch 501 batch loss 1.39729118 epoch total loss 1.21634388\n",
      "Trained batch 502 batch loss 1.22559988 epoch total loss 1.21636236\n",
      "Trained batch 503 batch loss 1.27240515 epoch total loss 1.2164737\n",
      "Trained batch 504 batch loss 1.31858873 epoch total loss 1.21667635\n",
      "Trained batch 505 batch loss 1.27552104 epoch total loss 1.21679294\n",
      "Trained batch 506 batch loss 1.14255881 epoch total loss 1.21664619\n",
      "Trained batch 507 batch loss 1.14547431 epoch total loss 1.21650577\n",
      "Trained batch 508 batch loss 1.03579819 epoch total loss 1.21615016\n",
      "Trained batch 509 batch loss 1.09133267 epoch total loss 1.21590483\n",
      "Trained batch 510 batch loss 1.11375916 epoch total loss 1.21570456\n",
      "Trained batch 511 batch loss 1.12201226 epoch total loss 1.21552122\n",
      "Trained batch 512 batch loss 0.990982652 epoch total loss 1.21508265\n",
      "Trained batch 513 batch loss 0.88355577 epoch total loss 1.21443641\n",
      "Trained batch 514 batch loss 0.966540158 epoch total loss 1.21395409\n",
      "Trained batch 515 batch loss 0.97506541 epoch total loss 1.21349025\n",
      "Trained batch 516 batch loss 1.16894674 epoch total loss 1.21340382\n",
      "Trained batch 517 batch loss 1.06946707 epoch total loss 1.21312547\n",
      "Trained batch 518 batch loss 1.17175317 epoch total loss 1.2130456\n",
      "Trained batch 519 batch loss 1.22098017 epoch total loss 1.21306086\n",
      "Trained batch 520 batch loss 1.09052503 epoch total loss 1.2128253\n",
      "Trained batch 521 batch loss 1.20626402 epoch total loss 1.21281266\n",
      "Trained batch 522 batch loss 1.21218538 epoch total loss 1.21281135\n",
      "Trained batch 523 batch loss 1.05479169 epoch total loss 1.21250927\n",
      "Trained batch 524 batch loss 1.08650959 epoch total loss 1.21226871\n",
      "Trained batch 525 batch loss 1.18052268 epoch total loss 1.21220827\n",
      "Trained batch 526 batch loss 1.03829694 epoch total loss 1.21187758\n",
      "Trained batch 527 batch loss 1.09056759 epoch total loss 1.21164751\n",
      "Trained batch 528 batch loss 1.08194506 epoch total loss 1.21140182\n",
      "Trained batch 529 batch loss 1.15558565 epoch total loss 1.21129632\n",
      "Trained batch 530 batch loss 1.11607182 epoch total loss 1.21111667\n",
      "Trained batch 531 batch loss 1.10518062 epoch total loss 1.21091712\n",
      "Trained batch 532 batch loss 1.07456934 epoch total loss 1.21066093\n",
      "Trained batch 533 batch loss 1.20104706 epoch total loss 1.21064281\n",
      "Trained batch 534 batch loss 1.26346648 epoch total loss 1.21074188\n",
      "Trained batch 535 batch loss 1.57071376 epoch total loss 1.21141469\n",
      "Trained batch 536 batch loss 1.27386224 epoch total loss 1.21153128\n",
      "Trained batch 537 batch loss 1.264606 epoch total loss 1.21163\n",
      "Trained batch 538 batch loss 1.38240683 epoch total loss 1.21194744\n",
      "Trained batch 539 batch loss 1.30919576 epoch total loss 1.2121278\n",
      "Trained batch 540 batch loss 1.30522954 epoch total loss 1.2123003\n",
      "Trained batch 541 batch loss 1.09323943 epoch total loss 1.21208024\n",
      "Trained batch 542 batch loss 1.11277866 epoch total loss 1.21189702\n",
      "Trained batch 543 batch loss 1.25569487 epoch total loss 1.21197772\n",
      "Trained batch 544 batch loss 1.24842513 epoch total loss 1.21204472\n",
      "Trained batch 545 batch loss 1.20604861 epoch total loss 1.21203363\n",
      "Trained batch 546 batch loss 1.30353522 epoch total loss 1.21220124\n",
      "Trained batch 547 batch loss 1.42161262 epoch total loss 1.21258414\n",
      "Trained batch 548 batch loss 1.27735543 epoch total loss 1.21270227\n",
      "Trained batch 549 batch loss 1.35154581 epoch total loss 1.21295524\n",
      "Trained batch 550 batch loss 1.23097789 epoch total loss 1.2129879\n",
      "Trained batch 551 batch loss 1.18638349 epoch total loss 1.21293974\n",
      "Trained batch 552 batch loss 1.22581172 epoch total loss 1.2129631\n",
      "Trained batch 553 batch loss 1.34636664 epoch total loss 1.21320426\n",
      "Trained batch 554 batch loss 1.27937603 epoch total loss 1.21332371\n",
      "Trained batch 555 batch loss 1.42248857 epoch total loss 1.21370053\n",
      "Trained batch 556 batch loss 1.51340425 epoch total loss 1.2142396\n",
      "Trained batch 557 batch loss 1.28555202 epoch total loss 1.21436763\n",
      "Trained batch 558 batch loss 1.20456219 epoch total loss 1.2143501\n",
      "Trained batch 559 batch loss 1.30681849 epoch total loss 1.21451557\n",
      "Trained batch 560 batch loss 1.36974478 epoch total loss 1.21479273\n",
      "Trained batch 561 batch loss 1.37447023 epoch total loss 1.21507728\n",
      "Trained batch 562 batch loss 1.2908802 epoch total loss 1.21521223\n",
      "Trained batch 563 batch loss 1.121261 epoch total loss 1.21504545\n",
      "Trained batch 564 batch loss 1.10659683 epoch total loss 1.21485305\n",
      "Trained batch 565 batch loss 0.97521019 epoch total loss 1.2144289\n",
      "Trained batch 566 batch loss 1.34765697 epoch total loss 1.21466434\n",
      "Trained batch 567 batch loss 1.27527559 epoch total loss 1.21477115\n",
      "Trained batch 568 batch loss 1.222996 epoch total loss 1.2147857\n",
      "Trained batch 569 batch loss 1.2928443 epoch total loss 1.2149229\n",
      "Trained batch 570 batch loss 1.27432179 epoch total loss 1.21502709\n",
      "Trained batch 571 batch loss 1.21665525 epoch total loss 1.21503\n",
      "Trained batch 572 batch loss 1.17849457 epoch total loss 1.21496606\n",
      "Trained batch 573 batch loss 1.11904252 epoch total loss 1.21479857\n",
      "Trained batch 574 batch loss 1.24633145 epoch total loss 1.21485353\n",
      "Trained batch 575 batch loss 1.17988014 epoch total loss 1.21479273\n",
      "Trained batch 576 batch loss 1.24033606 epoch total loss 1.21483707\n",
      "Trained batch 577 batch loss 1.31039405 epoch total loss 1.21500266\n",
      "Trained batch 578 batch loss 1.27938318 epoch total loss 1.215114\n",
      "Trained batch 579 batch loss 1.27516711 epoch total loss 1.21521771\n",
      "Trained batch 580 batch loss 1.2990979 epoch total loss 1.21536219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 581 batch loss 1.34401846 epoch total loss 1.21558368\n",
      "Trained batch 582 batch loss 1.45752561 epoch total loss 1.21599936\n",
      "Trained batch 583 batch loss 1.30369079 epoch total loss 1.21614981\n",
      "Trained batch 584 batch loss 1.38177633 epoch total loss 1.21643341\n",
      "Trained batch 585 batch loss 1.3848865 epoch total loss 1.2167213\n",
      "Trained batch 586 batch loss 1.2575295 epoch total loss 1.21679091\n",
      "Trained batch 587 batch loss 1.30159974 epoch total loss 1.2169354\n",
      "Trained batch 588 batch loss 1.25063801 epoch total loss 1.21699262\n",
      "Trained batch 589 batch loss 1.25146723 epoch total loss 1.21705115\n",
      "Trained batch 590 batch loss 1.3116709 epoch total loss 1.21721148\n",
      "Trained batch 591 batch loss 1.2851764 epoch total loss 1.21732652\n",
      "Trained batch 592 batch loss 1.29337525 epoch total loss 1.21745491\n",
      "Trained batch 593 batch loss 1.3029294 epoch total loss 1.21759903\n",
      "Trained batch 594 batch loss 1.30997682 epoch total loss 1.2177546\n",
      "Trained batch 595 batch loss 1.19517672 epoch total loss 1.21771669\n",
      "Trained batch 596 batch loss 1.23978853 epoch total loss 1.21775377\n",
      "Trained batch 597 batch loss 1.25640035 epoch total loss 1.2178185\n",
      "Trained batch 598 batch loss 1.33443415 epoch total loss 1.21801353\n",
      "Trained batch 599 batch loss 1.22482073 epoch total loss 1.21802485\n",
      "Trained batch 600 batch loss 1.20450473 epoch total loss 1.21800232\n",
      "Trained batch 601 batch loss 1.06492865 epoch total loss 1.21774769\n",
      "Trained batch 602 batch loss 0.998466253 epoch total loss 1.21738338\n",
      "Trained batch 603 batch loss 1.14609993 epoch total loss 1.21726525\n",
      "Trained batch 604 batch loss 1.1614989 epoch total loss 1.21717286\n",
      "Trained batch 605 batch loss 1.16941929 epoch total loss 1.21709394\n",
      "Trained batch 606 batch loss 1.04172802 epoch total loss 1.21680462\n",
      "Trained batch 607 batch loss 1.19379222 epoch total loss 1.21676672\n",
      "Trained batch 608 batch loss 1.37122345 epoch total loss 1.21702075\n",
      "Trained batch 609 batch loss 1.17459345 epoch total loss 1.21695113\n",
      "Trained batch 610 batch loss 1.25894129 epoch total loss 1.21701992\n",
      "Trained batch 611 batch loss 1.16589177 epoch total loss 1.21693623\n",
      "Trained batch 612 batch loss 1.13983548 epoch total loss 1.21681023\n",
      "Trained batch 613 batch loss 1.18373632 epoch total loss 1.21675622\n",
      "Trained batch 614 batch loss 1.20301247 epoch total loss 1.21673381\n",
      "Trained batch 615 batch loss 1.2338326 epoch total loss 1.21676159\n",
      "Trained batch 616 batch loss 1.23555207 epoch total loss 1.21679211\n",
      "Trained batch 617 batch loss 1.25519812 epoch total loss 1.21685433\n",
      "Trained batch 618 batch loss 1.20798063 epoch total loss 1.21684\n",
      "Trained batch 619 batch loss 1.18640077 epoch total loss 1.21679091\n",
      "Trained batch 620 batch loss 1.08398592 epoch total loss 1.2165767\n",
      "Trained batch 621 batch loss 1.11914897 epoch total loss 1.2164197\n",
      "Trained batch 622 batch loss 1.2212193 epoch total loss 1.21642745\n",
      "Trained batch 623 batch loss 1.10864949 epoch total loss 1.21625447\n",
      "Trained batch 624 batch loss 1.27876496 epoch total loss 1.21635461\n",
      "Trained batch 625 batch loss 1.1916492 epoch total loss 1.21631503\n",
      "Trained batch 626 batch loss 1.16852474 epoch total loss 1.21623874\n",
      "Trained batch 627 batch loss 1.23034644 epoch total loss 1.21626115\n",
      "Trained batch 628 batch loss 1.14137018 epoch total loss 1.21614194\n",
      "Trained batch 629 batch loss 1.09486687 epoch total loss 1.21594906\n",
      "Trained batch 630 batch loss 1.29428339 epoch total loss 1.21607351\n",
      "Trained batch 631 batch loss 1.24678683 epoch total loss 1.21612215\n",
      "Trained batch 632 batch loss 1.13010752 epoch total loss 1.21598601\n",
      "Trained batch 633 batch loss 1.06002653 epoch total loss 1.21573961\n",
      "Trained batch 634 batch loss 1.16113901 epoch total loss 1.21565342\n",
      "Trained batch 635 batch loss 1.15534639 epoch total loss 1.21555853\n",
      "Trained batch 636 batch loss 1.11173785 epoch total loss 1.21539533\n",
      "Trained batch 637 batch loss 1.19316328 epoch total loss 1.2153604\n",
      "Trained batch 638 batch loss 1.3166039 epoch total loss 1.21551907\n",
      "Trained batch 639 batch loss 1.20418763 epoch total loss 1.21550131\n",
      "Trained batch 640 batch loss 1.29220188 epoch total loss 1.21562111\n",
      "Trained batch 641 batch loss 1.27329886 epoch total loss 1.21571112\n",
      "Trained batch 642 batch loss 1.39056551 epoch total loss 1.21598351\n",
      "Trained batch 643 batch loss 1.40363932 epoch total loss 1.21627533\n",
      "Trained batch 644 batch loss 1.52436841 epoch total loss 1.2167536\n",
      "Trained batch 645 batch loss 1.30826712 epoch total loss 1.21689558\n",
      "Trained batch 646 batch loss 1.19274282 epoch total loss 1.21685815\n",
      "Trained batch 647 batch loss 1.18593526 epoch total loss 1.21681035\n",
      "Trained batch 648 batch loss 1.29053521 epoch total loss 1.21692407\n",
      "Trained batch 649 batch loss 1.43092203 epoch total loss 1.2172538\n",
      "Trained batch 650 batch loss 1.36876702 epoch total loss 1.21748698\n",
      "Trained batch 651 batch loss 1.332026 epoch total loss 1.21766293\n",
      "Trained batch 652 batch loss 1.21374226 epoch total loss 1.21765685\n",
      "Trained batch 653 batch loss 1.36092353 epoch total loss 1.2178762\n",
      "Trained batch 654 batch loss 1.22704828 epoch total loss 1.21789026\n",
      "Trained batch 655 batch loss 1.35302711 epoch total loss 1.21809661\n",
      "Trained batch 656 batch loss 1.43399537 epoch total loss 1.21842575\n",
      "Trained batch 657 batch loss 1.37091208 epoch total loss 1.21865785\n",
      "Trained batch 658 batch loss 1.29807067 epoch total loss 1.21877861\n",
      "Trained batch 659 batch loss 1.24262309 epoch total loss 1.21881473\n",
      "Trained batch 660 batch loss 1.22287703 epoch total loss 1.21882093\n",
      "Trained batch 661 batch loss 1.20168293 epoch total loss 1.21879494\n",
      "Trained batch 662 batch loss 1.33475554 epoch total loss 1.21897018\n",
      "Trained batch 663 batch loss 1.3877883 epoch total loss 1.21922481\n",
      "Trained batch 664 batch loss 1.27446032 epoch total loss 1.21930802\n",
      "Trained batch 665 batch loss 1.41366506 epoch total loss 1.21960032\n",
      "Trained batch 666 batch loss 1.38544178 epoch total loss 1.21984923\n",
      "Trained batch 667 batch loss 1.52020574 epoch total loss 1.2202996\n",
      "Trained batch 668 batch loss 1.38703191 epoch total loss 1.22054923\n",
      "Trained batch 669 batch loss 1.38124156 epoch total loss 1.22078931\n",
      "Trained batch 670 batch loss 1.32922626 epoch total loss 1.2209512\n",
      "Trained batch 671 batch loss 1.35650623 epoch total loss 1.22115326\n",
      "Trained batch 672 batch loss 1.31049407 epoch total loss 1.22128618\n",
      "Trained batch 673 batch loss 1.27149749 epoch total loss 1.22136068\n",
      "Trained batch 674 batch loss 1.28915787 epoch total loss 1.2214613\n",
      "Trained batch 675 batch loss 1.25258517 epoch total loss 1.22150743\n",
      "Trained batch 676 batch loss 1.17718482 epoch total loss 1.22144186\n",
      "Trained batch 677 batch loss 1.08721185 epoch total loss 1.22124362\n",
      "Trained batch 678 batch loss 0.984389603 epoch total loss 1.22089422\n",
      "Trained batch 679 batch loss 1.23514152 epoch total loss 1.22091532\n",
      "Trained batch 680 batch loss 1.25990343 epoch total loss 1.22097254\n",
      "Trained batch 681 batch loss 1.36318362 epoch total loss 1.22118139\n",
      "Trained batch 682 batch loss 1.45799947 epoch total loss 1.22152865\n",
      "Trained batch 683 batch loss 1.42007 epoch total loss 1.22181928\n",
      "Trained batch 684 batch loss 1.3808918 epoch total loss 1.22205186\n",
      "Trained batch 685 batch loss 1.24935198 epoch total loss 1.22209167\n",
      "Trained batch 686 batch loss 1.14811492 epoch total loss 1.22198391\n",
      "Trained batch 687 batch loss 1.1627512 epoch total loss 1.22189772\n",
      "Trained batch 688 batch loss 1.21176279 epoch total loss 1.22188306\n",
      "Trained batch 689 batch loss 1.1187278 epoch total loss 1.22173333\n",
      "Trained batch 690 batch loss 1.2324872 epoch total loss 1.22174883\n",
      "Trained batch 691 batch loss 1.29917693 epoch total loss 1.22186089\n",
      "Trained batch 692 batch loss 1.24380767 epoch total loss 1.22189271\n",
      "Trained batch 693 batch loss 1.23501062 epoch total loss 1.22191155\n",
      "Trained batch 694 batch loss 1.22035289 epoch total loss 1.22190928\n",
      "Trained batch 695 batch loss 1.29127741 epoch total loss 1.22200906\n",
      "Trained batch 696 batch loss 1.24212813 epoch total loss 1.22203803\n",
      "Trained batch 697 batch loss 1.22927845 epoch total loss 1.2220484\n",
      "Trained batch 698 batch loss 1.09793496 epoch total loss 1.22187054\n",
      "Trained batch 699 batch loss 1.07524228 epoch total loss 1.22166085\n",
      "Trained batch 700 batch loss 1.15356553 epoch total loss 1.22156358\n",
      "Trained batch 701 batch loss 1.10629272 epoch total loss 1.22139919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 702 batch loss 1.07819736 epoch total loss 1.2211951\n",
      "Trained batch 703 batch loss 1.149158 epoch total loss 1.2210927\n",
      "Trained batch 704 batch loss 1.21412385 epoch total loss 1.22108281\n",
      "Trained batch 705 batch loss 1.06287205 epoch total loss 1.22085834\n",
      "Trained batch 706 batch loss 1.15465212 epoch total loss 1.22076464\n",
      "Trained batch 707 batch loss 1.22836161 epoch total loss 1.22077525\n",
      "Trained batch 708 batch loss 1.22725964 epoch total loss 1.22078443\n",
      "Trained batch 709 batch loss 1.1808598 epoch total loss 1.22072804\n",
      "Trained batch 710 batch loss 1.36827302 epoch total loss 1.22093594\n",
      "Trained batch 711 batch loss 1.24002552 epoch total loss 1.22096276\n",
      "Trained batch 712 batch loss 1.27993858 epoch total loss 1.22104573\n",
      "Trained batch 713 batch loss 1.22280908 epoch total loss 1.22104824\n",
      "Trained batch 714 batch loss 1.22505116 epoch total loss 1.22105372\n",
      "Trained batch 715 batch loss 1.19897437 epoch total loss 1.22102284\n",
      "Trained batch 716 batch loss 1.21630263 epoch total loss 1.22101629\n",
      "Trained batch 717 batch loss 1.37711406 epoch total loss 1.22123408\n",
      "Trained batch 718 batch loss 1.32360077 epoch total loss 1.22137666\n",
      "Trained batch 719 batch loss 1.33835042 epoch total loss 1.22153938\n",
      "Trained batch 720 batch loss 1.21950603 epoch total loss 1.22153652\n",
      "Trained batch 721 batch loss 1.23857534 epoch total loss 1.22156012\n",
      "Trained batch 722 batch loss 1.20967746 epoch total loss 1.22154367\n",
      "Trained batch 723 batch loss 1.2160579 epoch total loss 1.22153604\n",
      "Trained batch 724 batch loss 1.17724681 epoch total loss 1.22147489\n",
      "Trained batch 725 batch loss 1.35967481 epoch total loss 1.2216655\n",
      "Trained batch 726 batch loss 1.19705069 epoch total loss 1.22163165\n",
      "Trained batch 727 batch loss 1.18321288 epoch total loss 1.22157872\n",
      "Trained batch 728 batch loss 1.3287288 epoch total loss 1.22172594\n",
      "Trained batch 729 batch loss 1.14832485 epoch total loss 1.22162521\n",
      "Trained batch 730 batch loss 1.15285349 epoch total loss 1.22153103\n",
      "Trained batch 731 batch loss 1.15236199 epoch total loss 1.22143638\n",
      "Trained batch 732 batch loss 1.15385652 epoch total loss 1.22134411\n",
      "Trained batch 733 batch loss 1.16564488 epoch total loss 1.22126806\n",
      "Trained batch 734 batch loss 1.11516 epoch total loss 1.22112358\n",
      "Trained batch 735 batch loss 1.15458405 epoch total loss 1.2210331\n",
      "Trained batch 736 batch loss 1.17570496 epoch total loss 1.22097147\n",
      "Trained batch 737 batch loss 1.26781917 epoch total loss 1.221035\n",
      "Trained batch 738 batch loss 1.19691467 epoch total loss 1.22100234\n",
      "Trained batch 739 batch loss 1.0995636 epoch total loss 1.22083795\n",
      "Trained batch 740 batch loss 1.22202778 epoch total loss 1.22083962\n",
      "Trained batch 741 batch loss 1.27280343 epoch total loss 1.22090983\n",
      "Trained batch 742 batch loss 1.23928213 epoch total loss 1.22093451\n",
      "Trained batch 743 batch loss 1.11978579 epoch total loss 1.22079837\n",
      "Trained batch 744 batch loss 1.18602967 epoch total loss 1.22075164\n",
      "Trained batch 745 batch loss 1.21700501 epoch total loss 1.22074664\n",
      "Trained batch 746 batch loss 1.21018457 epoch total loss 1.22073245\n",
      "Trained batch 747 batch loss 1.29700422 epoch total loss 1.22083461\n",
      "Trained batch 748 batch loss 1.28004384 epoch total loss 1.22091377\n",
      "Trained batch 749 batch loss 1.16599894 epoch total loss 1.22084045\n",
      "Trained batch 750 batch loss 1.17793345 epoch total loss 1.22078323\n",
      "Trained batch 751 batch loss 1.25723374 epoch total loss 1.22083175\n",
      "Trained batch 752 batch loss 1.19877946 epoch total loss 1.22080243\n",
      "Trained batch 753 batch loss 1.30801499 epoch total loss 1.2209183\n",
      "Trained batch 754 batch loss 1.31087112 epoch total loss 1.22103763\n",
      "Trained batch 755 batch loss 1.37214804 epoch total loss 1.22123778\n",
      "Trained batch 756 batch loss 1.37587583 epoch total loss 1.22144222\n",
      "Trained batch 757 batch loss 1.31863523 epoch total loss 1.22157073\n",
      "Trained batch 758 batch loss 1.19458961 epoch total loss 1.22153509\n",
      "Trained batch 759 batch loss 1.12190652 epoch total loss 1.22140384\n",
      "Trained batch 760 batch loss 1.11085784 epoch total loss 1.22125828\n",
      "Trained batch 761 batch loss 1.16363645 epoch total loss 1.22118258\n",
      "Trained batch 762 batch loss 1.08601785 epoch total loss 1.2210052\n",
      "Trained batch 763 batch loss 1.22916043 epoch total loss 1.22101593\n",
      "Trained batch 764 batch loss 1.29412127 epoch total loss 1.22111166\n",
      "Trained batch 765 batch loss 1.35640967 epoch total loss 1.22128844\n",
      "Trained batch 766 batch loss 1.27080321 epoch total loss 1.22135305\n",
      "Trained batch 767 batch loss 1.22450924 epoch total loss 1.22135711\n",
      "Trained batch 768 batch loss 1.23963928 epoch total loss 1.22138095\n",
      "Trained batch 769 batch loss 1.31415641 epoch total loss 1.22150159\n",
      "Trained batch 770 batch loss 1.23852825 epoch total loss 1.22152364\n",
      "Trained batch 771 batch loss 1.17591417 epoch total loss 1.22146451\n",
      "Trained batch 772 batch loss 1.13811338 epoch total loss 1.22135651\n",
      "Trained batch 773 batch loss 1.16068673 epoch total loss 1.22127807\n",
      "Trained batch 774 batch loss 1.1313467 epoch total loss 1.22116196\n",
      "Trained batch 775 batch loss 1.31331444 epoch total loss 1.22128081\n",
      "Trained batch 776 batch loss 1.13485956 epoch total loss 1.22116947\n",
      "Trained batch 777 batch loss 1.17829978 epoch total loss 1.22111428\n",
      "Trained batch 778 batch loss 1.08337808 epoch total loss 1.22093725\n",
      "Trained batch 779 batch loss 1.07211483 epoch total loss 1.22074616\n",
      "Trained batch 780 batch loss 1.23636 epoch total loss 1.22076631\n",
      "Trained batch 781 batch loss 1.06737161 epoch total loss 1.22056985\n",
      "Trained batch 782 batch loss 1.2352854 epoch total loss 1.22058868\n",
      "Trained batch 783 batch loss 1.24999642 epoch total loss 1.22062624\n",
      "Trained batch 784 batch loss 1.32141328 epoch total loss 1.22075486\n",
      "Trained batch 785 batch loss 1.55863905 epoch total loss 1.22118521\n",
      "Trained batch 786 batch loss 1.38230562 epoch total loss 1.22139025\n",
      "Trained batch 787 batch loss 1.36855674 epoch total loss 1.22157729\n",
      "Trained batch 788 batch loss 1.30796194 epoch total loss 1.22168684\n",
      "Trained batch 789 batch loss 1.20655894 epoch total loss 1.22166765\n",
      "Trained batch 790 batch loss 1.17823315 epoch total loss 1.22161269\n",
      "Trained batch 791 batch loss 1.16064501 epoch total loss 1.22153568\n",
      "Trained batch 792 batch loss 1.26883626 epoch total loss 1.22159541\n",
      "Trained batch 793 batch loss 1.39229846 epoch total loss 1.22181058\n",
      "Trained batch 794 batch loss 1.18567514 epoch total loss 1.22176504\n",
      "Trained batch 795 batch loss 1.18925261 epoch total loss 1.22172427\n",
      "Trained batch 796 batch loss 1.09474552 epoch total loss 1.22156465\n",
      "Trained batch 797 batch loss 1.10683703 epoch total loss 1.22142065\n",
      "Trained batch 798 batch loss 1.09511757 epoch total loss 1.22126234\n",
      "Trained batch 799 batch loss 1.20217717 epoch total loss 1.22123849\n",
      "Trained batch 800 batch loss 1.17758119 epoch total loss 1.2211839\n",
      "Trained batch 801 batch loss 1.09401703 epoch total loss 1.22102511\n",
      "Trained batch 802 batch loss 1.21419358 epoch total loss 1.22101653\n",
      "Trained batch 803 batch loss 1.3418653 epoch total loss 1.22116697\n",
      "Trained batch 804 batch loss 1.39819074 epoch total loss 1.22138715\n",
      "Trained batch 805 batch loss 1.25470686 epoch total loss 1.22142851\n",
      "Trained batch 806 batch loss 1.48519599 epoch total loss 1.22175574\n",
      "Trained batch 807 batch loss 1.27015018 epoch total loss 1.22181571\n",
      "Trained batch 808 batch loss 1.36056852 epoch total loss 1.22198749\n",
      "Trained batch 809 batch loss 1.31153893 epoch total loss 1.22209823\n",
      "Trained batch 810 batch loss 1.12361622 epoch total loss 1.22197652\n",
      "Trained batch 811 batch loss 1.21444988 epoch total loss 1.22196734\n",
      "Trained batch 812 batch loss 1.17774367 epoch total loss 1.22191286\n",
      "Trained batch 813 batch loss 1.26920033 epoch total loss 1.22197104\n",
      "Trained batch 814 batch loss 1.2734077 epoch total loss 1.22203434\n",
      "Trained batch 815 batch loss 1.33215117 epoch total loss 1.2221694\n",
      "Trained batch 816 batch loss 1.29003 epoch total loss 1.22225261\n",
      "Trained batch 817 batch loss 1.26500702 epoch total loss 1.22230494\n",
      "Trained batch 818 batch loss 1.35499012 epoch total loss 1.22246706\n",
      "Trained batch 819 batch loss 1.10410559 epoch total loss 1.22232258\n",
      "Trained batch 820 batch loss 1.12615633 epoch total loss 1.22220528\n",
      "Trained batch 821 batch loss 1.18452 epoch total loss 1.22215939\n",
      "Trained batch 822 batch loss 1.24671888 epoch total loss 1.22218931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 823 batch loss 1.22137809 epoch total loss 1.22218823\n",
      "Trained batch 824 batch loss 1.14771843 epoch total loss 1.22209787\n",
      "Trained batch 825 batch loss 1.28031635 epoch total loss 1.22216845\n",
      "Trained batch 826 batch loss 1.13875735 epoch total loss 1.22206748\n",
      "Trained batch 827 batch loss 1.24477422 epoch total loss 1.22209489\n",
      "Trained batch 828 batch loss 1.30557787 epoch total loss 1.22219574\n",
      "Trained batch 829 batch loss 1.27383101 epoch total loss 1.22225797\n",
      "Trained batch 830 batch loss 1.26195419 epoch total loss 1.22230589\n",
      "Trained batch 831 batch loss 0.990479469 epoch total loss 1.22202682\n",
      "Trained batch 832 batch loss 1.15425038 epoch total loss 1.22194541\n",
      "Trained batch 833 batch loss 1.11852396 epoch total loss 1.22182119\n",
      "Trained batch 834 batch loss 1.07163858 epoch total loss 1.22164118\n",
      "Trained batch 835 batch loss 1.09256029 epoch total loss 1.22148669\n",
      "Trained batch 836 batch loss 1.1044538 epoch total loss 1.22134662\n",
      "Trained batch 837 batch loss 1.21151543 epoch total loss 1.22133482\n",
      "Trained batch 838 batch loss 1.15656173 epoch total loss 1.22125757\n",
      "Trained batch 839 batch loss 1.1540519 epoch total loss 1.22117746\n",
      "Trained batch 840 batch loss 1.16280603 epoch total loss 1.22110796\n",
      "Trained batch 841 batch loss 1.04727232 epoch total loss 1.22090125\n",
      "Trained batch 842 batch loss 1.30662847 epoch total loss 1.22100306\n",
      "Trained batch 843 batch loss 1.28848433 epoch total loss 1.22108305\n",
      "Trained batch 844 batch loss 1.25100124 epoch total loss 1.22111857\n",
      "Trained batch 845 batch loss 1.2520349 epoch total loss 1.22115517\n",
      "Trained batch 846 batch loss 1.20670068 epoch total loss 1.221138\n",
      "Trained batch 847 batch loss 1.28520119 epoch total loss 1.22121358\n",
      "Trained batch 848 batch loss 1.2578522 epoch total loss 1.22125673\n",
      "Trained batch 849 batch loss 1.27177513 epoch total loss 1.22131622\n",
      "Trained batch 850 batch loss 1.27839589 epoch total loss 1.22138345\n",
      "Trained batch 851 batch loss 1.29510975 epoch total loss 1.22147012\n",
      "Trained batch 852 batch loss 1.22298539 epoch total loss 1.22147191\n",
      "Trained batch 853 batch loss 1.26611364 epoch total loss 1.22152424\n",
      "Trained batch 854 batch loss 1.49126434 epoch total loss 1.22184\n",
      "Trained batch 855 batch loss 1.29790044 epoch total loss 1.22192895\n",
      "Trained batch 856 batch loss 1.20875263 epoch total loss 1.22191358\n",
      "Trained batch 857 batch loss 1.34517038 epoch total loss 1.22205746\n",
      "Trained batch 858 batch loss 1.19569755 epoch total loss 1.22202671\n",
      "Trained batch 859 batch loss 1.1158849 epoch total loss 1.22190309\n",
      "Trained batch 860 batch loss 1.26931822 epoch total loss 1.22195816\n",
      "Trained batch 861 batch loss 1.20705235 epoch total loss 1.22194088\n",
      "Trained batch 862 batch loss 1.26962852 epoch total loss 1.22199619\n",
      "Trained batch 863 batch loss 1.22842765 epoch total loss 1.22200358\n",
      "Trained batch 864 batch loss 1.16619217 epoch total loss 1.22193897\n",
      "Trained batch 865 batch loss 1.19374943 epoch total loss 1.2219063\n",
      "Trained batch 866 batch loss 1.25115311 epoch total loss 1.22194\n",
      "Trained batch 867 batch loss 1.15433383 epoch total loss 1.22186208\n",
      "Trained batch 868 batch loss 1.23346281 epoch total loss 1.22187543\n",
      "Trained batch 869 batch loss 1.18080807 epoch total loss 1.22182822\n",
      "Trained batch 870 batch loss 1.13388515 epoch total loss 1.22172713\n",
      "Trained batch 871 batch loss 1.1669488 epoch total loss 1.22166431\n",
      "Trained batch 872 batch loss 1.23770165 epoch total loss 1.22168267\n",
      "Trained batch 873 batch loss 1.10471392 epoch total loss 1.22154868\n",
      "Trained batch 874 batch loss 1.21159923 epoch total loss 1.22153723\n",
      "Trained batch 875 batch loss 1.21761942 epoch total loss 1.22153282\n",
      "Trained batch 876 batch loss 1.11082816 epoch total loss 1.22140646\n",
      "Trained batch 877 batch loss 1.0077517 epoch total loss 1.22116292\n",
      "Trained batch 878 batch loss 1.04932177 epoch total loss 1.22096717\n",
      "Trained batch 879 batch loss 1.21180868 epoch total loss 1.22095668\n",
      "Trained batch 880 batch loss 1.20483494 epoch total loss 1.22093844\n",
      "Trained batch 881 batch loss 1.24175692 epoch total loss 1.22096193\n",
      "Trained batch 882 batch loss 1.36427259 epoch total loss 1.22112441\n",
      "Trained batch 883 batch loss 1.17167425 epoch total loss 1.22106838\n",
      "Trained batch 884 batch loss 1.10755467 epoch total loss 1.22094\n",
      "Trained batch 885 batch loss 1.14443111 epoch total loss 1.22085345\n",
      "Trained batch 886 batch loss 1.15337038 epoch total loss 1.22077727\n",
      "Trained batch 887 batch loss 1.16255021 epoch total loss 1.22071171\n",
      "Trained batch 888 batch loss 1.27428222 epoch total loss 1.22077203\n",
      "Trained batch 889 batch loss 1.11649704 epoch total loss 1.22065461\n",
      "Trained batch 890 batch loss 1.03047109 epoch total loss 1.22044098\n",
      "Trained batch 891 batch loss 0.950572789 epoch total loss 1.22013807\n",
      "Trained batch 892 batch loss 1.10708141 epoch total loss 1.22001135\n",
      "Trained batch 893 batch loss 1.26068473 epoch total loss 1.22005701\n",
      "Trained batch 894 batch loss 1.46443784 epoch total loss 1.22033036\n",
      "Trained batch 895 batch loss 1.447523 epoch total loss 1.22058415\n",
      "Trained batch 896 batch loss 1.32953334 epoch total loss 1.22070587\n",
      "Trained batch 897 batch loss 1.13591874 epoch total loss 1.22061121\n",
      "Trained batch 898 batch loss 1.27737522 epoch total loss 1.2206744\n",
      "Trained batch 899 batch loss 1.20422125 epoch total loss 1.22065616\n",
      "Trained batch 900 batch loss 1.34111631 epoch total loss 1.22078991\n",
      "Trained batch 901 batch loss 1.2752676 epoch total loss 1.22085035\n",
      "Trained batch 902 batch loss 1.26907992 epoch total loss 1.22090387\n",
      "Trained batch 903 batch loss 1.26316857 epoch total loss 1.2209506\n",
      "Trained batch 904 batch loss 1.26224542 epoch total loss 1.22099626\n",
      "Trained batch 905 batch loss 1.18668103 epoch total loss 1.22095835\n",
      "Trained batch 906 batch loss 1.15964758 epoch total loss 1.22089064\n",
      "Trained batch 907 batch loss 1.10758889 epoch total loss 1.22076571\n",
      "Trained batch 908 batch loss 1.1782577 epoch total loss 1.22071886\n",
      "Trained batch 909 batch loss 1.17411113 epoch total loss 1.22066748\n",
      "Trained batch 910 batch loss 1.08296597 epoch total loss 1.2205162\n",
      "Trained batch 911 batch loss 1.10416698 epoch total loss 1.22038853\n",
      "Trained batch 912 batch loss 1.10383534 epoch total loss 1.22026074\n",
      "Trained batch 913 batch loss 1.12513542 epoch total loss 1.22015655\n",
      "Trained batch 914 batch loss 1.16022146 epoch total loss 1.22009099\n",
      "Trained batch 915 batch loss 1.13887382 epoch total loss 1.22000229\n",
      "Trained batch 916 batch loss 1.14708853 epoch total loss 1.21992278\n",
      "Trained batch 917 batch loss 1.23112965 epoch total loss 1.21993494\n",
      "Trained batch 918 batch loss 1.36164284 epoch total loss 1.22008932\n",
      "Trained batch 919 batch loss 1.29915059 epoch total loss 1.22017539\n",
      "Trained batch 920 batch loss 1.28613508 epoch total loss 1.22024703\n",
      "Trained batch 921 batch loss 1.17262292 epoch total loss 1.22019541\n",
      "Trained batch 922 batch loss 1.23685598 epoch total loss 1.22021341\n",
      "Trained batch 923 batch loss 1.05924022 epoch total loss 1.22003889\n",
      "Trained batch 924 batch loss 1.16060042 epoch total loss 1.21997464\n",
      "Trained batch 925 batch loss 1.1431 epoch total loss 1.21989155\n",
      "Trained batch 926 batch loss 1.34811 epoch total loss 1.22003007\n",
      "Trained batch 927 batch loss 1.29989839 epoch total loss 1.22011626\n",
      "Trained batch 928 batch loss 1.19225454 epoch total loss 1.22008622\n",
      "Trained batch 929 batch loss 1.22731161 epoch total loss 1.22009397\n",
      "Trained batch 930 batch loss 1.20787787 epoch total loss 1.22008085\n",
      "Trained batch 931 batch loss 1.16444826 epoch total loss 1.22002101\n",
      "Trained batch 932 batch loss 1.0536437 epoch total loss 1.21984243\n",
      "Trained batch 933 batch loss 1.26623774 epoch total loss 1.21989214\n",
      "Trained batch 934 batch loss 1.2442944 epoch total loss 1.21991825\n",
      "Trained batch 935 batch loss 1.20417678 epoch total loss 1.21990156\n",
      "Trained batch 936 batch loss 1.25500071 epoch total loss 1.21993899\n",
      "Trained batch 937 batch loss 1.21473575 epoch total loss 1.21993339\n",
      "Trained batch 938 batch loss 1.26550746 epoch total loss 1.21998203\n",
      "Trained batch 939 batch loss 1.27860582 epoch total loss 1.22004437\n",
      "Trained batch 940 batch loss 1.22171867 epoch total loss 1.22004616\n",
      "Trained batch 941 batch loss 1.32054174 epoch total loss 1.22015297\n",
      "Trained batch 942 batch loss 1.4080447 epoch total loss 1.22035241\n",
      "Trained batch 943 batch loss 1.30130029 epoch total loss 1.22043824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 944 batch loss 1.2844857 epoch total loss 1.22050619\n",
      "Trained batch 945 batch loss 1.22498918 epoch total loss 1.22051096\n",
      "Trained batch 946 batch loss 1.19249392 epoch total loss 1.22048128\n",
      "Trained batch 947 batch loss 1.21897554 epoch total loss 1.22047973\n",
      "Trained batch 948 batch loss 1.19099116 epoch total loss 1.22044873\n",
      "Trained batch 949 batch loss 1.31882334 epoch total loss 1.22055233\n",
      "Trained batch 950 batch loss 1.20058942 epoch total loss 1.22053134\n",
      "Trained batch 951 batch loss 1.34603071 epoch total loss 1.22066331\n",
      "Trained batch 952 batch loss 1.2385118 epoch total loss 1.22068214\n",
      "Trained batch 953 batch loss 1.23067427 epoch total loss 1.22069263\n",
      "Trained batch 954 batch loss 1.217906 epoch total loss 1.22068965\n",
      "Trained batch 955 batch loss 1.3807559 epoch total loss 1.22085726\n",
      "Trained batch 956 batch loss 1.24752676 epoch total loss 1.22088516\n",
      "Trained batch 957 batch loss 1.31622446 epoch total loss 1.22098494\n",
      "Trained batch 958 batch loss 1.33218205 epoch total loss 1.22110093\n",
      "Trained batch 959 batch loss 1.34837162 epoch total loss 1.22123373\n",
      "Trained batch 960 batch loss 1.28171301 epoch total loss 1.22129667\n",
      "Trained batch 961 batch loss 1.24705076 epoch total loss 1.22132349\n",
      "Trained batch 962 batch loss 1.23426557 epoch total loss 1.22133696\n",
      "Trained batch 963 batch loss 1.22240663 epoch total loss 1.22133803\n",
      "Trained batch 964 batch loss 1.20275 epoch total loss 1.22131884\n",
      "Trained batch 965 batch loss 1.20435715 epoch total loss 1.2213012\n",
      "Trained batch 966 batch loss 1.16589868 epoch total loss 1.22124386\n",
      "Trained batch 967 batch loss 1.19612551 epoch total loss 1.22121787\n",
      "Trained batch 968 batch loss 1.1186105 epoch total loss 1.22111201\n",
      "Trained batch 969 batch loss 1.12114036 epoch total loss 1.22100878\n",
      "Trained batch 970 batch loss 1.08768678 epoch total loss 1.22087121\n",
      "Trained batch 971 batch loss 1.13500738 epoch total loss 1.22078288\n",
      "Trained batch 972 batch loss 1.2199856 epoch total loss 1.22078204\n",
      "Trained batch 973 batch loss 1.2733953 epoch total loss 1.22083616\n",
      "Trained batch 974 batch loss 1.24580908 epoch total loss 1.22086179\n",
      "Trained batch 975 batch loss 1.27398443 epoch total loss 1.22091627\n",
      "Trained batch 976 batch loss 1.24750626 epoch total loss 1.22094357\n",
      "Trained batch 977 batch loss 1.12344861 epoch total loss 1.22084367\n",
      "Trained batch 978 batch loss 1.20987892 epoch total loss 1.22083247\n",
      "Trained batch 979 batch loss 1.15810323 epoch total loss 1.22076833\n",
      "Trained batch 980 batch loss 1.09660375 epoch total loss 1.22064161\n",
      "Trained batch 981 batch loss 1.28373313 epoch total loss 1.22070587\n",
      "Trained batch 982 batch loss 1.35654974 epoch total loss 1.22084427\n",
      "Trained batch 983 batch loss 1.37763071 epoch total loss 1.22100377\n",
      "Trained batch 984 batch loss 1.37654936 epoch total loss 1.22116184\n",
      "Trained batch 985 batch loss 1.36094224 epoch total loss 1.22130382\n",
      "Trained batch 986 batch loss 1.34512329 epoch total loss 1.22142935\n",
      "Trained batch 987 batch loss 1.26160872 epoch total loss 1.22147\n",
      "Trained batch 988 batch loss 1.25219488 epoch total loss 1.22150111\n",
      "Trained batch 989 batch loss 1.22177672 epoch total loss 1.22150147\n",
      "Trained batch 990 batch loss 1.28669095 epoch total loss 1.22156739\n",
      "Trained batch 991 batch loss 1.17358184 epoch total loss 1.22151899\n",
      "Trained batch 992 batch loss 1.21530354 epoch total loss 1.22151268\n",
      "Trained batch 993 batch loss 1.29782379 epoch total loss 1.22158957\n",
      "Trained batch 994 batch loss 1.18841314 epoch total loss 1.22155619\n",
      "Trained batch 995 batch loss 1.15280962 epoch total loss 1.22148705\n",
      "Trained batch 996 batch loss 1.16387153 epoch total loss 1.22142923\n",
      "Trained batch 997 batch loss 1.19192 epoch total loss 1.22139955\n",
      "Trained batch 998 batch loss 1.16989553 epoch total loss 1.22134793\n",
      "Trained batch 999 batch loss 1.28223777 epoch total loss 1.22140896\n",
      "Trained batch 1000 batch loss 1.20209837 epoch total loss 1.22138965\n",
      "Trained batch 1001 batch loss 1.27752411 epoch total loss 1.22144568\n",
      "Trained batch 1002 batch loss 1.30712068 epoch total loss 1.22153115\n",
      "Trained batch 1003 batch loss 1.37311924 epoch total loss 1.22168231\n",
      "Trained batch 1004 batch loss 1.13396 epoch total loss 1.22159493\n",
      "Trained batch 1005 batch loss 1.33695483 epoch total loss 1.22170973\n",
      "Trained batch 1006 batch loss 1.25349677 epoch total loss 1.22174132\n",
      "Trained batch 1007 batch loss 1.30296588 epoch total loss 1.22182202\n",
      "Trained batch 1008 batch loss 1.33107686 epoch total loss 1.22193038\n",
      "Trained batch 1009 batch loss 1.24253905 epoch total loss 1.22195077\n",
      "Trained batch 1010 batch loss 1.31412828 epoch total loss 1.22204208\n",
      "Trained batch 1011 batch loss 1.32166529 epoch total loss 1.22214055\n",
      "Trained batch 1012 batch loss 1.29940593 epoch total loss 1.22221696\n",
      "Trained batch 1013 batch loss 1.24830246 epoch total loss 1.22224271\n",
      "Trained batch 1014 batch loss 1.2521559 epoch total loss 1.22227228\n",
      "Trained batch 1015 batch loss 1.23542666 epoch total loss 1.22228527\n",
      "Trained batch 1016 batch loss 1.27522731 epoch total loss 1.22233737\n",
      "Trained batch 1017 batch loss 1.2090652 epoch total loss 1.22232437\n",
      "Trained batch 1018 batch loss 1.14507258 epoch total loss 1.22224844\n",
      "Trained batch 1019 batch loss 1.09297848 epoch total loss 1.2221216\n",
      "Trained batch 1020 batch loss 1.09429121 epoch total loss 1.22199619\n",
      "Trained batch 1021 batch loss 1.03234315 epoch total loss 1.22181046\n",
      "Trained batch 1022 batch loss 1.08163762 epoch total loss 1.22167337\n",
      "Trained batch 1023 batch loss 1.163782 epoch total loss 1.22161674\n",
      "Trained batch 1024 batch loss 1.34897745 epoch total loss 1.2217412\n",
      "Trained batch 1025 batch loss 1.24254107 epoch total loss 1.22176147\n",
      "Trained batch 1026 batch loss 1.27205563 epoch total loss 1.22181058\n",
      "Trained batch 1027 batch loss 1.26618147 epoch total loss 1.22185385\n",
      "Trained batch 1028 batch loss 1.25385296 epoch total loss 1.22188497\n",
      "Trained batch 1029 batch loss 1.23312521 epoch total loss 1.22189593\n",
      "Trained batch 1030 batch loss 1.31859207 epoch total loss 1.22198987\n",
      "Trained batch 1031 batch loss 1.28716195 epoch total loss 1.22205305\n",
      "Trained batch 1032 batch loss 1.0882268 epoch total loss 1.22192335\n",
      "Trained batch 1033 batch loss 1.11383212 epoch total loss 1.2218188\n",
      "Trained batch 1034 batch loss 1.23665857 epoch total loss 1.22183311\n",
      "Trained batch 1035 batch loss 1.19482374 epoch total loss 1.22180712\n",
      "Trained batch 1036 batch loss 1.27330732 epoch total loss 1.22185683\n",
      "Trained batch 1037 batch loss 1.39920473 epoch total loss 1.22202778\n",
      "Trained batch 1038 batch loss 1.31911922 epoch total loss 1.22212124\n",
      "Trained batch 1039 batch loss 1.09038329 epoch total loss 1.2219944\n",
      "Trained batch 1040 batch loss 1.1836133 epoch total loss 1.22195756\n",
      "Trained batch 1041 batch loss 1.2995646 epoch total loss 1.22203207\n",
      "Trained batch 1042 batch loss 1.1944176 epoch total loss 1.22200561\n",
      "Trained batch 1043 batch loss 1.22413719 epoch total loss 1.22200763\n",
      "Trained batch 1044 batch loss 1.1181438 epoch total loss 1.22190821\n",
      "Trained batch 1045 batch loss 1.10103691 epoch total loss 1.22179258\n",
      "Trained batch 1046 batch loss 1.10368466 epoch total loss 1.22167957\n",
      "Trained batch 1047 batch loss 1.18456018 epoch total loss 1.22164416\n",
      "Trained batch 1048 batch loss 1.09353435 epoch total loss 1.22152185\n",
      "Trained batch 1049 batch loss 1.09135652 epoch total loss 1.22139776\n",
      "Trained batch 1050 batch loss 1.18392324 epoch total loss 1.22136211\n",
      "Trained batch 1051 batch loss 1.11569655 epoch total loss 1.2212615\n",
      "Trained batch 1052 batch loss 1.10772634 epoch total loss 1.22115362\n",
      "Trained batch 1053 batch loss 1.08668399 epoch total loss 1.22102582\n",
      "Trained batch 1054 batch loss 1.0999819 epoch total loss 1.22091103\n",
      "Trained batch 1055 batch loss 1.24281883 epoch total loss 1.22093177\n",
      "Trained batch 1056 batch loss 1.24018598 epoch total loss 1.22095\n",
      "Trained batch 1057 batch loss 1.19924617 epoch total loss 1.2209295\n",
      "Trained batch 1058 batch loss 1.261132 epoch total loss 1.22096741\n",
      "Trained batch 1059 batch loss 1.23547959 epoch total loss 1.22098112\n",
      "Trained batch 1060 batch loss 1.28463221 epoch total loss 1.2210412\n",
      "Trained batch 1061 batch loss 1.28823674 epoch total loss 1.2211045\n",
      "Trained batch 1062 batch loss 1.29626691 epoch total loss 1.22117531\n",
      "Trained batch 1063 batch loss 1.18640184 epoch total loss 1.22114265\n",
      "Trained batch 1064 batch loss 1.24221778 epoch total loss 1.22116244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1065 batch loss 1.18658566 epoch total loss 1.22113\n",
      "Trained batch 1066 batch loss 1.24804854 epoch total loss 1.22115517\n",
      "Trained batch 1067 batch loss 1.27398777 epoch total loss 1.22120476\n",
      "Trained batch 1068 batch loss 1.31414223 epoch total loss 1.22129178\n",
      "Trained batch 1069 batch loss 1.2824918 epoch total loss 1.221349\n",
      "Trained batch 1070 batch loss 1.10366619 epoch total loss 1.22123897\n",
      "Trained batch 1071 batch loss 1.11950779 epoch total loss 1.22114396\n",
      "Trained batch 1072 batch loss 1.14746475 epoch total loss 1.2210753\n",
      "Trained batch 1073 batch loss 1.17178774 epoch total loss 1.22102928\n",
      "Trained batch 1074 batch loss 1.26721454 epoch total loss 1.22107232\n",
      "Trained batch 1075 batch loss 1.34930611 epoch total loss 1.22119164\n",
      "Trained batch 1076 batch loss 1.27434778 epoch total loss 1.221241\n",
      "Trained batch 1077 batch loss 1.17007589 epoch total loss 1.22119343\n",
      "Trained batch 1078 batch loss 1.27699053 epoch total loss 1.22124517\n",
      "Trained batch 1079 batch loss 1.28790236 epoch total loss 1.22130692\n",
      "Trained batch 1080 batch loss 1.10315704 epoch total loss 1.22119749\n",
      "Trained batch 1081 batch loss 1.13272285 epoch total loss 1.22111559\n",
      "Trained batch 1082 batch loss 1.16022086 epoch total loss 1.22105944\n",
      "Trained batch 1083 batch loss 1.21914518 epoch total loss 1.22105765\n",
      "Trained batch 1084 batch loss 1.14354837 epoch total loss 1.22098613\n",
      "Trained batch 1085 batch loss 1.23070025 epoch total loss 1.22099507\n",
      "Trained batch 1086 batch loss 1.25172448 epoch total loss 1.22102332\n",
      "Trained batch 1087 batch loss 1.42691958 epoch total loss 1.22121274\n",
      "Trained batch 1088 batch loss 1.27302814 epoch total loss 1.22126043\n",
      "Trained batch 1089 batch loss 1.01808357 epoch total loss 1.22107387\n",
      "Trained batch 1090 batch loss 1.04264021 epoch total loss 1.22091007\n",
      "Trained batch 1091 batch loss 1.099859 epoch total loss 1.22079909\n",
      "Trained batch 1092 batch loss 1.14171839 epoch total loss 1.22072673\n",
      "Trained batch 1093 batch loss 1.25083911 epoch total loss 1.22075427\n",
      "Trained batch 1094 batch loss 1.28437507 epoch total loss 1.22081244\n",
      "Trained batch 1095 batch loss 1.27025127 epoch total loss 1.22085762\n",
      "Trained batch 1096 batch loss 1.19680762 epoch total loss 1.22083569\n",
      "Trained batch 1097 batch loss 1.16817594 epoch total loss 1.22078764\n",
      "Trained batch 1098 batch loss 1.10637677 epoch total loss 1.22068346\n",
      "Trained batch 1099 batch loss 1.23419952 epoch total loss 1.22069585\n",
      "Trained batch 1100 batch loss 1.34257936 epoch total loss 1.2208066\n",
      "Trained batch 1101 batch loss 1.28266764 epoch total loss 1.22086275\n",
      "Trained batch 1102 batch loss 1.10964072 epoch total loss 1.22076178\n",
      "Trained batch 1103 batch loss 1.23369455 epoch total loss 1.22077346\n",
      "Trained batch 1104 batch loss 1.08611989 epoch total loss 1.22065151\n",
      "Trained batch 1105 batch loss 1.11827528 epoch total loss 1.22055888\n",
      "Trained batch 1106 batch loss 1.31335425 epoch total loss 1.22064281\n",
      "Trained batch 1107 batch loss 1.36382914 epoch total loss 1.22077203\n",
      "Trained batch 1108 batch loss 1.12942898 epoch total loss 1.22068954\n",
      "Trained batch 1109 batch loss 1.21390474 epoch total loss 1.22068346\n",
      "Trained batch 1110 batch loss 1.34986126 epoch total loss 1.2207998\n",
      "Trained batch 1111 batch loss 1.12570226 epoch total loss 1.22071421\n",
      "Trained batch 1112 batch loss 1.05799985 epoch total loss 1.22056782\n",
      "Trained batch 1113 batch loss 1.27959967 epoch total loss 1.22062087\n",
      "Trained batch 1114 batch loss 1.40083909 epoch total loss 1.22078264\n",
      "Trained batch 1115 batch loss 1.29681945 epoch total loss 1.22085094\n",
      "Trained batch 1116 batch loss 1.05752182 epoch total loss 1.22070456\n",
      "Trained batch 1117 batch loss 1.08554435 epoch total loss 1.22058356\n",
      "Trained batch 1118 batch loss 1.12366796 epoch total loss 1.22049689\n",
      "Trained batch 1119 batch loss 0.982206821 epoch total loss 1.22028387\n",
      "Trained batch 1120 batch loss 1.01372266 epoch total loss 1.22009945\n",
      "Trained batch 1121 batch loss 1.12404108 epoch total loss 1.22001374\n",
      "Trained batch 1122 batch loss 1.05321586 epoch total loss 1.21986508\n",
      "Trained batch 1123 batch loss 1.09543443 epoch total loss 1.21975434\n",
      "Trained batch 1124 batch loss 1.2058816 epoch total loss 1.21974194\n",
      "Trained batch 1125 batch loss 1.28586078 epoch total loss 1.21980083\n",
      "Trained batch 1126 batch loss 1.26007891 epoch total loss 1.21983659\n",
      "Trained batch 1127 batch loss 1.25509858 epoch total loss 1.21986794\n",
      "Trained batch 1128 batch loss 1.21587372 epoch total loss 1.21986437\n",
      "Trained batch 1129 batch loss 1.11529183 epoch total loss 1.21977162\n",
      "Trained batch 1130 batch loss 1.31372547 epoch total loss 1.21985483\n",
      "Trained batch 1131 batch loss 1.29841232 epoch total loss 1.21992433\n",
      "Trained batch 1132 batch loss 1.34119511 epoch total loss 1.22003138\n",
      "Trained batch 1133 batch loss 1.10464382 epoch total loss 1.21992958\n",
      "Trained batch 1134 batch loss 1.22264433 epoch total loss 1.21993196\n",
      "Trained batch 1135 batch loss 1.11674559 epoch total loss 1.219841\n",
      "Trained batch 1136 batch loss 1.07794774 epoch total loss 1.21971619\n",
      "Trained batch 1137 batch loss 1.10324407 epoch total loss 1.21961367\n",
      "Trained batch 1138 batch loss 1.19230878 epoch total loss 1.21958971\n",
      "Trained batch 1139 batch loss 1.25100946 epoch total loss 1.21961725\n",
      "Trained batch 1140 batch loss 1.13556123 epoch total loss 1.21954358\n",
      "Trained batch 1141 batch loss 1.08391595 epoch total loss 1.21942461\n",
      "Trained batch 1142 batch loss 1.20072961 epoch total loss 1.21940827\n",
      "Trained batch 1143 batch loss 1.37322545 epoch total loss 1.21954274\n",
      "Trained batch 1144 batch loss 1.16570687 epoch total loss 1.21949565\n",
      "Trained batch 1145 batch loss 1.26825011 epoch total loss 1.21953833\n",
      "Trained batch 1146 batch loss 1.05461478 epoch total loss 1.21939433\n",
      "Trained batch 1147 batch loss 0.898631096 epoch total loss 1.21911478\n",
      "Trained batch 1148 batch loss 0.936264515 epoch total loss 1.21886837\n",
      "Trained batch 1149 batch loss 0.933689 epoch total loss 1.21862018\n",
      "Trained batch 1150 batch loss 1.22186327 epoch total loss 1.21862304\n",
      "Trained batch 1151 batch loss 1.21781075 epoch total loss 1.21862233\n",
      "Trained batch 1152 batch loss 1.23033118 epoch total loss 1.21863246\n",
      "Trained batch 1153 batch loss 1.42546439 epoch total loss 1.21881187\n",
      "Trained batch 1154 batch loss 1.40537143 epoch total loss 1.21897352\n",
      "Trained batch 1155 batch loss 1.21377504 epoch total loss 1.21896899\n",
      "Trained batch 1156 batch loss 1.20072532 epoch total loss 1.21895313\n",
      "Trained batch 1157 batch loss 1.17851305 epoch total loss 1.2189182\n",
      "Trained batch 1158 batch loss 1.17404938 epoch total loss 1.21887946\n",
      "Trained batch 1159 batch loss 1.27533281 epoch total loss 1.21892822\n",
      "Trained batch 1160 batch loss 1.12949812 epoch total loss 1.21885109\n",
      "Trained batch 1161 batch loss 1.06171167 epoch total loss 1.21871579\n",
      "Trained batch 1162 batch loss 1.1157192 epoch total loss 1.21862721\n",
      "Trained batch 1163 batch loss 1.05931759 epoch total loss 1.21849024\n",
      "Trained batch 1164 batch loss 0.969098 epoch total loss 1.21827602\n",
      "Trained batch 1165 batch loss 1.19599366 epoch total loss 1.21825695\n",
      "Trained batch 1166 batch loss 1.51243222 epoch total loss 1.2185092\n",
      "Trained batch 1167 batch loss 1.4104377 epoch total loss 1.21867359\n",
      "Trained batch 1168 batch loss 1.47683024 epoch total loss 1.2188946\n",
      "Trained batch 1169 batch loss 1.33460402 epoch total loss 1.21899366\n",
      "Trained batch 1170 batch loss 1.44566369 epoch total loss 1.21918738\n",
      "Trained batch 1171 batch loss 1.34472704 epoch total loss 1.21929455\n",
      "Trained batch 1172 batch loss 1.31498241 epoch total loss 1.21937621\n",
      "Trained batch 1173 batch loss 1.19428587 epoch total loss 1.21935487\n",
      "Trained batch 1174 batch loss 1.1986618 epoch total loss 1.21933711\n",
      "Trained batch 1175 batch loss 1.23504734 epoch total loss 1.21935058\n",
      "Trained batch 1176 batch loss 1.26966155 epoch total loss 1.21939337\n",
      "Trained batch 1177 batch loss 1.36790919 epoch total loss 1.2195195\n",
      "Trained batch 1178 batch loss 1.26910329 epoch total loss 1.21956158\n",
      "Trained batch 1179 batch loss 1.20305562 epoch total loss 1.21954751\n",
      "Trained batch 1180 batch loss 1.2555058 epoch total loss 1.21957803\n",
      "Trained batch 1181 batch loss 1.23759902 epoch total loss 1.21959317\n",
      "Trained batch 1182 batch loss 1.21160901 epoch total loss 1.21958649\n",
      "Trained batch 1183 batch loss 1.191944 epoch total loss 1.21956313\n",
      "Trained batch 1184 batch loss 1.21032834 epoch total loss 1.21955526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1185 batch loss 1.21766698 epoch total loss 1.21955371\n",
      "Trained batch 1186 batch loss 1.25850725 epoch total loss 1.21958661\n",
      "Trained batch 1187 batch loss 1.21028161 epoch total loss 1.21957874\n",
      "Trained batch 1188 batch loss 1.30989838 epoch total loss 1.2196548\n",
      "Trained batch 1189 batch loss 1.34144342 epoch total loss 1.21975732\n",
      "Trained batch 1190 batch loss 1.23818398 epoch total loss 1.2197727\n",
      "Trained batch 1191 batch loss 1.3149147 epoch total loss 1.21985269\n",
      "Trained batch 1192 batch loss 1.26406264 epoch total loss 1.21988964\n",
      "Trained batch 1193 batch loss 1.0902276 epoch total loss 1.21978104\n",
      "Trained batch 1194 batch loss 1.16291595 epoch total loss 1.21973336\n",
      "Trained batch 1195 batch loss 1.17595875 epoch total loss 1.21969676\n",
      "Trained batch 1196 batch loss 1.16515398 epoch total loss 1.2196511\n",
      "Trained batch 1197 batch loss 1.24123681 epoch total loss 1.2196691\n",
      "Trained batch 1198 batch loss 1.17900562 epoch total loss 1.21963513\n",
      "Trained batch 1199 batch loss 1.25348949 epoch total loss 1.21966338\n",
      "Trained batch 1200 batch loss 1.37319052 epoch total loss 1.21979141\n",
      "Trained batch 1201 batch loss 1.28850389 epoch total loss 1.21984851\n",
      "Trained batch 1202 batch loss 1.40589869 epoch total loss 1.22000325\n",
      "Trained batch 1203 batch loss 1.38992262 epoch total loss 1.22014451\n",
      "Trained batch 1204 batch loss 1.17595541 epoch total loss 1.22010779\n",
      "Trained batch 1205 batch loss 1.14523065 epoch total loss 1.22004569\n",
      "Trained batch 1206 batch loss 1.27506852 epoch total loss 1.22009122\n",
      "Trained batch 1207 batch loss 1.32203913 epoch total loss 1.22017574\n",
      "Trained batch 1208 batch loss 1.12950814 epoch total loss 1.22010064\n",
      "Trained batch 1209 batch loss 1.16622376 epoch total loss 1.22005618\n",
      "Trained batch 1210 batch loss 1.12502885 epoch total loss 1.21997762\n",
      "Trained batch 1211 batch loss 1.13474834 epoch total loss 1.21990716\n",
      "Trained batch 1212 batch loss 1.10756516 epoch total loss 1.21981454\n",
      "Trained batch 1213 batch loss 1.13455153 epoch total loss 1.21974421\n",
      "Trained batch 1214 batch loss 1.187621 epoch total loss 1.21971774\n",
      "Trained batch 1215 batch loss 1.18297541 epoch total loss 1.21968746\n",
      "Trained batch 1216 batch loss 1.28208876 epoch total loss 1.21973884\n",
      "Trained batch 1217 batch loss 1.24721956 epoch total loss 1.21976137\n",
      "Trained batch 1218 batch loss 1.33000469 epoch total loss 1.21985185\n",
      "Trained batch 1219 batch loss 1.22604728 epoch total loss 1.21985698\n",
      "Trained batch 1220 batch loss 1.08876729 epoch total loss 1.21974945\n",
      "Trained batch 1221 batch loss 1.05593729 epoch total loss 1.21961534\n",
      "Trained batch 1222 batch loss 1.14900517 epoch total loss 1.21955752\n",
      "Trained batch 1223 batch loss 1.32122505 epoch total loss 1.21964061\n",
      "Trained batch 1224 batch loss 1.24355245 epoch total loss 1.21966016\n",
      "Trained batch 1225 batch loss 1.33449435 epoch total loss 1.21975386\n",
      "Trained batch 1226 batch loss 1.44930732 epoch total loss 1.21994114\n",
      "Trained batch 1227 batch loss 1.23550797 epoch total loss 1.21995378\n",
      "Trained batch 1228 batch loss 1.20067573 epoch total loss 1.21993804\n",
      "Trained batch 1229 batch loss 1.28385615 epoch total loss 1.21999\n",
      "Trained batch 1230 batch loss 1.26782298 epoch total loss 1.220029\n",
      "Trained batch 1231 batch loss 1.25013018 epoch total loss 1.22005343\n",
      "Trained batch 1232 batch loss 1.26261115 epoch total loss 1.22008789\n",
      "Trained batch 1233 batch loss 1.28088987 epoch total loss 1.22013724\n",
      "Trained batch 1234 batch loss 1.30456543 epoch total loss 1.22020566\n",
      "Trained batch 1235 batch loss 1.25439739 epoch total loss 1.22023332\n",
      "Trained batch 1236 batch loss 1.26471043 epoch total loss 1.22026932\n",
      "Trained batch 1237 batch loss 1.2767508 epoch total loss 1.22031498\n",
      "Trained batch 1238 batch loss 1.14124382 epoch total loss 1.22025108\n",
      "Trained batch 1239 batch loss 1.37796569 epoch total loss 1.2203784\n",
      "Trained batch 1240 batch loss 1.20117497 epoch total loss 1.2203629\n",
      "Trained batch 1241 batch loss 1.18382514 epoch total loss 1.22033346\n",
      "Trained batch 1242 batch loss 1.19124663 epoch total loss 1.22031009\n",
      "Trained batch 1243 batch loss 1.25247288 epoch total loss 1.22033596\n",
      "Trained batch 1244 batch loss 1.29538321 epoch total loss 1.22039628\n",
      "Trained batch 1245 batch loss 1.2043345 epoch total loss 1.22038341\n",
      "Trained batch 1246 batch loss 1.00274277 epoch total loss 1.22020864\n",
      "Trained batch 1247 batch loss 1.00882161 epoch total loss 1.22003913\n",
      "Trained batch 1248 batch loss 1.14900279 epoch total loss 1.21998227\n",
      "Trained batch 1249 batch loss 1.22020519 epoch total loss 1.21998239\n",
      "Trained batch 1250 batch loss 1.15307319 epoch total loss 1.21992886\n",
      "Trained batch 1251 batch loss 1.22083354 epoch total loss 1.21992958\n",
      "Trained batch 1252 batch loss 1.14595342 epoch total loss 1.21987057\n",
      "Trained batch 1253 batch loss 1.25037456 epoch total loss 1.21989489\n",
      "Trained batch 1254 batch loss 1.22695851 epoch total loss 1.21990049\n",
      "Trained batch 1255 batch loss 1.23953307 epoch total loss 1.21991611\n",
      "Trained batch 1256 batch loss 1.39839697 epoch total loss 1.22005832\n",
      "Trained batch 1257 batch loss 1.40886593 epoch total loss 1.22020841\n",
      "Trained batch 1258 batch loss 1.3067143 epoch total loss 1.22027719\n",
      "Trained batch 1259 batch loss 1.21220577 epoch total loss 1.22027075\n",
      "Trained batch 1260 batch loss 1.36878669 epoch total loss 1.22038865\n",
      "Trained batch 1261 batch loss 1.35590863 epoch total loss 1.22049618\n",
      "Trained batch 1262 batch loss 1.35495281 epoch total loss 1.22060275\n",
      "Trained batch 1263 batch loss 1.26376557 epoch total loss 1.22063696\n",
      "Trained batch 1264 batch loss 1.33726513 epoch total loss 1.22072923\n",
      "Trained batch 1265 batch loss 1.25289094 epoch total loss 1.22075462\n",
      "Trained batch 1266 batch loss 1.24888217 epoch total loss 1.22077692\n",
      "Trained batch 1267 batch loss 1.15367031 epoch total loss 1.22072399\n",
      "Trained batch 1268 batch loss 1.31069255 epoch total loss 1.22079492\n",
      "Trained batch 1269 batch loss 1.16356468 epoch total loss 1.22074974\n",
      "Trained batch 1270 batch loss 1.21393466 epoch total loss 1.22074449\n",
      "Trained batch 1271 batch loss 1.13980436 epoch total loss 1.22068071\n",
      "Trained batch 1272 batch loss 1.17673051 epoch total loss 1.22064626\n",
      "Trained batch 1273 batch loss 1.1125083 epoch total loss 1.22056127\n",
      "Trained batch 1274 batch loss 1.03359056 epoch total loss 1.22041452\n",
      "Trained batch 1275 batch loss 1.04253542 epoch total loss 1.22027493\n",
      "Trained batch 1276 batch loss 1.26702189 epoch total loss 1.22031152\n",
      "Trained batch 1277 batch loss 1.23382831 epoch total loss 1.22032225\n",
      "Trained batch 1278 batch loss 1.37101114 epoch total loss 1.22044\n",
      "Trained batch 1279 batch loss 1.32422078 epoch total loss 1.22052121\n",
      "Trained batch 1280 batch loss 1.35951507 epoch total loss 1.22062981\n",
      "Trained batch 1281 batch loss 1.20345056 epoch total loss 1.22061646\n",
      "Trained batch 1282 batch loss 1.22914279 epoch total loss 1.22062302\n",
      "Trained batch 1283 batch loss 1.22748458 epoch total loss 1.22062838\n",
      "Trained batch 1284 batch loss 1.21532834 epoch total loss 1.22062433\n",
      "Trained batch 1285 batch loss 1.19942188 epoch total loss 1.22060788\n",
      "Trained batch 1286 batch loss 1.12052941 epoch total loss 1.22053\n",
      "Trained batch 1287 batch loss 1.23639214 epoch total loss 1.22054231\n",
      "Trained batch 1288 batch loss 1.19620693 epoch total loss 1.22052348\n",
      "Trained batch 1289 batch loss 1.12231147 epoch total loss 1.22044718\n",
      "Trained batch 1290 batch loss 1.234043 epoch total loss 1.22045779\n",
      "Trained batch 1291 batch loss 1.15723062 epoch total loss 1.2204088\n",
      "Trained batch 1292 batch loss 1.18258357 epoch total loss 1.22037947\n",
      "Trained batch 1293 batch loss 1.11395144 epoch total loss 1.22029722\n",
      "Trained batch 1294 batch loss 1.12627852 epoch total loss 1.2202245\n",
      "Trained batch 1295 batch loss 1.25245249 epoch total loss 1.2202493\n",
      "Trained batch 1296 batch loss 1.11189246 epoch total loss 1.22016573\n",
      "Trained batch 1297 batch loss 1.26375151 epoch total loss 1.22019947\n",
      "Trained batch 1298 batch loss 1.20288754 epoch total loss 1.22018611\n",
      "Trained batch 1299 batch loss 1.07964659 epoch total loss 1.22007787\n",
      "Trained batch 1300 batch loss 1.26386225 epoch total loss 1.22011161\n",
      "Trained batch 1301 batch loss 1.25903606 epoch total loss 1.22014141\n",
      "Trained batch 1302 batch loss 1.18886805 epoch total loss 1.22011745\n",
      "Trained batch 1303 batch loss 1.30808127 epoch total loss 1.22018492\n",
      "Trained batch 1304 batch loss 1.23563194 epoch total loss 1.22019672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1305 batch loss 1.33571386 epoch total loss 1.2202853\n",
      "Trained batch 1306 batch loss 1.25146306 epoch total loss 1.22030914\n",
      "Trained batch 1307 batch loss 1.33113432 epoch total loss 1.22039402\n",
      "Trained batch 1308 batch loss 1.23006904 epoch total loss 1.22040141\n",
      "Trained batch 1309 batch loss 1.28835309 epoch total loss 1.22045326\n",
      "Trained batch 1310 batch loss 1.27744758 epoch total loss 1.22049677\n",
      "Trained batch 1311 batch loss 1.20519054 epoch total loss 1.22048509\n",
      "Trained batch 1312 batch loss 1.25290942 epoch total loss 1.22050989\n",
      "Trained batch 1313 batch loss 1.31765473 epoch total loss 1.2205838\n",
      "Trained batch 1314 batch loss 1.31431389 epoch total loss 1.2206552\n",
      "Trained batch 1315 batch loss 1.28094792 epoch total loss 1.2207011\n",
      "Trained batch 1316 batch loss 1.17551351 epoch total loss 1.22066677\n",
      "Trained batch 1317 batch loss 1.03903246 epoch total loss 1.22052884\n",
      "Trained batch 1318 batch loss 1.1003902 epoch total loss 1.22043765\n",
      "Trained batch 1319 batch loss 1.1383301 epoch total loss 1.22037542\n",
      "Trained batch 1320 batch loss 1.15152383 epoch total loss 1.2203232\n",
      "Trained batch 1321 batch loss 1.34500408 epoch total loss 1.22041762\n",
      "Trained batch 1322 batch loss 1.29138148 epoch total loss 1.22047126\n",
      "Trained batch 1323 batch loss 1.14772069 epoch total loss 1.22041631\n",
      "Trained batch 1324 batch loss 1.23231077 epoch total loss 1.22042525\n",
      "Trained batch 1325 batch loss 1.37318575 epoch total loss 1.22054052\n",
      "Trained batch 1326 batch loss 1.22431302 epoch total loss 1.22054338\n",
      "Trained batch 1327 batch loss 1.27362621 epoch total loss 1.22058344\n",
      "Trained batch 1328 batch loss 1.30770159 epoch total loss 1.22064912\n",
      "Trained batch 1329 batch loss 1.23925829 epoch total loss 1.22066307\n",
      "Trained batch 1330 batch loss 1.32578921 epoch total loss 1.22074211\n",
      "Trained batch 1331 batch loss 1.28109193 epoch total loss 1.22078753\n",
      "Trained batch 1332 batch loss 1.31775987 epoch total loss 1.22086024\n",
      "Trained batch 1333 batch loss 1.30354 epoch total loss 1.22092235\n",
      "Trained batch 1334 batch loss 1.23210204 epoch total loss 1.2209307\n",
      "Trained batch 1335 batch loss 1.3646605 epoch total loss 1.22103834\n",
      "Trained batch 1336 batch loss 1.16933012 epoch total loss 1.2209996\n",
      "Trained batch 1337 batch loss 1.24927127 epoch total loss 1.22102082\n",
      "Trained batch 1338 batch loss 1.26782584 epoch total loss 1.22105575\n",
      "Trained batch 1339 batch loss 1.16065383 epoch total loss 1.22101068\n",
      "Trained batch 1340 batch loss 1.29181826 epoch total loss 1.22106349\n",
      "Trained batch 1341 batch loss 1.08314061 epoch total loss 1.22096062\n",
      "Trained batch 1342 batch loss 1.28328419 epoch total loss 1.22100711\n",
      "Trained batch 1343 batch loss 1.24054813 epoch total loss 1.22102165\n",
      "Trained batch 1344 batch loss 1.20987129 epoch total loss 1.22101343\n",
      "Trained batch 1345 batch loss 1.23934913 epoch total loss 1.22102702\n",
      "Trained batch 1346 batch loss 1.17869353 epoch total loss 1.22099555\n",
      "Trained batch 1347 batch loss 1.24856615 epoch total loss 1.22101605\n",
      "Trained batch 1348 batch loss 1.23942471 epoch total loss 1.22102964\n",
      "Trained batch 1349 batch loss 1.13234985 epoch total loss 1.22096395\n",
      "Trained batch 1350 batch loss 1.23795617 epoch total loss 1.22097647\n",
      "Trained batch 1351 batch loss 1.1656245 epoch total loss 1.22093546\n",
      "Trained batch 1352 batch loss 1.12392926 epoch total loss 1.2208637\n",
      "Trained batch 1353 batch loss 1.20991552 epoch total loss 1.22085571\n",
      "Trained batch 1354 batch loss 1.40112376 epoch total loss 1.22098887\n",
      "Trained batch 1355 batch loss 1.19868636 epoch total loss 1.22097242\n",
      "Trained batch 1356 batch loss 1.33675 epoch total loss 1.22105777\n",
      "Trained batch 1357 batch loss 1.26360059 epoch total loss 1.22108912\n",
      "Trained batch 1358 batch loss 1.3682301 epoch total loss 1.22119749\n",
      "Trained batch 1359 batch loss 1.29331899 epoch total loss 1.22125065\n",
      "Trained batch 1360 batch loss 1.29849124 epoch total loss 1.2213074\n",
      "Trained batch 1361 batch loss 1.19203186 epoch total loss 1.22128582\n",
      "Trained batch 1362 batch loss 1.27412724 epoch total loss 1.22132468\n",
      "Trained batch 1363 batch loss 1.18699479 epoch total loss 1.22129953\n",
      "Trained batch 1364 batch loss 1.08904612 epoch total loss 1.22120249\n",
      "Trained batch 1365 batch loss 1.04411459 epoch total loss 1.22107279\n",
      "Trained batch 1366 batch loss 1.02435338 epoch total loss 1.22092879\n",
      "Trained batch 1367 batch loss 1.16347229 epoch total loss 1.22088671\n",
      "Trained batch 1368 batch loss 1.24862921 epoch total loss 1.22090697\n",
      "Trained batch 1369 batch loss 1.23134589 epoch total loss 1.2209146\n",
      "Trained batch 1370 batch loss 1.2883147 epoch total loss 1.22096384\n",
      "Trained batch 1371 batch loss 1.39662552 epoch total loss 1.22109199\n",
      "Trained batch 1372 batch loss 1.36128342 epoch total loss 1.22119415\n",
      "Trained batch 1373 batch loss 1.47210121 epoch total loss 1.2213769\n",
      "Trained batch 1374 batch loss 1.2522037 epoch total loss 1.22139931\n",
      "Trained batch 1375 batch loss 1.14593267 epoch total loss 1.22134435\n",
      "Trained batch 1376 batch loss 1.21536314 epoch total loss 1.22134006\n",
      "Trained batch 1377 batch loss 1.20483065 epoch total loss 1.22132802\n",
      "Trained batch 1378 batch loss 1.00598252 epoch total loss 1.22117174\n",
      "Trained batch 1379 batch loss 1.05214632 epoch total loss 1.22104919\n",
      "Trained batch 1380 batch loss 1.11608398 epoch total loss 1.22097313\n",
      "Trained batch 1381 batch loss 1.10100746 epoch total loss 1.22088623\n",
      "Trained batch 1382 batch loss 1.0272795 epoch total loss 1.22074604\n",
      "Trained batch 1383 batch loss 1.05521417 epoch total loss 1.22062635\n",
      "Trained batch 1384 batch loss 1.12097406 epoch total loss 1.22055435\n",
      "Trained batch 1385 batch loss 1.20308173 epoch total loss 1.22054172\n",
      "Trained batch 1386 batch loss 1.03013325 epoch total loss 1.22040439\n",
      "Trained batch 1387 batch loss 1.01624322 epoch total loss 1.22025716\n",
      "Trained batch 1388 batch loss 1.01553261 epoch total loss 1.2201097\n",
      "Epoch 4 train loss 1.2201097011566162\n",
      "Validated batch 1 batch loss 1.22110152\n",
      "Validated batch 2 batch loss 1.22944188\n",
      "Validated batch 3 batch loss 1.18036425\n",
      "Validated batch 4 batch loss 1.26425099\n",
      "Validated batch 5 batch loss 1.23433173\n",
      "Validated batch 6 batch loss 1.29048884\n",
      "Validated batch 7 batch loss 1.33545446\n",
      "Validated batch 8 batch loss 1.2766695\n",
      "Validated batch 9 batch loss 1.25772834\n",
      "Validated batch 10 batch loss 1.19324219\n",
      "Validated batch 11 batch loss 1.29261482\n",
      "Validated batch 12 batch loss 1.21856678\n",
      "Validated batch 13 batch loss 1.22016633\n",
      "Validated batch 14 batch loss 1.33409786\n",
      "Validated batch 15 batch loss 1.29390037\n",
      "Validated batch 16 batch loss 1.23531\n",
      "Validated batch 17 batch loss 1.37893629\n",
      "Validated batch 18 batch loss 1.12701154\n",
      "Validated batch 19 batch loss 1.27569389\n",
      "Validated batch 20 batch loss 0.987554371\n",
      "Validated batch 21 batch loss 1.23527622\n",
      "Validated batch 22 batch loss 1.32496\n",
      "Validated batch 23 batch loss 1.09409702\n",
      "Validated batch 24 batch loss 1.22405565\n",
      "Validated batch 25 batch loss 1.17833567\n",
      "Validated batch 26 batch loss 1.1679635\n",
      "Validated batch 27 batch loss 1.16104388\n",
      "Validated batch 28 batch loss 1.15472937\n",
      "Validated batch 29 batch loss 1.29329014\n",
      "Validated batch 30 batch loss 1.20097709\n",
      "Validated batch 31 batch loss 1.06688941\n",
      "Validated batch 32 batch loss 1.09351552\n",
      "Validated batch 33 batch loss 1.15519619\n",
      "Validated batch 34 batch loss 1.13358104\n",
      "Validated batch 35 batch loss 1.1625185\n",
      "Validated batch 36 batch loss 1.15165222\n",
      "Validated batch 37 batch loss 1.22082663\n",
      "Validated batch 38 batch loss 1.31062961\n",
      "Validated batch 39 batch loss 1.27556968\n",
      "Validated batch 40 batch loss 1.18298268\n",
      "Validated batch 41 batch loss 1.2660768\n",
      "Validated batch 42 batch loss 0.954548717\n",
      "Validated batch 43 batch loss 1.11136901\n",
      "Validated batch 44 batch loss 1.08176446\n",
      "Validated batch 45 batch loss 1.1951642\n",
      "Validated batch 46 batch loss 1.40583718\n",
      "Validated batch 47 batch loss 1.12798512\n",
      "Validated batch 48 batch loss 1.25845504\n",
      "Validated batch 49 batch loss 1.2682662\n",
      "Validated batch 50 batch loss 1.15177727\n",
      "Validated batch 51 batch loss 1.29150271\n",
      "Validated batch 52 batch loss 1.39958882\n",
      "Validated batch 53 batch loss 1.07733464\n",
      "Validated batch 54 batch loss 1.23768175\n",
      "Validated batch 55 batch loss 1.228755\n",
      "Validated batch 56 batch loss 1.28886569\n",
      "Validated batch 57 batch loss 1.27495754\n",
      "Validated batch 58 batch loss 1.09940636\n",
      "Validated batch 59 batch loss 1.11333704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 60 batch loss 1.15362263\n",
      "Validated batch 61 batch loss 1.1696316\n",
      "Validated batch 62 batch loss 1.1394186\n",
      "Validated batch 63 batch loss 1.21734214\n",
      "Validated batch 64 batch loss 1.12509346\n",
      "Validated batch 65 batch loss 1.22038651\n",
      "Validated batch 66 batch loss 1.27094424\n",
      "Validated batch 67 batch loss 1.25358224\n",
      "Validated batch 68 batch loss 1.25182617\n",
      "Validated batch 69 batch loss 1.10713029\n",
      "Validated batch 70 batch loss 1.35642505\n",
      "Validated batch 71 batch loss 1.23718274\n",
      "Validated batch 72 batch loss 1.16762257\n",
      "Validated batch 73 batch loss 1.22635031\n",
      "Validated batch 74 batch loss 1.15540099\n",
      "Validated batch 75 batch loss 1.26246905\n",
      "Validated batch 76 batch loss 1.24500751\n",
      "Validated batch 77 batch loss 1.16855192\n",
      "Validated batch 78 batch loss 1.21303785\n",
      "Validated batch 79 batch loss 1.21809042\n",
      "Validated batch 80 batch loss 1.26246524\n",
      "Validated batch 81 batch loss 1.31565416\n",
      "Validated batch 82 batch loss 1.24146867\n",
      "Validated batch 83 batch loss 1.13911629\n",
      "Validated batch 84 batch loss 1.18961978\n",
      "Validated batch 85 batch loss 1.20222318\n",
      "Validated batch 86 batch loss 1.17565489\n",
      "Validated batch 87 batch loss 1.27533436\n",
      "Validated batch 88 batch loss 1.29482555\n",
      "Validated batch 89 batch loss 1.48775077\n",
      "Validated batch 90 batch loss 1.32356334\n",
      "Validated batch 91 batch loss 1.25972009\n",
      "Validated batch 92 batch loss 1.15208089\n",
      "Validated batch 93 batch loss 1.17545664\n",
      "Validated batch 94 batch loss 1.22656918\n",
      "Validated batch 95 batch loss 1.20025969\n",
      "Validated batch 96 batch loss 1.19651103\n",
      "Validated batch 97 batch loss 1.30879021\n",
      "Validated batch 98 batch loss 1.39662647\n",
      "Validated batch 99 batch loss 1.17311215\n",
      "Validated batch 100 batch loss 1.24782431\n",
      "Validated batch 101 batch loss 1.16499567\n",
      "Validated batch 102 batch loss 1.27551079\n",
      "Validated batch 103 batch loss 1.24827766\n",
      "Validated batch 104 batch loss 1.16140866\n",
      "Validated batch 105 batch loss 1.34518635\n",
      "Validated batch 106 batch loss 1.26726723\n",
      "Validated batch 107 batch loss 1.2741735\n",
      "Validated batch 108 batch loss 1.24926257\n",
      "Validated batch 109 batch loss 1.27898407\n",
      "Validated batch 110 batch loss 1.15475631\n",
      "Validated batch 111 batch loss 1.20371437\n",
      "Validated batch 112 batch loss 1.21549392\n",
      "Validated batch 113 batch loss 1.15418077\n",
      "Validated batch 114 batch loss 1.27976561\n",
      "Validated batch 115 batch loss 1.1915915\n",
      "Validated batch 116 batch loss 1.32215858\n",
      "Validated batch 117 batch loss 1.2560997\n",
      "Validated batch 118 batch loss 1.14729333\n",
      "Validated batch 119 batch loss 1.18485522\n",
      "Validated batch 120 batch loss 1.21390676\n",
      "Validated batch 121 batch loss 1.22462189\n",
      "Validated batch 122 batch loss 1.27688932\n",
      "Validated batch 123 batch loss 1.19863796\n",
      "Validated batch 124 batch loss 1.19038749\n",
      "Validated batch 125 batch loss 1.31719184\n",
      "Validated batch 126 batch loss 1.16271472\n",
      "Validated batch 127 batch loss 1.11592627\n",
      "Validated batch 128 batch loss 1.18260288\n",
      "Validated batch 129 batch loss 1.3811543\n",
      "Validated batch 130 batch loss 1.33054876\n",
      "Validated batch 131 batch loss 1.35224974\n",
      "Validated batch 132 batch loss 1.22412539\n",
      "Validated batch 133 batch loss 1.38926792\n",
      "Validated batch 134 batch loss 1.21069908\n",
      "Validated batch 135 batch loss 1.31927431\n",
      "Validated batch 136 batch loss 1.31305993\n",
      "Validated batch 137 batch loss 1.01167369\n",
      "Validated batch 138 batch loss 1.19817209\n",
      "Validated batch 139 batch loss 1.24088192\n",
      "Validated batch 140 batch loss 1.23968422\n",
      "Validated batch 141 batch loss 1.16000652\n",
      "Validated batch 142 batch loss 1.1596837\n",
      "Validated batch 143 batch loss 1.15315628\n",
      "Validated batch 144 batch loss 1.31835163\n",
      "Validated batch 145 batch loss 1.21514225\n",
      "Validated batch 146 batch loss 1.34394634\n",
      "Validated batch 147 batch loss 1.29995537\n",
      "Validated batch 148 batch loss 1.29471934\n",
      "Validated batch 149 batch loss 1.22985959\n",
      "Validated batch 150 batch loss 1.36117864\n",
      "Validated batch 151 batch loss 1.25430989\n",
      "Validated batch 152 batch loss 1.31357932\n",
      "Validated batch 153 batch loss 1.28035426\n",
      "Validated batch 154 batch loss 1.39203334\n",
      "Validated batch 155 batch loss 1.3349731\n",
      "Validated batch 156 batch loss 1.17161417\n",
      "Validated batch 157 batch loss 1.18717611\n",
      "Validated batch 158 batch loss 1.31581438\n",
      "Validated batch 159 batch loss 1.33968973\n",
      "Validated batch 160 batch loss 1.17909622\n",
      "Validated batch 161 batch loss 1.24064529\n",
      "Validated batch 162 batch loss 1.21269441\n",
      "Validated batch 163 batch loss 1.09736061\n",
      "Validated batch 164 batch loss 1.21365333\n",
      "Validated batch 165 batch loss 1.19633889\n",
      "Validated batch 166 batch loss 1.1479404\n",
      "Validated batch 167 batch loss 1.23757267\n",
      "Validated batch 168 batch loss 1.15639114\n",
      "Validated batch 169 batch loss 1.25447643\n",
      "Validated batch 170 batch loss 1.33282042\n",
      "Validated batch 171 batch loss 1.07576752\n",
      "Validated batch 172 batch loss 1.29141557\n",
      "Validated batch 173 batch loss 1.22482061\n",
      "Validated batch 174 batch loss 1.13310027\n",
      "Validated batch 175 batch loss 1.23772931\n",
      "Validated batch 176 batch loss 1.24892426\n",
      "Validated batch 177 batch loss 1.16826963\n",
      "Validated batch 178 batch loss 1.33022892\n",
      "Validated batch 179 batch loss 1.27603519\n",
      "Validated batch 180 batch loss 1.28356576\n",
      "Validated batch 181 batch loss 1.15410018\n",
      "Validated batch 182 batch loss 1.28288651\n",
      "Validated batch 183 batch loss 1.22309577\n",
      "Validated batch 184 batch loss 1.12318504\n",
      "Validated batch 185 batch loss 1.33416486\n",
      "Epoch 4 val loss 1.2276856899261475\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-4-loss-1.2277.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.17782402 epoch total loss 1.17782402\n",
      "Trained batch 2 batch loss 1.25722408 epoch total loss 1.21752405\n",
      "Trained batch 3 batch loss 1.18840766 epoch total loss 1.20781863\n",
      "Trained batch 4 batch loss 1.28441 epoch total loss 1.22696638\n",
      "Trained batch 5 batch loss 1.29971659 epoch total loss 1.24151635\n",
      "Trained batch 6 batch loss 1.21328568 epoch total loss 1.23681128\n",
      "Trained batch 7 batch loss 1.00928771 epoch total loss 1.20430791\n",
      "Trained batch 8 batch loss 1.09917235 epoch total loss 1.19116604\n",
      "Trained batch 9 batch loss 1.08039558 epoch total loss 1.17885828\n",
      "Trained batch 10 batch loss 1.03579628 epoch total loss 1.16455197\n",
      "Trained batch 11 batch loss 1.1687727 epoch total loss 1.16493571\n",
      "Trained batch 12 batch loss 1.09592021 epoch total loss 1.15918446\n",
      "Trained batch 13 batch loss 1.16160488 epoch total loss 1.15937066\n",
      "Trained batch 14 batch loss 1.13608491 epoch total loss 1.15770733\n",
      "Trained batch 15 batch loss 1.1495316 epoch total loss 1.15716231\n",
      "Trained batch 16 batch loss 1.04263866 epoch total loss 1.15000463\n",
      "Trained batch 17 batch loss 1.08233809 epoch total loss 1.14602423\n",
      "Trained batch 18 batch loss 1.21119606 epoch total loss 1.14964497\n",
      "Trained batch 19 batch loss 1.20206964 epoch total loss 1.15240419\n",
      "Trained batch 20 batch loss 1.2749666 epoch total loss 1.15853238\n",
      "Trained batch 21 batch loss 1.18488383 epoch total loss 1.15978718\n",
      "Trained batch 22 batch loss 1.15104067 epoch total loss 1.15938962\n",
      "Trained batch 23 batch loss 1.14219451 epoch total loss 1.15864193\n",
      "Trained batch 24 batch loss 1.25152886 epoch total loss 1.1625123\n",
      "Trained batch 25 batch loss 1.1092031 epoch total loss 1.16037989\n",
      "Trained batch 26 batch loss 1.08695579 epoch total loss 1.15755594\n",
      "Trained batch 27 batch loss 1.15063417 epoch total loss 1.15729964\n",
      "Trained batch 28 batch loss 1.21818805 epoch total loss 1.15947425\n",
      "Trained batch 29 batch loss 1.20398974 epoch total loss 1.16100931\n",
      "Trained batch 30 batch loss 1.24115014 epoch total loss 1.16368067\n",
      "Trained batch 31 batch loss 1.24759781 epoch total loss 1.16638768\n",
      "Trained batch 32 batch loss 1.15482295 epoch total loss 1.16602623\n",
      "Trained batch 33 batch loss 1.14719152 epoch total loss 1.16545546\n",
      "Trained batch 34 batch loss 1.27561164 epoch total loss 1.16869533\n",
      "Trained batch 35 batch loss 1.2131424 epoch total loss 1.16996527\n",
      "Trained batch 36 batch loss 1.27129924 epoch total loss 1.17278\n",
      "Trained batch 37 batch loss 1.19520378 epoch total loss 1.17338598\n",
      "Trained batch 38 batch loss 1.17446351 epoch total loss 1.17341447\n",
      "Trained batch 39 batch loss 1.10667467 epoch total loss 1.1717031\n",
      "Trained batch 40 batch loss 1.16771305 epoch total loss 1.17160344\n",
      "Trained batch 41 batch loss 1.20341837 epoch total loss 1.17237937\n",
      "Trained batch 42 batch loss 1.18921745 epoch total loss 1.17278028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 43 batch loss 1.16339254 epoch total loss 1.17256188\n",
      "Trained batch 44 batch loss 1.1344862 epoch total loss 1.17169654\n",
      "Trained batch 45 batch loss 1.29223585 epoch total loss 1.1743753\n",
      "Trained batch 46 batch loss 1.09868968 epoch total loss 1.17272985\n",
      "Trained batch 47 batch loss 1.24734557 epoch total loss 1.17431748\n",
      "Trained batch 48 batch loss 1.109249 epoch total loss 1.17296183\n",
      "Trained batch 49 batch loss 1.06935585 epoch total loss 1.17084742\n",
      "Trained batch 50 batch loss 1.0113517 epoch total loss 1.16765749\n",
      "Trained batch 51 batch loss 1.18190551 epoch total loss 1.16793692\n",
      "Trained batch 52 batch loss 1.19201112 epoch total loss 1.16839993\n",
      "Trained batch 53 batch loss 1.27766037 epoch total loss 1.17046142\n",
      "Trained batch 54 batch loss 1.31068695 epoch total loss 1.17305815\n",
      "Trained batch 55 batch loss 1.26985598 epoch total loss 1.17481816\n",
      "Trained batch 56 batch loss 1.26077747 epoch total loss 1.17635322\n",
      "Trained batch 57 batch loss 1.22478056 epoch total loss 1.1772027\n",
      "Trained batch 58 batch loss 1.23057508 epoch total loss 1.178123\n",
      "Trained batch 59 batch loss 1.21220231 epoch total loss 1.17870057\n",
      "Trained batch 60 batch loss 1.25098538 epoch total loss 1.1799053\n",
      "Trained batch 61 batch loss 1.23436725 epoch total loss 1.18079817\n",
      "Trained batch 62 batch loss 1.37015676 epoch total loss 1.18385231\n",
      "Trained batch 63 batch loss 1.25789154 epoch total loss 1.18502748\n",
      "Trained batch 64 batch loss 1.15740585 epoch total loss 1.18459582\n",
      "Trained batch 65 batch loss 1.16648674 epoch total loss 1.18431723\n",
      "Trained batch 66 batch loss 1.19770813 epoch total loss 1.18452013\n",
      "Trained batch 67 batch loss 1.17636824 epoch total loss 1.18439853\n",
      "Trained batch 68 batch loss 1.22444046 epoch total loss 1.18498731\n",
      "Trained batch 69 batch loss 1.25879788 epoch total loss 1.18605709\n",
      "Trained batch 70 batch loss 1.14717364 epoch total loss 1.18550158\n",
      "Trained batch 71 batch loss 1.25741303 epoch total loss 1.18651438\n",
      "Trained batch 72 batch loss 1.14149582 epoch total loss 1.18588912\n",
      "Trained batch 73 batch loss 1.11647046 epoch total loss 1.18493819\n",
      "Trained batch 74 batch loss 1.19409227 epoch total loss 1.18506193\n",
      "Trained batch 75 batch loss 1.09223294 epoch total loss 1.18382418\n",
      "Trained batch 76 batch loss 1.01802158 epoch total loss 1.18164253\n",
      "Trained batch 77 batch loss 1.06248236 epoch total loss 1.18009508\n",
      "Trained batch 78 batch loss 1.16552305 epoch total loss 1.17990816\n",
      "Trained batch 79 batch loss 1.10877526 epoch total loss 1.17900777\n",
      "Trained batch 80 batch loss 1.12407255 epoch total loss 1.178321\n",
      "Trained batch 81 batch loss 1.26110041 epoch total loss 1.17934299\n",
      "Trained batch 82 batch loss 1.26081967 epoch total loss 1.18033659\n",
      "Trained batch 83 batch loss 1.22193646 epoch total loss 1.18083775\n",
      "Trained batch 84 batch loss 1.16190994 epoch total loss 1.18061244\n",
      "Trained batch 85 batch loss 1.24316525 epoch total loss 1.18134832\n",
      "Trained batch 86 batch loss 1.39484525 epoch total loss 1.18383086\n",
      "Trained batch 87 batch loss 1.28759897 epoch total loss 1.18502355\n",
      "Trained batch 88 batch loss 1.23573971 epoch total loss 1.18559992\n",
      "Trained batch 89 batch loss 1.25636482 epoch total loss 1.18639505\n",
      "Trained batch 90 batch loss 1.13149965 epoch total loss 1.18578506\n",
      "Trained batch 91 batch loss 1.13885951 epoch total loss 1.18526947\n",
      "Trained batch 92 batch loss 1.29475176 epoch total loss 1.18645954\n",
      "Trained batch 93 batch loss 1.27894974 epoch total loss 1.1874541\n",
      "Trained batch 94 batch loss 1.26433969 epoch total loss 1.188272\n",
      "Trained batch 95 batch loss 1.09383512 epoch total loss 1.18727791\n",
      "Trained batch 96 batch loss 1.25435376 epoch total loss 1.18797672\n",
      "Trained batch 97 batch loss 1.16377056 epoch total loss 1.18772709\n",
      "Trained batch 98 batch loss 1.2191143 epoch total loss 1.18804741\n",
      "Trained batch 99 batch loss 1.18133688 epoch total loss 1.1879797\n",
      "Trained batch 100 batch loss 1.20745873 epoch total loss 1.18817449\n",
      "Trained batch 101 batch loss 1.18821549 epoch total loss 1.18817484\n",
      "Trained batch 102 batch loss 1.218683 epoch total loss 1.18847394\n",
      "Trained batch 103 batch loss 1.23461723 epoch total loss 1.18892193\n",
      "Trained batch 104 batch loss 1.28271306 epoch total loss 1.18982387\n",
      "Trained batch 105 batch loss 1.28986132 epoch total loss 1.19077659\n",
      "Trained batch 106 batch loss 1.3087399 epoch total loss 1.19188941\n",
      "Trained batch 107 batch loss 1.23929238 epoch total loss 1.19233251\n",
      "Trained batch 108 batch loss 1.11589873 epoch total loss 1.19162476\n",
      "Trained batch 109 batch loss 1.00621307 epoch total loss 1.18992376\n",
      "Trained batch 110 batch loss 1.03984535 epoch total loss 1.18855941\n",
      "Trained batch 111 batch loss 1.12526965 epoch total loss 1.18798923\n",
      "Trained batch 112 batch loss 1.2817744 epoch total loss 1.18882656\n",
      "Trained batch 113 batch loss 1.26380038 epoch total loss 1.18949\n",
      "Trained batch 114 batch loss 1.2419858 epoch total loss 1.18995047\n",
      "Trained batch 115 batch loss 1.30258012 epoch total loss 1.19092989\n",
      "Trained batch 116 batch loss 1.27988052 epoch total loss 1.19169664\n",
      "Trained batch 117 batch loss 1.28008401 epoch total loss 1.19245219\n",
      "Trained batch 118 batch loss 1.23662198 epoch total loss 1.19282651\n",
      "Trained batch 119 batch loss 1.35549927 epoch total loss 1.19419348\n",
      "Trained batch 120 batch loss 1.2211138 epoch total loss 1.19441783\n",
      "Trained batch 121 batch loss 1.13772023 epoch total loss 1.19394934\n",
      "Trained batch 122 batch loss 1.14627171 epoch total loss 1.19355845\n",
      "Trained batch 123 batch loss 1.05644524 epoch total loss 1.19244373\n",
      "Trained batch 124 batch loss 1.0494169 epoch total loss 1.19129038\n",
      "Trained batch 125 batch loss 1.09131014 epoch total loss 1.19049048\n",
      "Trained batch 126 batch loss 1.158916 epoch total loss 1.19023991\n",
      "Trained batch 127 batch loss 1.23446119 epoch total loss 1.19058812\n",
      "Trained batch 128 batch loss 1.26081038 epoch total loss 1.19113672\n",
      "Trained batch 129 batch loss 1.41626966 epoch total loss 1.19288194\n",
      "Trained batch 130 batch loss 1.14411688 epoch total loss 1.19250691\n",
      "Trained batch 131 batch loss 1.31643963 epoch total loss 1.19345295\n",
      "Trained batch 132 batch loss 1.18809664 epoch total loss 1.1934123\n",
      "Trained batch 133 batch loss 1.19174993 epoch total loss 1.19339991\n",
      "Trained batch 134 batch loss 1.20334411 epoch total loss 1.19347405\n",
      "Trained batch 135 batch loss 1.39118958 epoch total loss 1.19493866\n",
      "Trained batch 136 batch loss 1.2732265 epoch total loss 1.1955142\n",
      "Trained batch 137 batch loss 1.05779874 epoch total loss 1.19450903\n",
      "Trained batch 138 batch loss 1.36556685 epoch total loss 1.19574857\n",
      "Trained batch 139 batch loss 1.34316492 epoch total loss 1.19680917\n",
      "Trained batch 140 batch loss 1.23710012 epoch total loss 1.19709706\n",
      "Trained batch 141 batch loss 1.20326698 epoch total loss 1.19714069\n",
      "Trained batch 142 batch loss 1.16146231 epoch total loss 1.19688952\n",
      "Trained batch 143 batch loss 1.13187563 epoch total loss 1.19643497\n",
      "Trained batch 144 batch loss 1.18112373 epoch total loss 1.19632864\n",
      "Trained batch 145 batch loss 1.28810847 epoch total loss 1.19696152\n",
      "Trained batch 146 batch loss 1.21646 epoch total loss 1.19709504\n",
      "Trained batch 147 batch loss 1.22999179 epoch total loss 1.19731891\n",
      "Trained batch 148 batch loss 1.06341124 epoch total loss 1.19641411\n",
      "Trained batch 149 batch loss 1.200266 epoch total loss 1.19644\n",
      "Trained batch 150 batch loss 1.18085277 epoch total loss 1.19633603\n",
      "Trained batch 151 batch loss 1.11748731 epoch total loss 1.19581389\n",
      "Trained batch 152 batch loss 1.17764318 epoch total loss 1.19569433\n",
      "Trained batch 153 batch loss 1.18865108 epoch total loss 1.19564831\n",
      "Trained batch 154 batch loss 1.30145037 epoch total loss 1.19633532\n",
      "Trained batch 155 batch loss 1.20778012 epoch total loss 1.19640911\n",
      "Trained batch 156 batch loss 1.13903022 epoch total loss 1.19604135\n",
      "Trained batch 157 batch loss 1.21839213 epoch total loss 1.19618368\n",
      "Trained batch 158 batch loss 1.1343379 epoch total loss 1.19579232\n",
      "Trained batch 159 batch loss 1.08607972 epoch total loss 1.19510221\n",
      "Trained batch 160 batch loss 1.16161168 epoch total loss 1.19489288\n",
      "Trained batch 161 batch loss 1.1561451 epoch total loss 1.1946522\n",
      "Trained batch 162 batch loss 1.18942571 epoch total loss 1.19461989\n",
      "Trained batch 163 batch loss 1.16196871 epoch total loss 1.19441962\n",
      "Trained batch 164 batch loss 1.16919351 epoch total loss 1.19426584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 165 batch loss 1.10526276 epoch total loss 1.1937263\n",
      "Trained batch 166 batch loss 1.16603494 epoch total loss 1.19355953\n",
      "Trained batch 167 batch loss 1.22582352 epoch total loss 1.19375277\n",
      "Trained batch 168 batch loss 1.12531328 epoch total loss 1.19334543\n",
      "Trained batch 169 batch loss 1.07725048 epoch total loss 1.19265842\n",
      "Trained batch 170 batch loss 1.19488788 epoch total loss 1.19267154\n",
      "Trained batch 171 batch loss 1.03813255 epoch total loss 1.19176781\n",
      "Trained batch 172 batch loss 1.17547154 epoch total loss 1.19167316\n",
      "Trained batch 173 batch loss 1.269315 epoch total loss 1.19212186\n",
      "Trained batch 174 batch loss 1.30268216 epoch total loss 1.19275737\n",
      "Trained batch 175 batch loss 1.23335052 epoch total loss 1.19298935\n",
      "Trained batch 176 batch loss 1.35009 epoch total loss 1.19388199\n",
      "Trained batch 177 batch loss 1.11553013 epoch total loss 1.19343925\n",
      "Trained batch 178 batch loss 1.10102355 epoch total loss 1.19292009\n",
      "Trained batch 179 batch loss 1.11382985 epoch total loss 1.1924783\n",
      "Trained batch 180 batch loss 1.14772725 epoch total loss 1.19222963\n",
      "Trained batch 181 batch loss 1.18594694 epoch total loss 1.19219494\n",
      "Trained batch 182 batch loss 1.14945722 epoch total loss 1.1919601\n",
      "Trained batch 183 batch loss 1.18160522 epoch total loss 1.19190359\n",
      "Trained batch 184 batch loss 1.17265487 epoch total loss 1.19179893\n",
      "Trained batch 185 batch loss 1.16346085 epoch total loss 1.19164574\n",
      "Trained batch 186 batch loss 1.11784101 epoch total loss 1.19124901\n",
      "Trained batch 187 batch loss 1.16060519 epoch total loss 1.1910851\n",
      "Trained batch 188 batch loss 1.09818041 epoch total loss 1.19059086\n",
      "Trained batch 189 batch loss 1.14833069 epoch total loss 1.19036722\n",
      "Trained batch 190 batch loss 1.20080674 epoch total loss 1.19042218\n",
      "Trained batch 191 batch loss 1.16137683 epoch total loss 1.19027019\n",
      "Trained batch 192 batch loss 1.1545049 epoch total loss 1.19008386\n",
      "Trained batch 193 batch loss 1.17239237 epoch total loss 1.18999219\n",
      "Trained batch 194 batch loss 1.11334658 epoch total loss 1.18959713\n",
      "Trained batch 195 batch loss 0.993574858 epoch total loss 1.18859196\n",
      "Trained batch 196 batch loss 0.99988389 epoch total loss 1.1876291\n",
      "Trained batch 197 batch loss 1.08394527 epoch total loss 1.18710268\n",
      "Trained batch 198 batch loss 1.17244613 epoch total loss 1.18702865\n",
      "Trained batch 199 batch loss 1.09858632 epoch total loss 1.18658423\n",
      "Trained batch 200 batch loss 1.15175939 epoch total loss 1.18641019\n",
      "Trained batch 201 batch loss 1.21464181 epoch total loss 1.18655062\n",
      "Trained batch 202 batch loss 1.14036715 epoch total loss 1.18632197\n",
      "Trained batch 203 batch loss 1.27846777 epoch total loss 1.18677592\n",
      "Trained batch 204 batch loss 1.31208611 epoch total loss 1.18739021\n",
      "Trained batch 205 batch loss 1.13598871 epoch total loss 1.18713939\n",
      "Trained batch 206 batch loss 1.26793015 epoch total loss 1.18753159\n",
      "Trained batch 207 batch loss 1.20298517 epoch total loss 1.18760633\n",
      "Trained batch 208 batch loss 1.06038499 epoch total loss 1.18699467\n",
      "Trained batch 209 batch loss 1.14392698 epoch total loss 1.18678856\n",
      "Trained batch 210 batch loss 1.29524422 epoch total loss 1.18730497\n",
      "Trained batch 211 batch loss 1.1550225 epoch total loss 1.18715203\n",
      "Trained batch 212 batch loss 1.27470529 epoch total loss 1.18756497\n",
      "Trained batch 213 batch loss 1.26661968 epoch total loss 1.18793619\n",
      "Trained batch 214 batch loss 1.26190746 epoch total loss 1.18828177\n",
      "Trained batch 215 batch loss 1.10143495 epoch total loss 1.18787789\n",
      "Trained batch 216 batch loss 1.22381842 epoch total loss 1.18804419\n",
      "Trained batch 217 batch loss 1.15061593 epoch total loss 1.18787169\n",
      "Trained batch 218 batch loss 1.17394948 epoch total loss 1.1878078\n",
      "Trained batch 219 batch loss 1.15214288 epoch total loss 1.18764496\n",
      "Trained batch 220 batch loss 1.16854846 epoch total loss 1.18755817\n",
      "Trained batch 221 batch loss 1.09538305 epoch total loss 1.18714106\n",
      "Trained batch 222 batch loss 1.11043203 epoch total loss 1.18679559\n",
      "Trained batch 223 batch loss 1.29330277 epoch total loss 1.18727326\n",
      "Trained batch 224 batch loss 1.08511615 epoch total loss 1.18681717\n",
      "Trained batch 225 batch loss 1.04964328 epoch total loss 1.18620753\n",
      "Trained batch 226 batch loss 1.02265453 epoch total loss 1.18548381\n",
      "Trained batch 227 batch loss 0.85250771 epoch total loss 1.18401694\n",
      "Trained batch 228 batch loss 1.09190845 epoch total loss 1.18361306\n",
      "Trained batch 229 batch loss 1.20921636 epoch total loss 1.18372488\n",
      "Trained batch 230 batch loss 1.21436024 epoch total loss 1.18385804\n",
      "Trained batch 231 batch loss 1.17463541 epoch total loss 1.1838181\n",
      "Trained batch 232 batch loss 1.14257181 epoch total loss 1.18364036\n",
      "Trained batch 233 batch loss 1.13749707 epoch total loss 1.18344235\n",
      "Trained batch 234 batch loss 1.04386938 epoch total loss 1.18284595\n",
      "Trained batch 235 batch loss 1.13825917 epoch total loss 1.18265617\n",
      "Trained batch 236 batch loss 1.08016515 epoch total loss 1.18222189\n",
      "Trained batch 237 batch loss 1.23273778 epoch total loss 1.18243504\n",
      "Trained batch 238 batch loss 1.22176445 epoch total loss 1.18260026\n",
      "Trained batch 239 batch loss 1.33212614 epoch total loss 1.18322587\n",
      "Trained batch 240 batch loss 1.22662163 epoch total loss 1.18340671\n",
      "Trained batch 241 batch loss 1.14636052 epoch total loss 1.18325305\n",
      "Trained batch 242 batch loss 1.26065183 epoch total loss 1.18357277\n",
      "Trained batch 243 batch loss 1.16527331 epoch total loss 1.18349755\n",
      "Trained batch 244 batch loss 1.23529887 epoch total loss 1.18370986\n",
      "Trained batch 245 batch loss 1.19567966 epoch total loss 1.18375862\n",
      "Trained batch 246 batch loss 1.10911179 epoch total loss 1.18345523\n",
      "Trained batch 247 batch loss 1.11884642 epoch total loss 1.18319356\n",
      "Trained batch 248 batch loss 1.14739311 epoch total loss 1.1830492\n",
      "Trained batch 249 batch loss 1.20928049 epoch total loss 1.18315458\n",
      "Trained batch 250 batch loss 1.25097442 epoch total loss 1.1834259\n",
      "Trained batch 251 batch loss 1.25606191 epoch total loss 1.18371534\n",
      "Trained batch 252 batch loss 1.217731 epoch total loss 1.18385041\n",
      "Trained batch 253 batch loss 1.18244839 epoch total loss 1.1838448\n",
      "Trained batch 254 batch loss 1.22841728 epoch total loss 1.18402028\n",
      "Trained batch 255 batch loss 1.17804563 epoch total loss 1.1839968\n",
      "Trained batch 256 batch loss 1.08169436 epoch total loss 1.18359721\n",
      "Trained batch 257 batch loss 1.1587286 epoch total loss 1.18350041\n",
      "Trained batch 258 batch loss 1.26284456 epoch total loss 1.18380797\n",
      "Trained batch 259 batch loss 1.27381253 epoch total loss 1.18415546\n",
      "Trained batch 260 batch loss 1.32597744 epoch total loss 1.18470097\n",
      "Trained batch 261 batch loss 1.34911823 epoch total loss 1.18533087\n",
      "Trained batch 262 batch loss 1.436517 epoch total loss 1.18628967\n",
      "Trained batch 263 batch loss 1.35249281 epoch total loss 1.1869216\n",
      "Trained batch 264 batch loss 1.14147615 epoch total loss 1.18674946\n",
      "Trained batch 265 batch loss 1.12960219 epoch total loss 1.18653381\n",
      "Trained batch 266 batch loss 1.10861015 epoch total loss 1.18624091\n",
      "Trained batch 267 batch loss 1.25374031 epoch total loss 1.18649375\n",
      "Trained batch 268 batch loss 1.17845929 epoch total loss 1.18646371\n",
      "Trained batch 269 batch loss 1.28971684 epoch total loss 1.18684757\n",
      "Trained batch 270 batch loss 1.35676968 epoch total loss 1.18747699\n",
      "Trained batch 271 batch loss 1.30037451 epoch total loss 1.18789363\n",
      "Trained batch 272 batch loss 1.3774507 epoch total loss 1.18859041\n",
      "Trained batch 273 batch loss 1.23380136 epoch total loss 1.18875599\n",
      "Trained batch 274 batch loss 1.18616509 epoch total loss 1.18874657\n",
      "Trained batch 275 batch loss 1.15562439 epoch total loss 1.18862617\n",
      "Trained batch 276 batch loss 1.2554388 epoch total loss 1.18886817\n",
      "Trained batch 277 batch loss 1.23622417 epoch total loss 1.18903923\n",
      "Trained batch 278 batch loss 1.16706538 epoch total loss 1.18896008\n",
      "Trained batch 279 batch loss 1.15775371 epoch total loss 1.18884826\n",
      "Trained batch 280 batch loss 1.15529227 epoch total loss 1.18872845\n",
      "Trained batch 281 batch loss 1.11772144 epoch total loss 1.18847573\n",
      "Trained batch 282 batch loss 1.13094759 epoch total loss 1.18827176\n",
      "Trained batch 283 batch loss 1.0562886 epoch total loss 1.18780529\n",
      "Trained batch 284 batch loss 1.04815817 epoch total loss 1.18731356\n",
      "Trained batch 285 batch loss 1.04702163 epoch total loss 1.18682134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 286 batch loss 1.17287922 epoch total loss 1.18677258\n",
      "Trained batch 287 batch loss 1.17773724 epoch total loss 1.18674111\n",
      "Trained batch 288 batch loss 1.21311343 epoch total loss 1.18683267\n",
      "Trained batch 289 batch loss 1.33275557 epoch total loss 1.18733764\n",
      "Trained batch 290 batch loss 1.23055542 epoch total loss 1.18748665\n",
      "Trained batch 291 batch loss 1.22831297 epoch total loss 1.18762684\n",
      "Trained batch 292 batch loss 1.23210084 epoch total loss 1.18777919\n",
      "Trained batch 293 batch loss 1.24922633 epoch total loss 1.18798888\n",
      "Trained batch 294 batch loss 1.11729848 epoch total loss 1.18774855\n",
      "Trained batch 295 batch loss 1.23970103 epoch total loss 1.18792462\n",
      "Trained batch 296 batch loss 1.30161643 epoch total loss 1.18830872\n",
      "Trained batch 297 batch loss 1.30891323 epoch total loss 1.18871474\n",
      "Trained batch 298 batch loss 1.1386807 epoch total loss 1.18854678\n",
      "Trained batch 299 batch loss 1.35330462 epoch total loss 1.18909788\n",
      "Trained batch 300 batch loss 1.20497739 epoch total loss 1.18915081\n",
      "Trained batch 301 batch loss 1.43816733 epoch total loss 1.18997812\n",
      "Trained batch 302 batch loss 1.23911667 epoch total loss 1.19014072\n",
      "Trained batch 303 batch loss 1.21130753 epoch total loss 1.19021058\n",
      "Trained batch 304 batch loss 1.25199568 epoch total loss 1.19041383\n",
      "Trained batch 305 batch loss 1.31546736 epoch total loss 1.19082379\n",
      "Trained batch 306 batch loss 1.18643188 epoch total loss 1.19080949\n",
      "Trained batch 307 batch loss 1.23369837 epoch total loss 1.1909492\n",
      "Trained batch 308 batch loss 1.21127892 epoch total loss 1.19101512\n",
      "Trained batch 309 batch loss 1.17948699 epoch total loss 1.19097781\n",
      "Trained batch 310 batch loss 1.20942008 epoch total loss 1.1910373\n",
      "Trained batch 311 batch loss 1.01679122 epoch total loss 1.19047701\n",
      "Trained batch 312 batch loss 0.976652741 epoch total loss 1.18979168\n",
      "Trained batch 313 batch loss 1.01470149 epoch total loss 1.18923223\n",
      "Trained batch 314 batch loss 1.12535441 epoch total loss 1.18902886\n",
      "Trained batch 315 batch loss 1.11997902 epoch total loss 1.18880963\n",
      "Trained batch 316 batch loss 1.07578659 epoch total loss 1.18845189\n",
      "Trained batch 317 batch loss 1.0583024 epoch total loss 1.18804133\n",
      "Trained batch 318 batch loss 1.21186066 epoch total loss 1.18811619\n",
      "Trained batch 319 batch loss 1.25006843 epoch total loss 1.18831038\n",
      "Trained batch 320 batch loss 1.13348055 epoch total loss 1.18813908\n",
      "Trained batch 321 batch loss 1.06384861 epoch total loss 1.18775189\n",
      "Trained batch 322 batch loss 1.18387008 epoch total loss 1.18773973\n",
      "Trained batch 323 batch loss 1.26781631 epoch total loss 1.18798769\n",
      "Trained batch 324 batch loss 1.20752907 epoch total loss 1.18804801\n",
      "Trained batch 325 batch loss 1.13737583 epoch total loss 1.18789208\n",
      "Trained batch 326 batch loss 1.26144135 epoch total loss 1.18811774\n",
      "Trained batch 327 batch loss 1.2110846 epoch total loss 1.18818796\n",
      "Trained batch 328 batch loss 1.24982297 epoch total loss 1.18837595\n",
      "Trained batch 329 batch loss 1.13596714 epoch total loss 1.18821657\n",
      "Trained batch 330 batch loss 1.46701932 epoch total loss 1.1890614\n",
      "Trained batch 331 batch loss 1.30405915 epoch total loss 1.18940878\n",
      "Trained batch 332 batch loss 1.35932708 epoch total loss 1.18992054\n",
      "Trained batch 333 batch loss 1.24111509 epoch total loss 1.19007432\n",
      "Trained batch 334 batch loss 1.12166715 epoch total loss 1.18986952\n",
      "Trained batch 335 batch loss 0.950214565 epoch total loss 1.18915415\n",
      "Trained batch 336 batch loss 1.093503 epoch total loss 1.18886948\n",
      "Trained batch 337 batch loss 1.16557193 epoch total loss 1.18880033\n",
      "Trained batch 338 batch loss 0.959335685 epoch total loss 1.18812144\n",
      "Trained batch 339 batch loss 0.966392398 epoch total loss 1.18746746\n",
      "Trained batch 340 batch loss 0.944781899 epoch total loss 1.18675363\n",
      "Trained batch 341 batch loss 0.966033161 epoch total loss 1.18610644\n",
      "Trained batch 342 batch loss 1.1275475 epoch total loss 1.18593514\n",
      "Trained batch 343 batch loss 1.09925151 epoch total loss 1.18568242\n",
      "Trained batch 344 batch loss 1.13057113 epoch total loss 1.1855222\n",
      "Trained batch 345 batch loss 1.20098805 epoch total loss 1.18556702\n",
      "Trained batch 346 batch loss 1.28793073 epoch total loss 1.1858629\n",
      "Trained batch 347 batch loss 1.20145392 epoch total loss 1.18590784\n",
      "Trained batch 348 batch loss 1.25341868 epoch total loss 1.18610179\n",
      "Trained batch 349 batch loss 1.18035936 epoch total loss 1.18608534\n",
      "Trained batch 350 batch loss 1.23878932 epoch total loss 1.1862359\n",
      "Trained batch 351 batch loss 1.10695124 epoch total loss 1.18601012\n",
      "Trained batch 352 batch loss 1.26455331 epoch total loss 1.18623328\n",
      "Trained batch 353 batch loss 1.12235785 epoch total loss 1.18605232\n",
      "Trained batch 354 batch loss 1.15563595 epoch total loss 1.18596637\n",
      "Trained batch 355 batch loss 1.15952468 epoch total loss 1.18589187\n",
      "Trained batch 356 batch loss 1.3317945 epoch total loss 1.18630171\n",
      "Trained batch 357 batch loss 1.16879535 epoch total loss 1.18625259\n",
      "Trained batch 358 batch loss 1.25714278 epoch total loss 1.1864506\n",
      "Trained batch 359 batch loss 1.36092889 epoch total loss 1.18693662\n",
      "Trained batch 360 batch loss 1.21442449 epoch total loss 1.18701303\n",
      "Trained batch 361 batch loss 1.24674833 epoch total loss 1.18717837\n",
      "Trained batch 362 batch loss 1.31207633 epoch total loss 1.18752348\n",
      "Trained batch 363 batch loss 1.19614768 epoch total loss 1.18754721\n",
      "Trained batch 364 batch loss 1.24691987 epoch total loss 1.18771029\n",
      "Trained batch 365 batch loss 1.10720956 epoch total loss 1.18748975\n",
      "Trained batch 366 batch loss 1.12243819 epoch total loss 1.18731201\n",
      "Trained batch 367 batch loss 1.09434938 epoch total loss 1.18705869\n",
      "Trained batch 368 batch loss 1.05681551 epoch total loss 1.18670475\n",
      "Trained batch 369 batch loss 1.07842898 epoch total loss 1.18641138\n",
      "Trained batch 370 batch loss 1.24414372 epoch total loss 1.18656743\n",
      "Trained batch 371 batch loss 1.42272723 epoch total loss 1.187204\n",
      "Trained batch 372 batch loss 1.25309277 epoch total loss 1.18738103\n",
      "Trained batch 373 batch loss 1.40142632 epoch total loss 1.1879549\n",
      "Trained batch 374 batch loss 1.24105072 epoch total loss 1.18809688\n",
      "Trained batch 375 batch loss 1.22232008 epoch total loss 1.1881882\n",
      "Trained batch 376 batch loss 1.26104987 epoch total loss 1.18838191\n",
      "Trained batch 377 batch loss 1.26074123 epoch total loss 1.18857384\n",
      "Trained batch 378 batch loss 1.21359694 epoch total loss 1.18864\n",
      "Trained batch 379 batch loss 1.18022585 epoch total loss 1.18861783\n",
      "Trained batch 380 batch loss 1.20178187 epoch total loss 1.18865252\n",
      "Trained batch 381 batch loss 1.1692903 epoch total loss 1.18860173\n",
      "Trained batch 382 batch loss 1.21232009 epoch total loss 1.18866372\n",
      "Trained batch 383 batch loss 1.17090607 epoch total loss 1.18861735\n",
      "Trained batch 384 batch loss 1.03252649 epoch total loss 1.18821084\n",
      "Trained batch 385 batch loss 0.969597936 epoch total loss 1.18764305\n",
      "Trained batch 386 batch loss 1.00943315 epoch total loss 1.18718135\n",
      "Trained batch 387 batch loss 1.20722854 epoch total loss 1.18723309\n",
      "Trained batch 388 batch loss 1.27576208 epoch total loss 1.18746126\n",
      "Trained batch 389 batch loss 1.32448387 epoch total loss 1.18781352\n",
      "Trained batch 390 batch loss 1.31548643 epoch total loss 1.18814099\n",
      "Trained batch 391 batch loss 1.337129 epoch total loss 1.18852198\n",
      "Trained batch 392 batch loss 1.33952355 epoch total loss 1.18890727\n",
      "Trained batch 393 batch loss 1.36034799 epoch total loss 1.18934345\n",
      "Trained batch 394 batch loss 1.24553514 epoch total loss 1.18948615\n",
      "Trained batch 395 batch loss 1.22418618 epoch total loss 1.189574\n",
      "Trained batch 396 batch loss 1.15897417 epoch total loss 1.18949664\n",
      "Trained batch 397 batch loss 1.23053622 epoch total loss 1.1896\n",
      "Trained batch 398 batch loss 1.18047118 epoch total loss 1.1895771\n",
      "Trained batch 399 batch loss 1.15428317 epoch total loss 1.18948865\n",
      "Trained batch 400 batch loss 1.23067927 epoch total loss 1.18959165\n",
      "Trained batch 401 batch loss 1.28044271 epoch total loss 1.18981826\n",
      "Trained batch 402 batch loss 1.15064299 epoch total loss 1.18972075\n",
      "Trained batch 403 batch loss 1.22024572 epoch total loss 1.18979657\n",
      "Trained batch 404 batch loss 1.22956753 epoch total loss 1.18989491\n",
      "Trained batch 405 batch loss 1.01788962 epoch total loss 1.18947017\n",
      "Trained batch 406 batch loss 1.15276682 epoch total loss 1.18937981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 407 batch loss 1.09687817 epoch total loss 1.1891526\n",
      "Trained batch 408 batch loss 1.03283632 epoch total loss 1.18876946\n",
      "Trained batch 409 batch loss 1.14798 epoch total loss 1.18866968\n",
      "Trained batch 410 batch loss 1.06619763 epoch total loss 1.18837106\n",
      "Trained batch 411 batch loss 1.19078922 epoch total loss 1.1883769\n",
      "Trained batch 412 batch loss 1.17045712 epoch total loss 1.18833351\n",
      "Trained batch 413 batch loss 1.15972531 epoch total loss 1.18826425\n",
      "Trained batch 414 batch loss 1.11079741 epoch total loss 1.18807709\n",
      "Trained batch 415 batch loss 1.16697836 epoch total loss 1.18802631\n",
      "Trained batch 416 batch loss 1.0525347 epoch total loss 1.18770051\n",
      "Trained batch 417 batch loss 1.05810058 epoch total loss 1.18738973\n",
      "Trained batch 418 batch loss 0.94766742 epoch total loss 1.18681622\n",
      "Trained batch 419 batch loss 1.08665729 epoch total loss 1.18657732\n",
      "Trained batch 420 batch loss 1.07987404 epoch total loss 1.18632317\n",
      "Trained batch 421 batch loss 1.02728271 epoch total loss 1.18594539\n",
      "Trained batch 422 batch loss 1.05201662 epoch total loss 1.18562806\n",
      "Trained batch 423 batch loss 1.06442916 epoch total loss 1.18534148\n",
      "Trained batch 424 batch loss 1.04674482 epoch total loss 1.18501461\n",
      "Trained batch 425 batch loss 1.16788816 epoch total loss 1.18497431\n",
      "Trained batch 426 batch loss 1.34258974 epoch total loss 1.18534434\n",
      "Trained batch 427 batch loss 1.37751174 epoch total loss 1.18579435\n",
      "Trained batch 428 batch loss 1.18083119 epoch total loss 1.18578267\n",
      "Trained batch 429 batch loss 1.23403955 epoch total loss 1.1858952\n",
      "Trained batch 430 batch loss 1.42934501 epoch total loss 1.18646133\n",
      "Trained batch 431 batch loss 1.24635458 epoch total loss 1.18660033\n",
      "Trained batch 432 batch loss 1.24529922 epoch total loss 1.18673623\n",
      "Trained batch 433 batch loss 1.03505516 epoch total loss 1.18638587\n",
      "Trained batch 434 batch loss 1.1159569 epoch total loss 1.18622363\n",
      "Trained batch 435 batch loss 1.14072359 epoch total loss 1.18611908\n",
      "Trained batch 436 batch loss 1.06405497 epoch total loss 1.18583906\n",
      "Trained batch 437 batch loss 1.06885481 epoch total loss 1.18557131\n",
      "Trained batch 438 batch loss 1.16038132 epoch total loss 1.18551385\n",
      "Trained batch 439 batch loss 1.23855019 epoch total loss 1.18563461\n",
      "Trained batch 440 batch loss 1.24807417 epoch total loss 1.18577647\n",
      "Trained batch 441 batch loss 1.07745302 epoch total loss 1.18553078\n",
      "Trained batch 442 batch loss 1.18009627 epoch total loss 1.18551862\n",
      "Trained batch 443 batch loss 1.08858705 epoch total loss 1.18529975\n",
      "Trained batch 444 batch loss 1.12806559 epoch total loss 1.18517077\n",
      "Trained batch 445 batch loss 1.14958787 epoch total loss 1.18509078\n",
      "Trained batch 446 batch loss 1.2103076 epoch total loss 1.1851474\n",
      "Trained batch 447 batch loss 1.27242923 epoch total loss 1.18534255\n",
      "Trained batch 448 batch loss 1.20512843 epoch total loss 1.18538678\n",
      "Trained batch 449 batch loss 1.22485471 epoch total loss 1.18547463\n",
      "Trained batch 450 batch loss 1.16540599 epoch total loss 1.18543\n",
      "Trained batch 451 batch loss 1.20675135 epoch total loss 1.18547726\n",
      "Trained batch 452 batch loss 1.14102817 epoch total loss 1.18537903\n",
      "Trained batch 453 batch loss 1.20071721 epoch total loss 1.185413\n",
      "Trained batch 454 batch loss 1.30148447 epoch total loss 1.18566871\n",
      "Trained batch 455 batch loss 1.15659046 epoch total loss 1.18560481\n",
      "Trained batch 456 batch loss 1.34989476 epoch total loss 1.18596518\n",
      "Trained batch 457 batch loss 1.30394459 epoch total loss 1.18622339\n",
      "Trained batch 458 batch loss 1.19026721 epoch total loss 1.18623209\n",
      "Trained batch 459 batch loss 1.24222541 epoch total loss 1.18635416\n",
      "Trained batch 460 batch loss 1.19106865 epoch total loss 1.18636429\n",
      "Trained batch 461 batch loss 1.20582473 epoch total loss 1.18640649\n",
      "Trained batch 462 batch loss 1.25645363 epoch total loss 1.18655813\n",
      "Trained batch 463 batch loss 1.27085757 epoch total loss 1.18674028\n",
      "Trained batch 464 batch loss 1.26242912 epoch total loss 1.18690348\n",
      "Trained batch 465 batch loss 1.31785703 epoch total loss 1.18718517\n",
      "Trained batch 466 batch loss 1.35289121 epoch total loss 1.18754077\n",
      "Trained batch 467 batch loss 1.33308661 epoch total loss 1.18785238\n",
      "Trained batch 468 batch loss 1.22713768 epoch total loss 1.18793619\n",
      "Trained batch 469 batch loss 1.13265944 epoch total loss 1.18781829\n",
      "Trained batch 470 batch loss 1.16257191 epoch total loss 1.18776464\n",
      "Trained batch 471 batch loss 1.17720616 epoch total loss 1.18774223\n",
      "Trained batch 472 batch loss 1.20737708 epoch total loss 1.18778384\n",
      "Trained batch 473 batch loss 1.21099222 epoch total loss 1.18783295\n",
      "Trained batch 474 batch loss 1.19443965 epoch total loss 1.1878469\n",
      "Trained batch 475 batch loss 1.25782633 epoch total loss 1.18799424\n",
      "Trained batch 476 batch loss 1.24731088 epoch total loss 1.18811882\n",
      "Trained batch 477 batch loss 1.30167294 epoch total loss 1.18835688\n",
      "Trained batch 478 batch loss 1.32724226 epoch total loss 1.18864751\n",
      "Trained batch 479 batch loss 1.15940857 epoch total loss 1.18858647\n",
      "Trained batch 480 batch loss 1.07694507 epoch total loss 1.18835402\n",
      "Trained batch 481 batch loss 1.03754032 epoch total loss 1.18804038\n",
      "Trained batch 482 batch loss 1.17083979 epoch total loss 1.18800473\n",
      "Trained batch 483 batch loss 1.33957386 epoch total loss 1.18831861\n",
      "Trained batch 484 batch loss 1.27019429 epoch total loss 1.18848777\n",
      "Trained batch 485 batch loss 1.28492558 epoch total loss 1.18868661\n",
      "Trained batch 486 batch loss 1.14399028 epoch total loss 1.18859458\n",
      "Trained batch 487 batch loss 1.27504539 epoch total loss 1.18877208\n",
      "Trained batch 488 batch loss 1.22482634 epoch total loss 1.18884599\n",
      "Trained batch 489 batch loss 1.20564544 epoch total loss 1.18888032\n",
      "Trained batch 490 batch loss 1.20240283 epoch total loss 1.18890786\n",
      "Trained batch 491 batch loss 1.19292164 epoch total loss 1.18891609\n",
      "Trained batch 492 batch loss 1.21746027 epoch total loss 1.18897414\n",
      "Trained batch 493 batch loss 1.15560222 epoch total loss 1.18890643\n",
      "Trained batch 494 batch loss 1.21563029 epoch total loss 1.18896055\n",
      "Trained batch 495 batch loss 1.19334757 epoch total loss 1.18896937\n",
      "Trained batch 496 batch loss 1.20757508 epoch total loss 1.18900692\n",
      "Trained batch 497 batch loss 1.25320554 epoch total loss 1.18913615\n",
      "Trained batch 498 batch loss 1.31070113 epoch total loss 1.18938029\n",
      "Trained batch 499 batch loss 1.15919328 epoch total loss 1.18931985\n",
      "Trained batch 500 batch loss 1.19117188 epoch total loss 1.18932354\n",
      "Trained batch 501 batch loss 1.07776511 epoch total loss 1.18910086\n",
      "Trained batch 502 batch loss 1.01874757 epoch total loss 1.18876147\n",
      "Trained batch 503 batch loss 1.02980423 epoch total loss 1.18844533\n",
      "Trained batch 504 batch loss 1.22752643 epoch total loss 1.18852293\n",
      "Trained batch 505 batch loss 1.16412961 epoch total loss 1.18847466\n",
      "Trained batch 506 batch loss 1.22209764 epoch total loss 1.18854105\n",
      "Trained batch 507 batch loss 1.1459589 epoch total loss 1.18845701\n",
      "Trained batch 508 batch loss 1.33563137 epoch total loss 1.18874681\n",
      "Trained batch 509 batch loss 1.1885972 epoch total loss 1.18874645\n",
      "Trained batch 510 batch loss 1.18419755 epoch total loss 1.18873763\n",
      "Trained batch 511 batch loss 1.18222868 epoch total loss 1.18872488\n",
      "Trained batch 512 batch loss 1.17810547 epoch total loss 1.18870413\n",
      "Trained batch 513 batch loss 1.11148036 epoch total loss 1.18855357\n",
      "Trained batch 514 batch loss 1.03958058 epoch total loss 1.18826365\n",
      "Trained batch 515 batch loss 0.848326802 epoch total loss 1.18760359\n",
      "Trained batch 516 batch loss 1.06533837 epoch total loss 1.18736672\n",
      "Trained batch 517 batch loss 1.25361252 epoch total loss 1.18749475\n",
      "Trained batch 518 batch loss 1.34868515 epoch total loss 1.18780601\n",
      "Trained batch 519 batch loss 1.36329103 epoch total loss 1.18814409\n",
      "Trained batch 520 batch loss 1.31622577 epoch total loss 1.18839037\n",
      "Trained batch 521 batch loss 1.19872439 epoch total loss 1.18841028\n",
      "Trained batch 522 batch loss 1.2580986 epoch total loss 1.1885438\n",
      "Trained batch 523 batch loss 1.21664286 epoch total loss 1.18859744\n",
      "Trained batch 524 batch loss 1.42004681 epoch total loss 1.18903911\n",
      "Trained batch 525 batch loss 1.3456099 epoch total loss 1.18933737\n",
      "Trained batch 526 batch loss 1.20567489 epoch total loss 1.18936837\n",
      "Trained batch 527 batch loss 1.28145111 epoch total loss 1.18954313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 528 batch loss 1.24277878 epoch total loss 1.18964398\n",
      "Trained batch 529 batch loss 1.22786212 epoch total loss 1.18971622\n",
      "Trained batch 530 batch loss 1.07978988 epoch total loss 1.1895088\n",
      "Trained batch 531 batch loss 1.14187801 epoch total loss 1.18941915\n",
      "Trained batch 532 batch loss 1.25354671 epoch total loss 1.18953967\n",
      "Trained batch 533 batch loss 1.30572021 epoch total loss 1.18975759\n",
      "Trained batch 534 batch loss 1.18559837 epoch total loss 1.18974984\n",
      "Trained batch 535 batch loss 1.11458027 epoch total loss 1.18960929\n",
      "Trained batch 536 batch loss 1.12763977 epoch total loss 1.18949366\n",
      "Trained batch 537 batch loss 1.06934524 epoch total loss 1.1892699\n",
      "Trained batch 538 batch loss 1.02646279 epoch total loss 1.18896735\n",
      "Trained batch 539 batch loss 1.16154742 epoch total loss 1.18891644\n",
      "Trained batch 540 batch loss 1.07096577 epoch total loss 1.18869805\n",
      "Trained batch 541 batch loss 1.16693759 epoch total loss 1.18865788\n",
      "Trained batch 542 batch loss 1.06056392 epoch total loss 1.18842149\n",
      "Trained batch 543 batch loss 1.19504738 epoch total loss 1.18843377\n",
      "Trained batch 544 batch loss 1.26369917 epoch total loss 1.18857205\n",
      "Trained batch 545 batch loss 1.18395388 epoch total loss 1.18856359\n",
      "Trained batch 546 batch loss 1.19831586 epoch total loss 1.18858147\n",
      "Trained batch 547 batch loss 1.27437162 epoch total loss 1.18873823\n",
      "Trained batch 548 batch loss 1.24843907 epoch total loss 1.18884706\n",
      "Trained batch 549 batch loss 1.24710202 epoch total loss 1.18895328\n",
      "Trained batch 550 batch loss 1.20843267 epoch total loss 1.18898869\n",
      "Trained batch 551 batch loss 1.38859499 epoch total loss 1.18935096\n",
      "Trained batch 552 batch loss 1.30236733 epoch total loss 1.18955576\n",
      "Trained batch 553 batch loss 1.13737965 epoch total loss 1.18946135\n",
      "Trained batch 554 batch loss 1.22921419 epoch total loss 1.18953311\n",
      "Trained batch 555 batch loss 1.25425172 epoch total loss 1.1896497\n",
      "Trained batch 556 batch loss 1.24096107 epoch total loss 1.18974209\n",
      "Trained batch 557 batch loss 1.22320402 epoch total loss 1.18980217\n",
      "Trained batch 558 batch loss 1.16052628 epoch total loss 1.18974972\n",
      "Trained batch 559 batch loss 1.2393899 epoch total loss 1.18983841\n",
      "Trained batch 560 batch loss 1.17207098 epoch total loss 1.1898067\n",
      "Trained batch 561 batch loss 1.22661567 epoch total loss 1.18987226\n",
      "Trained batch 562 batch loss 1.18514109 epoch total loss 1.18986392\n",
      "Trained batch 563 batch loss 1.28217351 epoch total loss 1.19002783\n",
      "Trained batch 564 batch loss 1.06763208 epoch total loss 1.18981075\n",
      "Trained batch 565 batch loss 1.14305246 epoch total loss 1.18972802\n",
      "Trained batch 566 batch loss 1.2080828 epoch total loss 1.18976045\n",
      "Trained batch 567 batch loss 1.15717268 epoch total loss 1.18970299\n",
      "Trained batch 568 batch loss 1.29286897 epoch total loss 1.18988454\n",
      "Trained batch 569 batch loss 1.40530574 epoch total loss 1.19026315\n",
      "Trained batch 570 batch loss 1.29322886 epoch total loss 1.19044387\n",
      "Trained batch 571 batch loss 1.16947579 epoch total loss 1.19040716\n",
      "Trained batch 572 batch loss 1.16366 epoch total loss 1.19036031\n",
      "Trained batch 573 batch loss 1.19873488 epoch total loss 1.19037497\n",
      "Trained batch 574 batch loss 1.08921862 epoch total loss 1.19019878\n",
      "Trained batch 575 batch loss 1.33400869 epoch total loss 1.19044876\n",
      "Trained batch 576 batch loss 1.28746247 epoch total loss 1.1906172\n",
      "Trained batch 577 batch loss 1.3781631 epoch total loss 1.19094229\n",
      "Trained batch 578 batch loss 1.35651398 epoch total loss 1.19122875\n",
      "Trained batch 579 batch loss 1.21013725 epoch total loss 1.19126141\n",
      "Trained batch 580 batch loss 1.37387621 epoch total loss 1.19157636\n",
      "Trained batch 581 batch loss 1.21106017 epoch total loss 1.19160986\n",
      "Trained batch 582 batch loss 1.20254874 epoch total loss 1.19162869\n",
      "Trained batch 583 batch loss 1.41047347 epoch total loss 1.19200408\n",
      "Trained batch 584 batch loss 1.2316165 epoch total loss 1.19207191\n",
      "Trained batch 585 batch loss 1.09154594 epoch total loss 1.1919\n",
      "Trained batch 586 batch loss 1.09286845 epoch total loss 1.1917311\n",
      "Trained batch 587 batch loss 1.04604626 epoch total loss 1.1914829\n",
      "Trained batch 588 batch loss 1.07668984 epoch total loss 1.19128764\n",
      "Trained batch 589 batch loss 1.08709908 epoch total loss 1.19111073\n",
      "Trained batch 590 batch loss 1.10435486 epoch total loss 1.19096375\n",
      "Trained batch 591 batch loss 1.23610342 epoch total loss 1.19104\n",
      "Trained batch 592 batch loss 1.24707103 epoch total loss 1.19113469\n",
      "Trained batch 593 batch loss 1.2274965 epoch total loss 1.19119596\n",
      "Trained batch 594 batch loss 1.22769046 epoch total loss 1.19125736\n",
      "Trained batch 595 batch loss 1.27402 epoch total loss 1.19139647\n",
      "Trained batch 596 batch loss 1.23556399 epoch total loss 1.1914705\n",
      "Trained batch 597 batch loss 1.11067045 epoch total loss 1.1913352\n",
      "Trained batch 598 batch loss 1.0972048 epoch total loss 1.19117785\n",
      "Trained batch 599 batch loss 1.12163401 epoch total loss 1.19106174\n",
      "Trained batch 600 batch loss 1.18445504 epoch total loss 1.19105077\n",
      "Trained batch 601 batch loss 1.2196542 epoch total loss 1.19109833\n",
      "Trained batch 602 batch loss 1.28070116 epoch total loss 1.19124722\n",
      "Trained batch 603 batch loss 1.24885893 epoch total loss 1.19134271\n",
      "Trained batch 604 batch loss 1.14355421 epoch total loss 1.19126356\n",
      "Trained batch 605 batch loss 1.15728438 epoch total loss 1.19120741\n",
      "Trained batch 606 batch loss 1.19267261 epoch total loss 1.19120979\n",
      "Trained batch 607 batch loss 1.11958742 epoch total loss 1.19109178\n",
      "Trained batch 608 batch loss 1.19820333 epoch total loss 1.19110346\n",
      "Trained batch 609 batch loss 1.15693641 epoch total loss 1.19104731\n",
      "Trained batch 610 batch loss 1.16063547 epoch total loss 1.19099748\n",
      "Trained batch 611 batch loss 1.23317051 epoch total loss 1.1910665\n",
      "Trained batch 612 batch loss 1.11976886 epoch total loss 1.19095\n",
      "Trained batch 613 batch loss 1.06493592 epoch total loss 1.1907444\n",
      "Trained batch 614 batch loss 1.1352855 epoch total loss 1.19065416\n",
      "Trained batch 615 batch loss 1.03560472 epoch total loss 1.19040203\n",
      "Trained batch 616 batch loss 1.07341683 epoch total loss 1.19021213\n",
      "Trained batch 617 batch loss 1.01641631 epoch total loss 1.18993044\n",
      "Trained batch 618 batch loss 1.01261389 epoch total loss 1.1896435\n",
      "Trained batch 619 batch loss 1.0819391 epoch total loss 1.18946946\n",
      "Trained batch 620 batch loss 1.17650557 epoch total loss 1.1894486\n",
      "Trained batch 621 batch loss 1.29644036 epoch total loss 1.18962085\n",
      "Trained batch 622 batch loss 1.08544552 epoch total loss 1.18945336\n",
      "Trained batch 623 batch loss 1.15340614 epoch total loss 1.18939555\n",
      "Trained batch 624 batch loss 1.20984817 epoch total loss 1.18942833\n",
      "Trained batch 625 batch loss 1.21334505 epoch total loss 1.18946648\n",
      "Trained batch 626 batch loss 1.00942147 epoch total loss 1.18917882\n",
      "Trained batch 627 batch loss 1.23473513 epoch total loss 1.18925154\n",
      "Trained batch 628 batch loss 1.18596387 epoch total loss 1.1892463\n",
      "Trained batch 629 batch loss 1.50390828 epoch total loss 1.18974662\n",
      "Trained batch 630 batch loss 1.23531985 epoch total loss 1.18981886\n",
      "Trained batch 631 batch loss 1.33819032 epoch total loss 1.19005394\n",
      "Trained batch 632 batch loss 1.20908535 epoch total loss 1.1900841\n",
      "Trained batch 633 batch loss 1.30570602 epoch total loss 1.19026685\n",
      "Trained batch 634 batch loss 1.28856039 epoch total loss 1.19042194\n",
      "Trained batch 635 batch loss 1.29732156 epoch total loss 1.19059026\n",
      "Trained batch 636 batch loss 1.11630106 epoch total loss 1.19047332\n",
      "Trained batch 637 batch loss 1.21031272 epoch total loss 1.19050455\n",
      "Trained batch 638 batch loss 1.10957551 epoch total loss 1.19037759\n",
      "Trained batch 639 batch loss 1.26888728 epoch total loss 1.1905005\n",
      "Trained batch 640 batch loss 1.26624894 epoch total loss 1.19061875\n",
      "Trained batch 641 batch loss 1.20476067 epoch total loss 1.19064093\n",
      "Trained batch 642 batch loss 1.29085 epoch total loss 1.19079697\n",
      "Trained batch 643 batch loss 1.23015273 epoch total loss 1.19085813\n",
      "Trained batch 644 batch loss 1.3260963 epoch total loss 1.19106817\n",
      "Trained batch 645 batch loss 1.12320185 epoch total loss 1.19096303\n",
      "Trained batch 646 batch loss 1.1246686 epoch total loss 1.19086039\n",
      "Trained batch 647 batch loss 1.07075977 epoch total loss 1.19067478\n",
      "Trained batch 648 batch loss 1.20148861 epoch total loss 1.19069147\n",
      "Trained batch 649 batch loss 1.20912325 epoch total loss 1.19071984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 650 batch loss 1.22623873 epoch total loss 1.19077444\n",
      "Trained batch 651 batch loss 1.21718335 epoch total loss 1.19081497\n",
      "Trained batch 652 batch loss 1.20407796 epoch total loss 1.19083536\n",
      "Trained batch 653 batch loss 1.16267657 epoch total loss 1.1907922\n",
      "Trained batch 654 batch loss 1.18822861 epoch total loss 1.19078839\n",
      "Trained batch 655 batch loss 1.23804259 epoch total loss 1.19086051\n",
      "Trained batch 656 batch loss 1.12343419 epoch total loss 1.19075763\n",
      "Trained batch 657 batch loss 1.1060251 epoch total loss 1.19062865\n",
      "Trained batch 658 batch loss 1.14486146 epoch total loss 1.19055903\n",
      "Trained batch 659 batch loss 1.03715777 epoch total loss 1.19032633\n",
      "Trained batch 660 batch loss 1.00102472 epoch total loss 1.19003952\n",
      "Trained batch 661 batch loss 0.97544539 epoch total loss 1.18971491\n",
      "Trained batch 662 batch loss 1.10063541 epoch total loss 1.18958032\n",
      "Trained batch 663 batch loss 1.11564505 epoch total loss 1.18946886\n",
      "Trained batch 664 batch loss 1.02152264 epoch total loss 1.18921602\n",
      "Trained batch 665 batch loss 1.04675066 epoch total loss 1.1890018\n",
      "Trained batch 666 batch loss 1.09061646 epoch total loss 1.1888541\n",
      "Trained batch 667 batch loss 1.027318 epoch total loss 1.18861187\n",
      "Trained batch 668 batch loss 1.37703562 epoch total loss 1.18889391\n",
      "Trained batch 669 batch loss 1.18079686 epoch total loss 1.18888187\n",
      "Trained batch 670 batch loss 1.26258755 epoch total loss 1.18899179\n",
      "Trained batch 671 batch loss 1.27509248 epoch total loss 1.18912017\n",
      "Trained batch 672 batch loss 1.2632947 epoch total loss 1.18923056\n",
      "Trained batch 673 batch loss 1.15885329 epoch total loss 1.18918538\n",
      "Trained batch 674 batch loss 1.1052047 epoch total loss 1.18906081\n",
      "Trained batch 675 batch loss 1.08897781 epoch total loss 1.18891263\n",
      "Trained batch 676 batch loss 1.07941759 epoch total loss 1.18875062\n",
      "Trained batch 677 batch loss 1.21986246 epoch total loss 1.18879652\n",
      "Trained batch 678 batch loss 1.22801709 epoch total loss 1.18885434\n",
      "Trained batch 679 batch loss 1.42997193 epoch total loss 1.18920958\n",
      "Trained batch 680 batch loss 1.25416076 epoch total loss 1.18930507\n",
      "Trained batch 681 batch loss 1.34632576 epoch total loss 1.18953562\n",
      "Trained batch 682 batch loss 1.20062447 epoch total loss 1.18955183\n",
      "Trained batch 683 batch loss 1.2407378 epoch total loss 1.18962681\n",
      "Trained batch 684 batch loss 1.24347532 epoch total loss 1.18970549\n",
      "Trained batch 685 batch loss 1.22316968 epoch total loss 1.18975425\n",
      "Trained batch 686 batch loss 1.23889744 epoch total loss 1.18982589\n",
      "Trained batch 687 batch loss 1.11557865 epoch total loss 1.18971789\n",
      "Trained batch 688 batch loss 1.10475886 epoch total loss 1.18959439\n",
      "Trained batch 689 batch loss 1.3048799 epoch total loss 1.18976164\n",
      "Trained batch 690 batch loss 1.12380707 epoch total loss 1.18966603\n",
      "Trained batch 691 batch loss 1.14980745 epoch total loss 1.18960834\n",
      "Trained batch 692 batch loss 1.16155267 epoch total loss 1.1895678\n",
      "Trained batch 693 batch loss 1.13777685 epoch total loss 1.18949306\n",
      "Trained batch 694 batch loss 1.09461141 epoch total loss 1.18935633\n",
      "Trained batch 695 batch loss 1.29540944 epoch total loss 1.18950891\n",
      "Trained batch 696 batch loss 1.15116251 epoch total loss 1.18945384\n",
      "Trained batch 697 batch loss 1.21967244 epoch total loss 1.18949711\n",
      "Trained batch 698 batch loss 1.23538756 epoch total loss 1.18956292\n",
      "Trained batch 699 batch loss 1.47282624 epoch total loss 1.18996823\n",
      "Trained batch 700 batch loss 1.45072937 epoch total loss 1.19034076\n",
      "Trained batch 701 batch loss 1.22891378 epoch total loss 1.19039583\n",
      "Trained batch 702 batch loss 1.21637571 epoch total loss 1.19043279\n",
      "Trained batch 703 batch loss 1.20058596 epoch total loss 1.19044721\n",
      "Trained batch 704 batch loss 1.18599784 epoch total loss 1.19044089\n",
      "Trained batch 705 batch loss 1.20818448 epoch total loss 1.19046605\n",
      "Trained batch 706 batch loss 1.31019473 epoch total loss 1.19063556\n",
      "Trained batch 707 batch loss 1.29413962 epoch total loss 1.19078195\n",
      "Trained batch 708 batch loss 1.36805785 epoch total loss 1.19103241\n",
      "Trained batch 709 batch loss 1.30192685 epoch total loss 1.19118881\n",
      "Trained batch 710 batch loss 1.37036669 epoch total loss 1.19144118\n",
      "Trained batch 711 batch loss 1.42723918 epoch total loss 1.19177282\n",
      "Trained batch 712 batch loss 1.41276336 epoch total loss 1.19208324\n",
      "Trained batch 713 batch loss 1.27712452 epoch total loss 1.19220245\n",
      "Trained batch 714 batch loss 1.35303736 epoch total loss 1.19242764\n",
      "Trained batch 715 batch loss 1.29713094 epoch total loss 1.19257414\n",
      "Trained batch 716 batch loss 1.26061666 epoch total loss 1.19266915\n",
      "Trained batch 717 batch loss 1.29707503 epoch total loss 1.19281471\n",
      "Trained batch 718 batch loss 1.22596216 epoch total loss 1.19286084\n",
      "Trained batch 719 batch loss 1.25807667 epoch total loss 1.19295156\n",
      "Trained batch 720 batch loss 1.28371084 epoch total loss 1.19307756\n",
      "Trained batch 721 batch loss 1.29229259 epoch total loss 1.19321525\n",
      "Trained batch 722 batch loss 1.2569828 epoch total loss 1.19330347\n",
      "Trained batch 723 batch loss 1.23767591 epoch total loss 1.19336486\n",
      "Trained batch 724 batch loss 1.22578037 epoch total loss 1.19340956\n",
      "Trained batch 725 batch loss 1.1435945 epoch total loss 1.1933409\n",
      "Trained batch 726 batch loss 1.16699934 epoch total loss 1.19330466\n",
      "Trained batch 727 batch loss 1.17543578 epoch total loss 1.19328\n",
      "Trained batch 728 batch loss 1.14577127 epoch total loss 1.19321477\n",
      "Trained batch 729 batch loss 1.10193646 epoch total loss 1.19308949\n",
      "Trained batch 730 batch loss 1.03642559 epoch total loss 1.19287491\n",
      "Trained batch 731 batch loss 1.03257036 epoch total loss 1.19265568\n",
      "Trained batch 732 batch loss 1.10529685 epoch total loss 1.19253635\n",
      "Trained batch 733 batch loss 1.20419502 epoch total loss 1.19255221\n",
      "Trained batch 734 batch loss 1.22259009 epoch total loss 1.19259322\n",
      "Trained batch 735 batch loss 1.20599318 epoch total loss 1.19261146\n",
      "Trained batch 736 batch loss 1.15094447 epoch total loss 1.19255483\n",
      "Trained batch 737 batch loss 1.24992728 epoch total loss 1.19263268\n",
      "Trained batch 738 batch loss 1.19107962 epoch total loss 1.19263065\n",
      "Trained batch 739 batch loss 1.25606751 epoch total loss 1.19271636\n",
      "Trained batch 740 batch loss 1.20964468 epoch total loss 1.19273925\n",
      "Trained batch 741 batch loss 0.902980566 epoch total loss 1.19234824\n",
      "Trained batch 742 batch loss 0.971494555 epoch total loss 1.19205058\n",
      "Trained batch 743 batch loss 1.05985403 epoch total loss 1.19187272\n",
      "Trained batch 744 batch loss 1.09177685 epoch total loss 1.19173813\n",
      "Trained batch 745 batch loss 1.20591521 epoch total loss 1.1917572\n",
      "Trained batch 746 batch loss 1.11308706 epoch total loss 1.19165182\n",
      "Trained batch 747 batch loss 1.17338252 epoch total loss 1.19162738\n",
      "Trained batch 748 batch loss 1.25730491 epoch total loss 1.19171512\n",
      "Trained batch 749 batch loss 1.20264077 epoch total loss 1.19172978\n",
      "Trained batch 750 batch loss 1.39332283 epoch total loss 1.19199848\n",
      "Trained batch 751 batch loss 1.13209581 epoch total loss 1.19191873\n",
      "Trained batch 752 batch loss 1.08991838 epoch total loss 1.19178307\n",
      "Trained batch 753 batch loss 1.16624522 epoch total loss 1.19174922\n",
      "Trained batch 754 batch loss 1.27874947 epoch total loss 1.19186461\n",
      "Trained batch 755 batch loss 1.35505819 epoch total loss 1.19208074\n",
      "Trained batch 756 batch loss 1.20729947 epoch total loss 1.19210076\n",
      "Trained batch 757 batch loss 1.21410167 epoch total loss 1.19212985\n",
      "Trained batch 758 batch loss 1.1886158 epoch total loss 1.1921252\n",
      "Trained batch 759 batch loss 1.14448118 epoch total loss 1.1920625\n",
      "Trained batch 760 batch loss 1.31465054 epoch total loss 1.19222367\n",
      "Trained batch 761 batch loss 1.18898976 epoch total loss 1.1922195\n",
      "Trained batch 762 batch loss 1.16534543 epoch total loss 1.19218421\n",
      "Trained batch 763 batch loss 1.22487545 epoch total loss 1.19222701\n",
      "Trained batch 764 batch loss 1.22966075 epoch total loss 1.192276\n",
      "Trained batch 765 batch loss 1.17174041 epoch total loss 1.19224918\n",
      "Trained batch 766 batch loss 1.19014072 epoch total loss 1.19224644\n",
      "Trained batch 767 batch loss 1.12986302 epoch total loss 1.19216514\n",
      "Trained batch 768 batch loss 1.15769446 epoch total loss 1.19212019\n",
      "Trained batch 769 batch loss 1.09695303 epoch total loss 1.19199646\n",
      "Trained batch 770 batch loss 1.2502656 epoch total loss 1.19207203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 771 batch loss 1.2960546 epoch total loss 1.19220698\n",
      "Trained batch 772 batch loss 1.08574486 epoch total loss 1.19206905\n",
      "Trained batch 773 batch loss 1.20932543 epoch total loss 1.19209146\n",
      "Trained batch 774 batch loss 1.24358046 epoch total loss 1.19215798\n",
      "Trained batch 775 batch loss 1.188676 epoch total loss 1.19215345\n",
      "Trained batch 776 batch loss 1.13246357 epoch total loss 1.19207656\n",
      "Trained batch 777 batch loss 1.12703955 epoch total loss 1.19199276\n",
      "Trained batch 778 batch loss 1.14474547 epoch total loss 1.19193208\n",
      "Trained batch 779 batch loss 1.09107876 epoch total loss 1.19180262\n",
      "Trained batch 780 batch loss 1.04592156 epoch total loss 1.19161558\n",
      "Trained batch 781 batch loss 1.18418157 epoch total loss 1.19160604\n",
      "Trained batch 782 batch loss 1.17486131 epoch total loss 1.19158471\n",
      "Trained batch 783 batch loss 1.25537705 epoch total loss 1.19166613\n",
      "Trained batch 784 batch loss 1.19292367 epoch total loss 1.1916678\n",
      "Trained batch 785 batch loss 1.35613012 epoch total loss 1.19187725\n",
      "Trained batch 786 batch loss 1.35054147 epoch total loss 1.19207907\n",
      "Trained batch 787 batch loss 1.3482064 epoch total loss 1.19227755\n",
      "Trained batch 788 batch loss 1.31849611 epoch total loss 1.19243765\n",
      "Trained batch 789 batch loss 1.13260031 epoch total loss 1.19236183\n",
      "Trained batch 790 batch loss 1.1779927 epoch total loss 1.19234359\n",
      "Trained batch 791 batch loss 1.20800614 epoch total loss 1.1923635\n",
      "Trained batch 792 batch loss 1.07184207 epoch total loss 1.19221127\n",
      "Trained batch 793 batch loss 1.09226799 epoch total loss 1.19208527\n",
      "Trained batch 794 batch loss 0.977961838 epoch total loss 1.19181561\n",
      "Trained batch 795 batch loss 0.985974431 epoch total loss 1.19155669\n",
      "Trained batch 796 batch loss 1.0209918 epoch total loss 1.19134235\n",
      "Trained batch 797 batch loss 1.01064229 epoch total loss 1.19111562\n",
      "Trained batch 798 batch loss 1.02509189 epoch total loss 1.1909076\n",
      "Trained batch 799 batch loss 1.15276623 epoch total loss 1.19085979\n",
      "Trained batch 800 batch loss 1.12015247 epoch total loss 1.19077146\n",
      "Trained batch 801 batch loss 1.03905833 epoch total loss 1.19058204\n",
      "Trained batch 802 batch loss 1.01479065 epoch total loss 1.19036281\n",
      "Trained batch 803 batch loss 1.13546312 epoch total loss 1.1902945\n",
      "Trained batch 804 batch loss 1.1866405 epoch total loss 1.19029\n",
      "Trained batch 805 batch loss 1.1177392 epoch total loss 1.19019985\n",
      "Trained batch 806 batch loss 1.06475496 epoch total loss 1.19004416\n",
      "Trained batch 807 batch loss 0.937328875 epoch total loss 1.189731\n",
      "Trained batch 808 batch loss 1.00684035 epoch total loss 1.18950462\n",
      "Trained batch 809 batch loss 1.12512136 epoch total loss 1.18942511\n",
      "Trained batch 810 batch loss 1.21601319 epoch total loss 1.18945789\n",
      "Trained batch 811 batch loss 1.3345027 epoch total loss 1.18963671\n",
      "Trained batch 812 batch loss 1.43772602 epoch total loss 1.18994224\n",
      "Trained batch 813 batch loss 1.40751624 epoch total loss 1.19020987\n",
      "Trained batch 814 batch loss 1.23912239 epoch total loss 1.19027\n",
      "Trained batch 815 batch loss 1.28436732 epoch total loss 1.19038546\n",
      "Trained batch 816 batch loss 1.13416052 epoch total loss 1.19031656\n",
      "Trained batch 817 batch loss 1.07614827 epoch total loss 1.19017684\n",
      "Trained batch 818 batch loss 1.1526072 epoch total loss 1.19013083\n",
      "Trained batch 819 batch loss 1.15006232 epoch total loss 1.19008195\n",
      "Trained batch 820 batch loss 1.15342069 epoch total loss 1.19003725\n",
      "Trained batch 821 batch loss 1.29984951 epoch total loss 1.190171\n",
      "Trained batch 822 batch loss 1.20984 epoch total loss 1.19019496\n",
      "Trained batch 823 batch loss 1.20055008 epoch total loss 1.1902076\n",
      "Trained batch 824 batch loss 1.14821243 epoch total loss 1.19015658\n",
      "Trained batch 825 batch loss 1.24238324 epoch total loss 1.19021988\n",
      "Trained batch 826 batch loss 1.17393827 epoch total loss 1.19020021\n",
      "Trained batch 827 batch loss 1.33636904 epoch total loss 1.19037688\n",
      "Trained batch 828 batch loss 1.18107986 epoch total loss 1.19036567\n",
      "Trained batch 829 batch loss 1.21874762 epoch total loss 1.19039989\n",
      "Trained batch 830 batch loss 1.32571638 epoch total loss 1.19056296\n",
      "Trained batch 831 batch loss 1.20634854 epoch total loss 1.19058204\n",
      "Trained batch 832 batch loss 1.27913713 epoch total loss 1.19068837\n",
      "Trained batch 833 batch loss 1.2689898 epoch total loss 1.19078243\n",
      "Trained batch 834 batch loss 1.16756868 epoch total loss 1.19075453\n",
      "Trained batch 835 batch loss 1.22387218 epoch total loss 1.19079423\n",
      "Trained batch 836 batch loss 1.27873898 epoch total loss 1.19089937\n",
      "Trained batch 837 batch loss 1.26521623 epoch total loss 1.19098818\n",
      "Trained batch 838 batch loss 1.23996353 epoch total loss 1.19104671\n",
      "Trained batch 839 batch loss 1.28896213 epoch total loss 1.1911633\n",
      "Trained batch 840 batch loss 1.03314495 epoch total loss 1.19097519\n",
      "Trained batch 841 batch loss 1.07740462 epoch total loss 1.19084013\n",
      "Trained batch 842 batch loss 1.09291899 epoch total loss 1.1907239\n",
      "Trained batch 843 batch loss 1.04743814 epoch total loss 1.1905539\n",
      "Trained batch 844 batch loss 1.28506541 epoch total loss 1.19066584\n",
      "Trained batch 845 batch loss 1.18806958 epoch total loss 1.19066274\n",
      "Trained batch 846 batch loss 1.27613568 epoch total loss 1.19076383\n",
      "Trained batch 847 batch loss 1.36872232 epoch total loss 1.19097388\n",
      "Trained batch 848 batch loss 1.29959083 epoch total loss 1.19110191\n",
      "Trained batch 849 batch loss 1.05141485 epoch total loss 1.1909374\n",
      "Trained batch 850 batch loss 1.12396705 epoch total loss 1.1908586\n",
      "Trained batch 851 batch loss 1.04923368 epoch total loss 1.19069219\n",
      "Trained batch 852 batch loss 1.13500166 epoch total loss 1.19062686\n",
      "Trained batch 853 batch loss 1.1550591 epoch total loss 1.19058514\n",
      "Trained batch 854 batch loss 1.21127021 epoch total loss 1.19060934\n",
      "Trained batch 855 batch loss 1.24625206 epoch total loss 1.19067442\n",
      "Trained batch 856 batch loss 1.13986301 epoch total loss 1.19061506\n",
      "Trained batch 857 batch loss 1.23960507 epoch total loss 1.19067228\n",
      "Trained batch 858 batch loss 1.070328 epoch total loss 1.19053197\n",
      "Trained batch 859 batch loss 1.36736178 epoch total loss 1.19073784\n",
      "Trained batch 860 batch loss 1.45649242 epoch total loss 1.19104683\n",
      "Trained batch 861 batch loss 1.44427907 epoch total loss 1.19134104\n",
      "Trained batch 862 batch loss 1.35203409 epoch total loss 1.19152749\n",
      "Trained batch 863 batch loss 1.12678516 epoch total loss 1.1914525\n",
      "Trained batch 864 batch loss 1.13313568 epoch total loss 1.19138503\n",
      "Trained batch 865 batch loss 1.22313142 epoch total loss 1.19142175\n",
      "Trained batch 866 batch loss 1.46210301 epoch total loss 1.19173443\n",
      "Trained batch 867 batch loss 1.20827317 epoch total loss 1.19175351\n",
      "Trained batch 868 batch loss 1.21687925 epoch total loss 1.19178247\n",
      "Trained batch 869 batch loss 1.08862329 epoch total loss 1.19166374\n",
      "Trained batch 870 batch loss 1.05611372 epoch total loss 1.19150794\n",
      "Trained batch 871 batch loss 1.25415361 epoch total loss 1.19157994\n",
      "Trained batch 872 batch loss 1.18483388 epoch total loss 1.19157219\n",
      "Trained batch 873 batch loss 1.25142872 epoch total loss 1.19164073\n",
      "Trained batch 874 batch loss 1.26279104 epoch total loss 1.19172215\n",
      "Trained batch 875 batch loss 1.35076904 epoch total loss 1.19190407\n",
      "Trained batch 876 batch loss 1.38120794 epoch total loss 1.19212019\n",
      "Trained batch 877 batch loss 1.35720253 epoch total loss 1.19230831\n",
      "Trained batch 878 batch loss 1.22611833 epoch total loss 1.19234681\n",
      "Trained batch 879 batch loss 1.26561522 epoch total loss 1.19243014\n",
      "Trained batch 880 batch loss 1.12836957 epoch total loss 1.19235742\n",
      "Trained batch 881 batch loss 1.37827849 epoch total loss 1.19256854\n",
      "Trained batch 882 batch loss 1.16581333 epoch total loss 1.19253814\n",
      "Trained batch 883 batch loss 1.18492746 epoch total loss 1.19252944\n",
      "Trained batch 884 batch loss 1.10690904 epoch total loss 1.19243264\n",
      "Trained batch 885 batch loss 1.09258342 epoch total loss 1.19231975\n",
      "Trained batch 886 batch loss 1.04228604 epoch total loss 1.19215035\n",
      "Trained batch 887 batch loss 1.16193366 epoch total loss 1.19211638\n",
      "Trained batch 888 batch loss 1.07807219 epoch total loss 1.19198799\n",
      "Trained batch 889 batch loss 1.12617981 epoch total loss 1.19191408\n",
      "Trained batch 890 batch loss 0.902116358 epoch total loss 1.1915884\n",
      "Trained batch 891 batch loss 1.16468084 epoch total loss 1.19155824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 892 batch loss 1.22101748 epoch total loss 1.19159126\n",
      "Trained batch 893 batch loss 1.21519327 epoch total loss 1.19161773\n",
      "Trained batch 894 batch loss 1.23505461 epoch total loss 1.19166636\n",
      "Trained batch 895 batch loss 1.30162942 epoch total loss 1.19178927\n",
      "Trained batch 896 batch loss 1.16664362 epoch total loss 1.19176114\n",
      "Trained batch 897 batch loss 1.06952846 epoch total loss 1.191625\n",
      "Trained batch 898 batch loss 1.06639338 epoch total loss 1.19148552\n",
      "Trained batch 899 batch loss 1.18292785 epoch total loss 1.19147599\n",
      "Trained batch 900 batch loss 1.19597912 epoch total loss 1.19148099\n",
      "Trained batch 901 batch loss 1.19629252 epoch total loss 1.19148636\n",
      "Trained batch 902 batch loss 1.1859138 epoch total loss 1.19148016\n",
      "Trained batch 903 batch loss 1.30706322 epoch total loss 1.19160807\n",
      "Trained batch 904 batch loss 1.22916365 epoch total loss 1.19164956\n",
      "Trained batch 905 batch loss 1.30914283 epoch total loss 1.19177938\n",
      "Trained batch 906 batch loss 1.2387327 epoch total loss 1.19183123\n",
      "Trained batch 907 batch loss 1.37121964 epoch total loss 1.192029\n",
      "Trained batch 908 batch loss 1.28342307 epoch total loss 1.19212961\n",
      "Trained batch 909 batch loss 1.33874094 epoch total loss 1.1922909\n",
      "Trained batch 910 batch loss 1.23121047 epoch total loss 1.1923337\n",
      "Trained batch 911 batch loss 1.12427938 epoch total loss 1.19225895\n",
      "Trained batch 912 batch loss 1.05578732 epoch total loss 1.19210935\n",
      "Trained batch 913 batch loss 1.01686907 epoch total loss 1.19191742\n",
      "Trained batch 914 batch loss 1.08922136 epoch total loss 1.19180501\n",
      "Trained batch 915 batch loss 1.1367619 epoch total loss 1.1917448\n",
      "Trained batch 916 batch loss 1.038818 epoch total loss 1.19157791\n",
      "Trained batch 917 batch loss 1.09959352 epoch total loss 1.19147766\n",
      "Trained batch 918 batch loss 1.08001697 epoch total loss 1.19135606\n",
      "Trained batch 919 batch loss 1.20349121 epoch total loss 1.1913693\n",
      "Trained batch 920 batch loss 1.10156012 epoch total loss 1.19127166\n",
      "Trained batch 921 batch loss 1.1564306 epoch total loss 1.19123387\n",
      "Trained batch 922 batch loss 1.12678838 epoch total loss 1.19116402\n",
      "Trained batch 923 batch loss 1.06676447 epoch total loss 1.19102919\n",
      "Trained batch 924 batch loss 1.01831818 epoch total loss 1.19084227\n",
      "Trained batch 925 batch loss 1.16650343 epoch total loss 1.19081593\n",
      "Trained batch 926 batch loss 1.09160411 epoch total loss 1.19070876\n",
      "Trained batch 927 batch loss 1.34762335 epoch total loss 1.19087803\n",
      "Trained batch 928 batch loss 1.42116702 epoch total loss 1.19112623\n",
      "Trained batch 929 batch loss 1.41175044 epoch total loss 1.19136369\n",
      "Trained batch 930 batch loss 1.10683274 epoch total loss 1.19127274\n",
      "Trained batch 931 batch loss 1.21303606 epoch total loss 1.1912961\n",
      "Trained batch 932 batch loss 1.1554358 epoch total loss 1.1912576\n",
      "Trained batch 933 batch loss 1.15346646 epoch total loss 1.19121706\n",
      "Trained batch 934 batch loss 1.27811897 epoch total loss 1.19131\n",
      "Trained batch 935 batch loss 1.11158085 epoch total loss 1.19122481\n",
      "Trained batch 936 batch loss 1.29171991 epoch total loss 1.19133222\n",
      "Trained batch 937 batch loss 1.13072896 epoch total loss 1.19126749\n",
      "Trained batch 938 batch loss 1.06739545 epoch total loss 1.19113541\n",
      "Trained batch 939 batch loss 1.17036057 epoch total loss 1.19111335\n",
      "Trained batch 940 batch loss 1.29443491 epoch total loss 1.19122326\n",
      "Trained batch 941 batch loss 1.33328819 epoch total loss 1.19137418\n",
      "Trained batch 942 batch loss 1.15904808 epoch total loss 1.19133985\n",
      "Trained batch 943 batch loss 0.920761764 epoch total loss 1.19105303\n",
      "Trained batch 944 batch loss 1.09983373 epoch total loss 1.19095635\n",
      "Trained batch 945 batch loss 1.01268899 epoch total loss 1.19076777\n",
      "Trained batch 946 batch loss 1.02707422 epoch total loss 1.19059467\n",
      "Trained batch 947 batch loss 0.938051343 epoch total loss 1.19032812\n",
      "Trained batch 948 batch loss 1.04530013 epoch total loss 1.19017518\n",
      "Trained batch 949 batch loss 1.09633327 epoch total loss 1.19007623\n",
      "Trained batch 950 batch loss 1.14383554 epoch total loss 1.19002748\n",
      "Trained batch 951 batch loss 1.17660391 epoch total loss 1.19001341\n",
      "Trained batch 952 batch loss 1.20259869 epoch total loss 1.19002664\n",
      "Trained batch 953 batch loss 1.21405149 epoch total loss 1.19005191\n",
      "Trained batch 954 batch loss 1.13409829 epoch total loss 1.18999338\n",
      "Trained batch 955 batch loss 1.11776972 epoch total loss 1.1899178\n",
      "Trained batch 956 batch loss 1.26376379 epoch total loss 1.18999505\n",
      "Trained batch 957 batch loss 1.32228553 epoch total loss 1.19013321\n",
      "Trained batch 958 batch loss 1.24285817 epoch total loss 1.19018817\n",
      "Trained batch 959 batch loss 1.28279364 epoch total loss 1.19028485\n",
      "Trained batch 960 batch loss 1.22137666 epoch total loss 1.19031727\n",
      "Trained batch 961 batch loss 1.14462364 epoch total loss 1.19026971\n",
      "Trained batch 962 batch loss 1.326967 epoch total loss 1.19041193\n",
      "Trained batch 963 batch loss 1.1444397 epoch total loss 1.19036412\n",
      "Trained batch 964 batch loss 1.26554847 epoch total loss 1.19044209\n",
      "Trained batch 965 batch loss 1.33117735 epoch total loss 1.19058788\n",
      "Trained batch 966 batch loss 1.17591906 epoch total loss 1.19057274\n",
      "Trained batch 967 batch loss 1.28225589 epoch total loss 1.19066751\n",
      "Trained batch 968 batch loss 1.33845222 epoch total loss 1.19082022\n",
      "Trained batch 969 batch loss 1.11168027 epoch total loss 1.19073856\n",
      "Trained batch 970 batch loss 1.2214148 epoch total loss 1.19077027\n",
      "Trained batch 971 batch loss 1.23065257 epoch total loss 1.1908114\n",
      "Trained batch 972 batch loss 1.22139645 epoch total loss 1.19084287\n",
      "Trained batch 973 batch loss 1.17977786 epoch total loss 1.19083154\n",
      "Trained batch 974 batch loss 1.17323744 epoch total loss 1.19081342\n",
      "Trained batch 975 batch loss 1.11246753 epoch total loss 1.19073308\n",
      "Trained batch 976 batch loss 1.12630665 epoch total loss 1.19066703\n",
      "Trained batch 977 batch loss 1.18414402 epoch total loss 1.19066048\n",
      "Trained batch 978 batch loss 1.23995256 epoch total loss 1.1907109\n",
      "Trained batch 979 batch loss 1.18488145 epoch total loss 1.19070494\n",
      "Trained batch 980 batch loss 1.08477974 epoch total loss 1.19059694\n",
      "Trained batch 981 batch loss 1.07326043 epoch total loss 1.19047737\n",
      "Trained batch 982 batch loss 1.14172328 epoch total loss 1.19042766\n",
      "Trained batch 983 batch loss 1.17352962 epoch total loss 1.19041061\n",
      "Trained batch 984 batch loss 1.19993329 epoch total loss 1.19042027\n",
      "Trained batch 985 batch loss 1.28859293 epoch total loss 1.19051993\n",
      "Trained batch 986 batch loss 1.25178802 epoch total loss 1.19058204\n",
      "Trained batch 987 batch loss 1.22752404 epoch total loss 1.19061959\n",
      "Trained batch 988 batch loss 1.27562356 epoch total loss 1.19070554\n",
      "Trained batch 989 batch loss 1.27101231 epoch total loss 1.19078672\n",
      "Trained batch 990 batch loss 1.38892472 epoch total loss 1.19098687\n",
      "Trained batch 991 batch loss 1.18835676 epoch total loss 1.19098425\n",
      "Trained batch 992 batch loss 1.06757581 epoch total loss 1.19085991\n",
      "Trained batch 993 batch loss 1.18139291 epoch total loss 1.19085038\n",
      "Trained batch 994 batch loss 1.09130275 epoch total loss 1.19075024\n",
      "Trained batch 995 batch loss 1.00406098 epoch total loss 1.19056261\n",
      "Trained batch 996 batch loss 1.1761632 epoch total loss 1.19054806\n",
      "Trained batch 997 batch loss 1.24329805 epoch total loss 1.19060099\n",
      "Trained batch 998 batch loss 1.19770396 epoch total loss 1.19060814\n",
      "Trained batch 999 batch loss 1.09967399 epoch total loss 1.19051719\n",
      "Trained batch 1000 batch loss 1.10563445 epoch total loss 1.19043231\n",
      "Trained batch 1001 batch loss 1.12239361 epoch total loss 1.19036436\n",
      "Trained batch 1002 batch loss 1.08003151 epoch total loss 1.19025421\n",
      "Trained batch 1003 batch loss 1.0948782 epoch total loss 1.19015908\n",
      "Trained batch 1004 batch loss 1.12606859 epoch total loss 1.19009531\n",
      "Trained batch 1005 batch loss 1.00540197 epoch total loss 1.18991148\n",
      "Trained batch 1006 batch loss 1.17833805 epoch total loss 1.1899\n",
      "Trained batch 1007 batch loss 1.10519302 epoch total loss 1.189816\n",
      "Trained batch 1008 batch loss 1.11789489 epoch total loss 1.18974459\n",
      "Trained batch 1009 batch loss 1.06586695 epoch total loss 1.18962193\n",
      "Trained batch 1010 batch loss 1.05606365 epoch total loss 1.1894896\n",
      "Trained batch 1011 batch loss 1.07734287 epoch total loss 1.18937874\n",
      "Trained batch 1012 batch loss 1.13540804 epoch total loss 1.18932533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1013 batch loss 1.26833034 epoch total loss 1.18940341\n",
      "Trained batch 1014 batch loss 1.14907563 epoch total loss 1.1893636\n",
      "Trained batch 1015 batch loss 1.18517649 epoch total loss 1.18935943\n",
      "Trained batch 1016 batch loss 1.2082727 epoch total loss 1.18937802\n",
      "Trained batch 1017 batch loss 1.24088085 epoch total loss 1.18942869\n",
      "Trained batch 1018 batch loss 1.25303411 epoch total loss 1.18949115\n",
      "Trained batch 1019 batch loss 1.18560517 epoch total loss 1.18948722\n",
      "Trained batch 1020 batch loss 1.07464194 epoch total loss 1.18937457\n",
      "Trained batch 1021 batch loss 1.1543622 epoch total loss 1.18934035\n",
      "Trained batch 1022 batch loss 1.21245527 epoch total loss 1.189363\n",
      "Trained batch 1023 batch loss 1.15466678 epoch total loss 1.18932903\n",
      "Trained batch 1024 batch loss 1.14681315 epoch total loss 1.18928754\n",
      "Trained batch 1025 batch loss 1.10677624 epoch total loss 1.18920708\n",
      "Trained batch 1026 batch loss 1.11805046 epoch total loss 1.1891377\n",
      "Trained batch 1027 batch loss 1.22414541 epoch total loss 1.18917179\n",
      "Trained batch 1028 batch loss 1.20217454 epoch total loss 1.18918443\n",
      "Trained batch 1029 batch loss 1.22772384 epoch total loss 1.18922186\n",
      "Trained batch 1030 batch loss 1.28464186 epoch total loss 1.1893146\n",
      "Trained batch 1031 batch loss 1.16952419 epoch total loss 1.18929541\n",
      "Trained batch 1032 batch loss 1.09688926 epoch total loss 1.18920588\n",
      "Trained batch 1033 batch loss 1.19367039 epoch total loss 1.1892103\n",
      "Trained batch 1034 batch loss 1.16093624 epoch total loss 1.18918288\n",
      "Trained batch 1035 batch loss 1.26876771 epoch total loss 1.18925977\n",
      "Trained batch 1036 batch loss 1.27510571 epoch total loss 1.18934274\n",
      "Trained batch 1037 batch loss 1.31586218 epoch total loss 1.18946481\n",
      "Trained batch 1038 batch loss 1.29100561 epoch total loss 1.18956256\n",
      "Trained batch 1039 batch loss 1.30714786 epoch total loss 1.18967581\n",
      "Trained batch 1040 batch loss 1.13066041 epoch total loss 1.18961895\n",
      "Trained batch 1041 batch loss 1.10567331 epoch total loss 1.18953836\n",
      "Trained batch 1042 batch loss 1.13052928 epoch total loss 1.18948174\n",
      "Trained batch 1043 batch loss 1.08746231 epoch total loss 1.18938386\n",
      "Trained batch 1044 batch loss 1.24599481 epoch total loss 1.18943799\n",
      "Trained batch 1045 batch loss 1.21606708 epoch total loss 1.1894635\n",
      "Trained batch 1046 batch loss 1.18448424 epoch total loss 1.18945873\n",
      "Trained batch 1047 batch loss 1.26335609 epoch total loss 1.1895293\n",
      "Trained batch 1048 batch loss 1.08032012 epoch total loss 1.18942511\n",
      "Trained batch 1049 batch loss 1.18792629 epoch total loss 1.18942356\n",
      "Trained batch 1050 batch loss 1.14292848 epoch total loss 1.18937933\n",
      "Trained batch 1051 batch loss 1.16033101 epoch total loss 1.18935156\n",
      "Trained batch 1052 batch loss 1.08494473 epoch total loss 1.18925238\n",
      "Trained batch 1053 batch loss 0.984293461 epoch total loss 1.18905771\n",
      "Trained batch 1054 batch loss 1.18793547 epoch total loss 1.18905663\n",
      "Trained batch 1055 batch loss 1.18084908 epoch total loss 1.18904901\n",
      "Trained batch 1056 batch loss 1.24832296 epoch total loss 1.18910503\n",
      "Trained batch 1057 batch loss 1.12273729 epoch total loss 1.18904221\n",
      "Trained batch 1058 batch loss 1.06563604 epoch total loss 1.18892562\n",
      "Trained batch 1059 batch loss 0.934888601 epoch total loss 1.18868577\n",
      "Trained batch 1060 batch loss 0.953107357 epoch total loss 1.18846357\n",
      "Trained batch 1061 batch loss 0.94608748 epoch total loss 1.18823504\n",
      "Trained batch 1062 batch loss 0.995400429 epoch total loss 1.18805349\n",
      "Trained batch 1063 batch loss 1.19210815 epoch total loss 1.1880573\n",
      "Trained batch 1064 batch loss 1.21886408 epoch total loss 1.18808627\n",
      "Trained batch 1065 batch loss 1.13143945 epoch total loss 1.1880331\n",
      "Trained batch 1066 batch loss 1.02787852 epoch total loss 1.18788278\n",
      "Trained batch 1067 batch loss 0.999155641 epoch total loss 1.18770587\n",
      "Trained batch 1068 batch loss 1.02840543 epoch total loss 1.18755686\n",
      "Trained batch 1069 batch loss 1.5458703 epoch total loss 1.18789196\n",
      "Trained batch 1070 batch loss 1.08150947 epoch total loss 1.18779266\n",
      "Trained batch 1071 batch loss 1.21656144 epoch total loss 1.18781948\n",
      "Trained batch 1072 batch loss 1.09794378 epoch total loss 1.18773556\n",
      "Trained batch 1073 batch loss 1.16641986 epoch total loss 1.18771565\n",
      "Trained batch 1074 batch loss 1.10321486 epoch total loss 1.18763709\n",
      "Trained batch 1075 batch loss 1.08992898 epoch total loss 1.18754625\n",
      "Trained batch 1076 batch loss 1.18461823 epoch total loss 1.18754339\n",
      "Trained batch 1077 batch loss 1.17753935 epoch total loss 1.18753409\n",
      "Trained batch 1078 batch loss 1.12524498 epoch total loss 1.18747628\n",
      "Trained batch 1079 batch loss 1.22232461 epoch total loss 1.18750858\n",
      "Trained batch 1080 batch loss 1.23224044 epoch total loss 1.18755007\n",
      "Trained batch 1081 batch loss 1.17682028 epoch total loss 1.18754017\n",
      "Trained batch 1082 batch loss 1.09446287 epoch total loss 1.18745422\n",
      "Trained batch 1083 batch loss 1.16243017 epoch total loss 1.1874311\n",
      "Trained batch 1084 batch loss 1.09262013 epoch total loss 1.18734372\n",
      "Trained batch 1085 batch loss 1.17244983 epoch total loss 1.18733\n",
      "Trained batch 1086 batch loss 1.18048882 epoch total loss 1.18732381\n",
      "Trained batch 1087 batch loss 1.05314398 epoch total loss 1.18720031\n",
      "Trained batch 1088 batch loss 1.1170435 epoch total loss 1.18713582\n",
      "Trained batch 1089 batch loss 1.29988527 epoch total loss 1.18723941\n",
      "Trained batch 1090 batch loss 1.24066508 epoch total loss 1.1872884\n",
      "Trained batch 1091 batch loss 1.23592973 epoch total loss 1.18733311\n",
      "Trained batch 1092 batch loss 1.19639659 epoch total loss 1.18734133\n",
      "Trained batch 1093 batch loss 1.21585763 epoch total loss 1.18736744\n",
      "Trained batch 1094 batch loss 1.33033156 epoch total loss 1.18749809\n",
      "Trained batch 1095 batch loss 1.22119915 epoch total loss 1.18752885\n",
      "Trained batch 1096 batch loss 1.14491451 epoch total loss 1.18749\n",
      "Trained batch 1097 batch loss 1.10583615 epoch total loss 1.1874156\n",
      "Trained batch 1098 batch loss 1.09872019 epoch total loss 1.18733478\n",
      "Trained batch 1099 batch loss 1.18344116 epoch total loss 1.18733132\n",
      "Trained batch 1100 batch loss 1.09818828 epoch total loss 1.18725026\n",
      "Trained batch 1101 batch loss 1.13555825 epoch total loss 1.18720317\n",
      "Trained batch 1102 batch loss 1.14401746 epoch total loss 1.18716407\n",
      "Trained batch 1103 batch loss 1.07268178 epoch total loss 1.18706024\n",
      "Trained batch 1104 batch loss 1.00569475 epoch total loss 1.18689597\n",
      "Trained batch 1105 batch loss 1.09377754 epoch total loss 1.18681169\n",
      "Trained batch 1106 batch loss 1.2248137 epoch total loss 1.18684602\n",
      "Trained batch 1107 batch loss 1.10662711 epoch total loss 1.18677354\n",
      "Trained batch 1108 batch loss 1.24011481 epoch total loss 1.1868217\n",
      "Trained batch 1109 batch loss 1.47106409 epoch total loss 1.187078\n",
      "Trained batch 1110 batch loss 1.09250355 epoch total loss 1.18699276\n",
      "Trained batch 1111 batch loss 1.00295889 epoch total loss 1.18682718\n",
      "Trained batch 1112 batch loss 1.05333352 epoch total loss 1.18670714\n",
      "Trained batch 1113 batch loss 1.08097053 epoch total loss 1.18661201\n",
      "Trained batch 1114 batch loss 1.17814469 epoch total loss 1.18660438\n",
      "Trained batch 1115 batch loss 1.21169937 epoch total loss 1.18662691\n",
      "Trained batch 1116 batch loss 1.28487039 epoch total loss 1.18671501\n",
      "Trained batch 1117 batch loss 1.1914196 epoch total loss 1.18671918\n",
      "Trained batch 1118 batch loss 1.18611348 epoch total loss 1.1867187\n",
      "Trained batch 1119 batch loss 1.13063908 epoch total loss 1.18666852\n",
      "Trained batch 1120 batch loss 1.1510911 epoch total loss 1.18663681\n",
      "Trained batch 1121 batch loss 1.22035539 epoch total loss 1.18666685\n",
      "Trained batch 1122 batch loss 1.34839582 epoch total loss 1.18681097\n",
      "Trained batch 1123 batch loss 1.27364862 epoch total loss 1.18688834\n",
      "Trained batch 1124 batch loss 1.11655426 epoch total loss 1.18682575\n",
      "Trained batch 1125 batch loss 1.12629151 epoch total loss 1.18677199\n",
      "Trained batch 1126 batch loss 0.996556759 epoch total loss 1.18660307\n",
      "Trained batch 1127 batch loss 1.08844149 epoch total loss 1.18651605\n",
      "Trained batch 1128 batch loss 1.19621396 epoch total loss 1.18652463\n",
      "Trained batch 1129 batch loss 1.15539801 epoch total loss 1.18649709\n",
      "Trained batch 1130 batch loss 1.32144356 epoch total loss 1.18661642\n",
      "Trained batch 1131 batch loss 1.22059917 epoch total loss 1.18664646\n",
      "Trained batch 1132 batch loss 1.30566657 epoch total loss 1.1867516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1133 batch loss 1.1960901 epoch total loss 1.18675983\n",
      "Trained batch 1134 batch loss 1.11639857 epoch total loss 1.18669784\n",
      "Trained batch 1135 batch loss 1.02464628 epoch total loss 1.18655503\n",
      "Trained batch 1136 batch loss 1.13020873 epoch total loss 1.18650544\n",
      "Trained batch 1137 batch loss 1.1095674 epoch total loss 1.18643785\n",
      "Trained batch 1138 batch loss 1.171808 epoch total loss 1.18642497\n",
      "Trained batch 1139 batch loss 1.13174891 epoch total loss 1.18637693\n",
      "Trained batch 1140 batch loss 1.07950187 epoch total loss 1.18628311\n",
      "Trained batch 1141 batch loss 1.09736514 epoch total loss 1.18620527\n",
      "Trained batch 1142 batch loss 1.09239721 epoch total loss 1.18612313\n",
      "Trained batch 1143 batch loss 1.29454052 epoch total loss 1.18621802\n",
      "Trained batch 1144 batch loss 1.27787399 epoch total loss 1.18629813\n",
      "Trained batch 1145 batch loss 1.11261642 epoch total loss 1.18623376\n",
      "Trained batch 1146 batch loss 1.18173969 epoch total loss 1.18622983\n",
      "Trained batch 1147 batch loss 0.840532422 epoch total loss 1.18592846\n",
      "Trained batch 1148 batch loss 0.787983298 epoch total loss 1.1855818\n",
      "Trained batch 1149 batch loss 0.866778433 epoch total loss 1.1853044\n",
      "Trained batch 1150 batch loss 1.09239912 epoch total loss 1.1852237\n",
      "Trained batch 1151 batch loss 1.22603953 epoch total loss 1.1852591\n",
      "Trained batch 1152 batch loss 1.23678637 epoch total loss 1.18530393\n",
      "Trained batch 1153 batch loss 1.33642614 epoch total loss 1.18543494\n",
      "Trained batch 1154 batch loss 1.1354 epoch total loss 1.18539155\n",
      "Trained batch 1155 batch loss 1.34355724 epoch total loss 1.18552852\n",
      "Trained batch 1156 batch loss 1.07942581 epoch total loss 1.18543673\n",
      "Trained batch 1157 batch loss 1.20270944 epoch total loss 1.18545175\n",
      "Trained batch 1158 batch loss 1.14939189 epoch total loss 1.18542063\n",
      "Trained batch 1159 batch loss 1.25391972 epoch total loss 1.18547964\n",
      "Trained batch 1160 batch loss 1.29789472 epoch total loss 1.18557656\n",
      "Trained batch 1161 batch loss 1.24964213 epoch total loss 1.18563175\n",
      "Trained batch 1162 batch loss 1.18203866 epoch total loss 1.18562865\n",
      "Trained batch 1163 batch loss 1.13800108 epoch total loss 1.18558776\n",
      "Trained batch 1164 batch loss 1.30805504 epoch total loss 1.18569291\n",
      "Trained batch 1165 batch loss 1.14944029 epoch total loss 1.18566179\n",
      "Trained batch 1166 batch loss 1.32699919 epoch total loss 1.18578303\n",
      "Trained batch 1167 batch loss 1.23154688 epoch total loss 1.18582225\n",
      "Trained batch 1168 batch loss 1.29060531 epoch total loss 1.18591201\n",
      "Trained batch 1169 batch loss 1.17122316 epoch total loss 1.1858995\n",
      "Trained batch 1170 batch loss 1.09436166 epoch total loss 1.18582129\n",
      "Trained batch 1171 batch loss 1.12333357 epoch total loss 1.18576789\n",
      "Trained batch 1172 batch loss 1.2770853 epoch total loss 1.18584585\n",
      "Trained batch 1173 batch loss 1.1974268 epoch total loss 1.18585563\n",
      "Trained batch 1174 batch loss 1.15886545 epoch total loss 1.18583262\n",
      "Trained batch 1175 batch loss 1.29635334 epoch total loss 1.18592668\n",
      "Trained batch 1176 batch loss 1.22564781 epoch total loss 1.18596053\n",
      "Trained batch 1177 batch loss 1.26321888 epoch total loss 1.1860261\n",
      "Trained batch 1178 batch loss 1.21421874 epoch total loss 1.18605\n",
      "Trained batch 1179 batch loss 1.21243882 epoch total loss 1.18607247\n",
      "Trained batch 1180 batch loss 1.24894214 epoch total loss 1.18612564\n",
      "Trained batch 1181 batch loss 1.13961351 epoch total loss 1.1860863\n",
      "Trained batch 1182 batch loss 1.28682983 epoch total loss 1.18617153\n",
      "Trained batch 1183 batch loss 1.19111276 epoch total loss 1.18617582\n",
      "Trained batch 1184 batch loss 1.19142938 epoch total loss 1.18618023\n",
      "Trained batch 1185 batch loss 1.15231776 epoch total loss 1.18615162\n",
      "Trained batch 1186 batch loss 1.09894681 epoch total loss 1.18607819\n",
      "Trained batch 1187 batch loss 1.04128933 epoch total loss 1.18595612\n",
      "Trained batch 1188 batch loss 1.02944314 epoch total loss 1.18582439\n",
      "Trained batch 1189 batch loss 0.989435911 epoch total loss 1.18565917\n",
      "Trained batch 1190 batch loss 1.10435176 epoch total loss 1.18559086\n",
      "Trained batch 1191 batch loss 1.1108551 epoch total loss 1.18552816\n",
      "Trained batch 1192 batch loss 1.02017438 epoch total loss 1.1853894\n",
      "Trained batch 1193 batch loss 1.21204543 epoch total loss 1.18541169\n",
      "Trained batch 1194 batch loss 1.26903296 epoch total loss 1.18548179\n",
      "Trained batch 1195 batch loss 1.28830719 epoch total loss 1.18556786\n",
      "Trained batch 1196 batch loss 1.24156833 epoch total loss 1.18561471\n",
      "Trained batch 1197 batch loss 1.21243894 epoch total loss 1.185637\n",
      "Trained batch 1198 batch loss 1.30082762 epoch total loss 1.18573308\n",
      "Trained batch 1199 batch loss 1.15047634 epoch total loss 1.18570375\n",
      "Trained batch 1200 batch loss 1.07705128 epoch total loss 1.18561316\n",
      "Trained batch 1201 batch loss 1.09698653 epoch total loss 1.18553948\n",
      "Trained batch 1202 batch loss 0.951845884 epoch total loss 1.18534505\n",
      "Trained batch 1203 batch loss 1.03300059 epoch total loss 1.18521845\n",
      "Trained batch 1204 batch loss 1.00642025 epoch total loss 1.18506992\n",
      "Trained batch 1205 batch loss 1.04464328 epoch total loss 1.18495345\n",
      "Trained batch 1206 batch loss 0.90433228 epoch total loss 1.18472075\n",
      "Trained batch 1207 batch loss 0.880520582 epoch total loss 1.18446863\n",
      "Trained batch 1208 batch loss 0.901147187 epoch total loss 1.18423414\n",
      "Trained batch 1209 batch loss 0.99417752 epoch total loss 1.18407691\n",
      "Trained batch 1210 batch loss 1.07641196 epoch total loss 1.18398786\n",
      "Trained batch 1211 batch loss 1.12252235 epoch total loss 1.18393719\n",
      "Trained batch 1212 batch loss 1.07841599 epoch total loss 1.18385\n",
      "Trained batch 1213 batch loss 1.15575945 epoch total loss 1.18382692\n",
      "Trained batch 1214 batch loss 1.16818237 epoch total loss 1.18381405\n",
      "Trained batch 1215 batch loss 1.24905801 epoch total loss 1.18386769\n",
      "Trained batch 1216 batch loss 1.16889739 epoch total loss 1.18385541\n",
      "Trained batch 1217 batch loss 1.03659308 epoch total loss 1.18373454\n",
      "Trained batch 1218 batch loss 0.958515048 epoch total loss 1.18354952\n",
      "Trained batch 1219 batch loss 0.893105626 epoch total loss 1.18331122\n",
      "Trained batch 1220 batch loss 1.13743114 epoch total loss 1.18327367\n",
      "Trained batch 1221 batch loss 1.11001134 epoch total loss 1.18321359\n",
      "Trained batch 1222 batch loss 1.30697775 epoch total loss 1.18331492\n",
      "Trained batch 1223 batch loss 1.39496326 epoch total loss 1.18348801\n",
      "Trained batch 1224 batch loss 1.13137674 epoch total loss 1.18344545\n",
      "Trained batch 1225 batch loss 1.04549336 epoch total loss 1.18333292\n",
      "Trained batch 1226 batch loss 1.10682762 epoch total loss 1.18327045\n",
      "Trained batch 1227 batch loss 1.17878103 epoch total loss 1.18326688\n",
      "Trained batch 1228 batch loss 1.22147083 epoch total loss 1.18329787\n",
      "Trained batch 1229 batch loss 1.20012915 epoch total loss 1.18331158\n",
      "Trained batch 1230 batch loss 1.15733171 epoch total loss 1.18329048\n",
      "Trained batch 1231 batch loss 1.37524676 epoch total loss 1.18344641\n",
      "Trained batch 1232 batch loss 1.28592968 epoch total loss 1.1835295\n",
      "Trained batch 1233 batch loss 1.20871067 epoch total loss 1.18355\n",
      "Trained batch 1234 batch loss 1.1870116 epoch total loss 1.18355274\n",
      "Trained batch 1235 batch loss 1.08798051 epoch total loss 1.18347549\n",
      "Trained batch 1236 batch loss 1.05757499 epoch total loss 1.18337357\n",
      "Trained batch 1237 batch loss 1.15298891 epoch total loss 1.18334901\n",
      "Trained batch 1238 batch loss 1.20403326 epoch total loss 1.1833657\n",
      "Trained batch 1239 batch loss 1.11135769 epoch total loss 1.18330753\n",
      "Trained batch 1240 batch loss 1.21115232 epoch total loss 1.18333\n",
      "Trained batch 1241 batch loss 1.21820283 epoch total loss 1.18335819\n",
      "Trained batch 1242 batch loss 1.16804934 epoch total loss 1.18334591\n",
      "Trained batch 1243 batch loss 1.11619306 epoch total loss 1.18329191\n",
      "Trained batch 1244 batch loss 1.17545342 epoch total loss 1.18328547\n",
      "Trained batch 1245 batch loss 1.13661051 epoch total loss 1.18324804\n",
      "Trained batch 1246 batch loss 1.30556619 epoch total loss 1.18334615\n",
      "Trained batch 1247 batch loss 1.16735339 epoch total loss 1.1833334\n",
      "Trained batch 1248 batch loss 1.02250063 epoch total loss 1.18320441\n",
      "Trained batch 1249 batch loss 1.07905316 epoch total loss 1.18312109\n",
      "Trained batch 1250 batch loss 1.14701843 epoch total loss 1.18309224\n",
      "Trained batch 1251 batch loss 1.18870616 epoch total loss 1.18309665\n",
      "Trained batch 1252 batch loss 1.13066983 epoch total loss 1.1830548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1253 batch loss 1.28007472 epoch total loss 1.18313217\n",
      "Trained batch 1254 batch loss 1.15475357 epoch total loss 1.18310952\n",
      "Trained batch 1255 batch loss 1.00033557 epoch total loss 1.18296397\n",
      "Trained batch 1256 batch loss 1.15290093 epoch total loss 1.18294\n",
      "Trained batch 1257 batch loss 1.05689919 epoch total loss 1.18283975\n",
      "Trained batch 1258 batch loss 1.01752925 epoch total loss 1.18270838\n",
      "Trained batch 1259 batch loss 1.19278216 epoch total loss 1.18271637\n",
      "Trained batch 1260 batch loss 1.25067139 epoch total loss 1.18277025\n",
      "Trained batch 1261 batch loss 1.02271104 epoch total loss 1.18264329\n",
      "Trained batch 1262 batch loss 1.121562 epoch total loss 1.1825949\n",
      "Trained batch 1263 batch loss 1.09024727 epoch total loss 1.18252182\n",
      "Trained batch 1264 batch loss 0.995949149 epoch total loss 1.18237424\n",
      "Trained batch 1265 batch loss 1.05056548 epoch total loss 1.18226993\n",
      "Trained batch 1266 batch loss 1.11270106 epoch total loss 1.18221498\n",
      "Trained batch 1267 batch loss 1.08852172 epoch total loss 1.18214107\n",
      "Trained batch 1268 batch loss 1.1115284 epoch total loss 1.18208539\n",
      "Trained batch 1269 batch loss 1.14071763 epoch total loss 1.18205285\n",
      "Trained batch 1270 batch loss 1.0374738 epoch total loss 1.18193901\n",
      "Trained batch 1271 batch loss 1.16331053 epoch total loss 1.18192434\n",
      "Trained batch 1272 batch loss 1.29197884 epoch total loss 1.18201089\n",
      "Trained batch 1273 batch loss 1.18036509 epoch total loss 1.18200958\n",
      "Trained batch 1274 batch loss 1.25019717 epoch total loss 1.18206322\n",
      "Trained batch 1275 batch loss 1.27483261 epoch total loss 1.18213594\n",
      "Trained batch 1276 batch loss 1.25083387 epoch total loss 1.1821897\n",
      "Trained batch 1277 batch loss 1.08981442 epoch total loss 1.18211746\n",
      "Trained batch 1278 batch loss 1.12645793 epoch total loss 1.18207383\n",
      "Trained batch 1279 batch loss 1.10095263 epoch total loss 1.18201041\n",
      "Trained batch 1280 batch loss 1.112872 epoch total loss 1.18195653\n",
      "Trained batch 1281 batch loss 1.06844139 epoch total loss 1.18186796\n",
      "Trained batch 1282 batch loss 1.24564028 epoch total loss 1.18191767\n",
      "Trained batch 1283 batch loss 1.10819066 epoch total loss 1.18186009\n",
      "Trained batch 1284 batch loss 1.00626171 epoch total loss 1.18172336\n",
      "Trained batch 1285 batch loss 1.11565471 epoch total loss 1.18167186\n",
      "Trained batch 1286 batch loss 1.1151408 epoch total loss 1.18162012\n",
      "Trained batch 1287 batch loss 1.19666195 epoch total loss 1.1816318\n",
      "Trained batch 1288 batch loss 1.09403729 epoch total loss 1.18156374\n",
      "Trained batch 1289 batch loss 1.14700401 epoch total loss 1.18153691\n",
      "Trained batch 1290 batch loss 1.16900682 epoch total loss 1.18152726\n",
      "Trained batch 1291 batch loss 1.13683414 epoch total loss 1.18149269\n",
      "Trained batch 1292 batch loss 1.13742197 epoch total loss 1.18145859\n",
      "Trained batch 1293 batch loss 1.1594466 epoch total loss 1.18144155\n",
      "Trained batch 1294 batch loss 1.10889792 epoch total loss 1.1813854\n",
      "Trained batch 1295 batch loss 1.22104073 epoch total loss 1.18141603\n",
      "Trained batch 1296 batch loss 1.22416401 epoch total loss 1.18144906\n",
      "Trained batch 1297 batch loss 1.13451767 epoch total loss 1.18141282\n",
      "Trained batch 1298 batch loss 1.18395424 epoch total loss 1.18141484\n",
      "Trained batch 1299 batch loss 1.09771872 epoch total loss 1.18135047\n",
      "Trained batch 1300 batch loss 1.15679955 epoch total loss 1.18133163\n",
      "Trained batch 1301 batch loss 1.17958426 epoch total loss 1.1813302\n",
      "Trained batch 1302 batch loss 1.29954219 epoch total loss 1.18142104\n",
      "Trained batch 1303 batch loss 1.19761562 epoch total loss 1.18143344\n",
      "Trained batch 1304 batch loss 1.11875093 epoch total loss 1.1813854\n",
      "Trained batch 1305 batch loss 1.2026912 epoch total loss 1.18140173\n",
      "Trained batch 1306 batch loss 1.18776751 epoch total loss 1.18140662\n",
      "Trained batch 1307 batch loss 0.989677787 epoch total loss 1.18125987\n",
      "Trained batch 1308 batch loss 1.12757874 epoch total loss 1.18121874\n",
      "Trained batch 1309 batch loss 1.25276637 epoch total loss 1.18127346\n",
      "Trained batch 1310 batch loss 1.11418283 epoch total loss 1.1812222\n",
      "Trained batch 1311 batch loss 1.07683063 epoch total loss 1.18114257\n",
      "Trained batch 1312 batch loss 1.20881104 epoch total loss 1.18116367\n",
      "Trained batch 1313 batch loss 1.29320347 epoch total loss 1.18124902\n",
      "Trained batch 1314 batch loss 1.13695073 epoch total loss 1.18121529\n",
      "Trained batch 1315 batch loss 1.12157524 epoch total loss 1.18117\n",
      "Trained batch 1316 batch loss 1.21916103 epoch total loss 1.18119884\n",
      "Trained batch 1317 batch loss 1.11500263 epoch total loss 1.18114853\n",
      "Trained batch 1318 batch loss 1.23527813 epoch total loss 1.18118954\n",
      "Trained batch 1319 batch loss 1.21772647 epoch total loss 1.18121731\n",
      "Trained batch 1320 batch loss 1.04641473 epoch total loss 1.18111515\n",
      "Trained batch 1321 batch loss 1.0693289 epoch total loss 1.18103051\n",
      "Trained batch 1322 batch loss 1.29321754 epoch total loss 1.18111539\n",
      "Trained batch 1323 batch loss 1.05142426 epoch total loss 1.1810174\n",
      "Trained batch 1324 batch loss 0.943705738 epoch total loss 1.18083811\n",
      "Trained batch 1325 batch loss 0.922501922 epoch total loss 1.1806432\n",
      "Trained batch 1326 batch loss 1.12157297 epoch total loss 1.18059862\n",
      "Trained batch 1327 batch loss 1.34917903 epoch total loss 1.18072557\n",
      "Trained batch 1328 batch loss 1.41657567 epoch total loss 1.1809032\n",
      "Trained batch 1329 batch loss 1.27668476 epoch total loss 1.18097532\n",
      "Trained batch 1330 batch loss 1.207376 epoch total loss 1.18099523\n",
      "Trained batch 1331 batch loss 1.21013 epoch total loss 1.18101704\n",
      "Trained batch 1332 batch loss 1.20110059 epoch total loss 1.18103206\n",
      "Trained batch 1333 batch loss 1.15539455 epoch total loss 1.18101287\n",
      "Trained batch 1334 batch loss 1.25353539 epoch total loss 1.18106723\n",
      "Trained batch 1335 batch loss 1.22103703 epoch total loss 1.18109715\n",
      "Trained batch 1336 batch loss 1.2008518 epoch total loss 1.18111193\n",
      "Trained batch 1337 batch loss 1.27559948 epoch total loss 1.18118262\n",
      "Trained batch 1338 batch loss 1.1300087 epoch total loss 1.18114436\n",
      "Trained batch 1339 batch loss 1.11817992 epoch total loss 1.18109739\n",
      "Trained batch 1340 batch loss 1.11398685 epoch total loss 1.18104732\n",
      "Trained batch 1341 batch loss 1.05363154 epoch total loss 1.18095231\n",
      "Trained batch 1342 batch loss 1.20015359 epoch total loss 1.18096662\n",
      "Trained batch 1343 batch loss 1.1392225 epoch total loss 1.18093562\n",
      "Trained batch 1344 batch loss 1.0550704 epoch total loss 1.18084192\n",
      "Trained batch 1345 batch loss 1.04032397 epoch total loss 1.18073738\n",
      "Trained batch 1346 batch loss 1.25352359 epoch total loss 1.1807915\n",
      "Trained batch 1347 batch loss 1.09285796 epoch total loss 1.18072629\n",
      "Trained batch 1348 batch loss 1.18107271 epoch total loss 1.18072641\n",
      "Trained batch 1349 batch loss 1.24745798 epoch total loss 1.18077588\n",
      "Trained batch 1350 batch loss 1.26963687 epoch total loss 1.18084168\n",
      "Trained batch 1351 batch loss 1.14030254 epoch total loss 1.18081176\n",
      "Trained batch 1352 batch loss 1.23772454 epoch total loss 1.18085372\n",
      "Trained batch 1353 batch loss 1.13238764 epoch total loss 1.18081796\n",
      "Trained batch 1354 batch loss 1.14218843 epoch total loss 1.18078947\n",
      "Trained batch 1355 batch loss 0.978937328 epoch total loss 1.18064046\n",
      "Trained batch 1356 batch loss 1.11268926 epoch total loss 1.18059039\n",
      "Trained batch 1357 batch loss 1.23544109 epoch total loss 1.1806308\n",
      "Trained batch 1358 batch loss 1.27934313 epoch total loss 1.1807034\n",
      "Trained batch 1359 batch loss 1.39445877 epoch total loss 1.18086064\n",
      "Trained batch 1360 batch loss 1.36733294 epoch total loss 1.18099773\n",
      "Trained batch 1361 batch loss 1.40878439 epoch total loss 1.18116522\n",
      "Trained batch 1362 batch loss 1.10271835 epoch total loss 1.18110752\n",
      "Trained batch 1363 batch loss 1.03712583 epoch total loss 1.1810019\n",
      "Trained batch 1364 batch loss 0.985368669 epoch total loss 1.18085849\n",
      "Trained batch 1365 batch loss 0.95348537 epoch total loss 1.18069184\n",
      "Trained batch 1366 batch loss 0.975616455 epoch total loss 1.18054175\n",
      "Trained batch 1367 batch loss 1.10662127 epoch total loss 1.18048763\n",
      "Trained batch 1368 batch loss 1.19648075 epoch total loss 1.18049932\n",
      "Trained batch 1369 batch loss 1.39203954 epoch total loss 1.18065393\n",
      "Trained batch 1370 batch loss 1.21117818 epoch total loss 1.18067622\n",
      "Trained batch 1371 batch loss 1.22679067 epoch total loss 1.18070984\n",
      "Trained batch 1372 batch loss 1.22542071 epoch total loss 1.1807425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1373 batch loss 1.17950034 epoch total loss 1.18074155\n",
      "Trained batch 1374 batch loss 1.05816603 epoch total loss 1.18065226\n",
      "Trained batch 1375 batch loss 1.18005502 epoch total loss 1.18065178\n",
      "Trained batch 1376 batch loss 1.18072891 epoch total loss 1.1806519\n",
      "Trained batch 1377 batch loss 1.0955 epoch total loss 1.18059\n",
      "Trained batch 1378 batch loss 1.06279039 epoch total loss 1.18050456\n",
      "Trained batch 1379 batch loss 1.07183433 epoch total loss 1.18042564\n",
      "Trained batch 1380 batch loss 0.949005663 epoch total loss 1.18025792\n",
      "Trained batch 1381 batch loss 0.979965687 epoch total loss 1.18011296\n",
      "Trained batch 1382 batch loss 1.24354887 epoch total loss 1.18015885\n",
      "Trained batch 1383 batch loss 1.3919661 epoch total loss 1.18031192\n",
      "Trained batch 1384 batch loss 1.33646095 epoch total loss 1.18042481\n",
      "Trained batch 1385 batch loss 1.35599506 epoch total loss 1.18055153\n",
      "Trained batch 1386 batch loss 1.38313341 epoch total loss 1.18069768\n",
      "Trained batch 1387 batch loss 1.34417319 epoch total loss 1.18081558\n",
      "Trained batch 1388 batch loss 1.30755925 epoch total loss 1.18090689\n",
      "Epoch 5 train loss 1.180906891822815\n",
      "Validated batch 1 batch loss 1.21128047\n",
      "Validated batch 2 batch loss 1.18474746\n",
      "Validated batch 3 batch loss 1.1604178\n",
      "Validated batch 4 batch loss 1.18257093\n",
      "Validated batch 5 batch loss 1.19655478\n",
      "Validated batch 6 batch loss 1.28297734\n",
      "Validated batch 7 batch loss 1.32270312\n",
      "Validated batch 8 batch loss 1.27140856\n",
      "Validated batch 9 batch loss 1.23750436\n",
      "Validated batch 10 batch loss 1.17381287\n",
      "Validated batch 11 batch loss 1.26535296\n",
      "Validated batch 12 batch loss 1.18713284\n",
      "Validated batch 13 batch loss 1.22310519\n",
      "Validated batch 14 batch loss 1.34995246\n",
      "Validated batch 15 batch loss 1.27246022\n",
      "Validated batch 16 batch loss 1.18438077\n",
      "Validated batch 17 batch loss 1.35975778\n",
      "Validated batch 18 batch loss 1.10063195\n",
      "Validated batch 19 batch loss 1.30677307\n",
      "Validated batch 20 batch loss 1.00754178\n",
      "Validated batch 21 batch loss 1.21509528\n",
      "Validated batch 22 batch loss 1.2924962\n",
      "Validated batch 23 batch loss 1.1173619\n",
      "Validated batch 24 batch loss 1.19710934\n",
      "Validated batch 25 batch loss 1.13784468\n",
      "Validated batch 26 batch loss 1.18212461\n",
      "Validated batch 27 batch loss 1.14773607\n",
      "Validated batch 28 batch loss 1.22292137\n",
      "Validated batch 29 batch loss 1.23815954\n",
      "Validated batch 30 batch loss 1.25636303\n",
      "Validated batch 31 batch loss 1.12023556\n",
      "Validated batch 32 batch loss 1.21538007\n",
      "Validated batch 33 batch loss 1.21645069\n",
      "Validated batch 34 batch loss 1.20893419\n",
      "Validated batch 35 batch loss 1.21174681\n",
      "Validated batch 36 batch loss 1.1491307\n",
      "Validated batch 37 batch loss 1.17959583\n",
      "Validated batch 38 batch loss 1.20382512\n",
      "Validated batch 39 batch loss 1.15094435\n",
      "Validated batch 40 batch loss 1.32983267\n",
      "Validated batch 41 batch loss 1.30926073\n",
      "Validated batch 42 batch loss 1.07620907\n",
      "Validated batch 43 batch loss 1.32238841\n",
      "Validated batch 44 batch loss 1.20271254\n",
      "Validated batch 45 batch loss 1.08999968\n",
      "Validated batch 46 batch loss 1.22438407\n",
      "Validated batch 47 batch loss 1.27629185\n",
      "Validated batch 48 batch loss 1.22882867\n",
      "Validated batch 49 batch loss 1.15806067\n",
      "Validated batch 50 batch loss 1.19279504\n",
      "Validated batch 51 batch loss 1.27135289\n",
      "Validated batch 52 batch loss 1.3930583\n",
      "Validated batch 53 batch loss 1.17492855\n",
      "Validated batch 54 batch loss 1.25892591\n",
      "Validated batch 55 batch loss 1.15666556\n",
      "Validated batch 56 batch loss 1.25579977\n",
      "Validated batch 57 batch loss 1.22522616\n",
      "Validated batch 58 batch loss 1.11388278\n",
      "Validated batch 59 batch loss 1.36973214\n",
      "Validated batch 60 batch loss 1.18037128\n",
      "Validated batch 61 batch loss 1.34786606\n",
      "Validated batch 62 batch loss 1.19714642\n",
      "Validated batch 63 batch loss 1.31301808\n",
      "Validated batch 64 batch loss 1.12212515\n",
      "Validated batch 65 batch loss 1.24449122\n",
      "Validated batch 66 batch loss 1.12452579\n",
      "Validated batch 67 batch loss 1.19632745\n",
      "Validated batch 68 batch loss 1.27087569\n",
      "Validated batch 69 batch loss 1.2111361\n",
      "Validated batch 70 batch loss 1.36456358\n",
      "Validated batch 71 batch loss 1.24171925\n",
      "Validated batch 72 batch loss 1.1420598\n",
      "Validated batch 73 batch loss 1.18096459\n",
      "Validated batch 74 batch loss 1.08685589\n",
      "Validated batch 75 batch loss 1.19990146\n",
      "Validated batch 76 batch loss 1.20466387\n",
      "Validated batch 77 batch loss 1.11297417\n",
      "Validated batch 78 batch loss 1.19506502\n",
      "Validated batch 79 batch loss 1.20788336\n",
      "Validated batch 80 batch loss 1.25950289\n",
      "Validated batch 81 batch loss 1.28361773\n",
      "Validated batch 82 batch loss 1.19054818\n",
      "Validated batch 83 batch loss 1.11757648\n",
      "Validated batch 84 batch loss 1.1412226\n",
      "Validated batch 85 batch loss 1.2311604\n",
      "Validated batch 86 batch loss 1.18346572\n",
      "Validated batch 87 batch loss 1.2142942\n",
      "Validated batch 88 batch loss 1.18282187\n",
      "Validated batch 89 batch loss 1.35454881\n",
      "Validated batch 90 batch loss 1.25639164\n",
      "Validated batch 91 batch loss 1.20742106\n",
      "Validated batch 92 batch loss 1.09888542\n",
      "Validated batch 93 batch loss 1.11218524\n",
      "Validated batch 94 batch loss 1.11970365\n",
      "Validated batch 95 batch loss 1.15997624\n",
      "Validated batch 96 batch loss 1.11462533\n",
      "Validated batch 97 batch loss 1.17258549\n",
      "Validated batch 98 batch loss 1.25180387\n",
      "Validated batch 99 batch loss 1.19149911\n",
      "Validated batch 100 batch loss 1.27965093\n",
      "Validated batch 101 batch loss 1.30184829\n",
      "Validated batch 102 batch loss 1.21483803\n",
      "Validated batch 103 batch loss 1.25366664\n",
      "Validated batch 104 batch loss 1.37965059\n",
      "Validated batch 105 batch loss 1.27309871\n",
      "Validated batch 106 batch loss 1.30416489\n",
      "Validated batch 107 batch loss 1.26309562\n",
      "Validated batch 108 batch loss 1.32045186\n",
      "Validated batch 109 batch loss 1.3033061\n",
      "Validated batch 110 batch loss 1.15250611\n",
      "Validated batch 111 batch loss 1.24077582\n",
      "Validated batch 112 batch loss 1.29592049\n",
      "Validated batch 113 batch loss 1.27598417\n",
      "Validated batch 114 batch loss 1.13799751\n",
      "Validated batch 115 batch loss 1.21250224\n",
      "Validated batch 116 batch loss 1.30050933\n",
      "Validated batch 117 batch loss 1.20235324\n",
      "Validated batch 118 batch loss 1.12830913\n",
      "Validated batch 119 batch loss 1.1729064\n",
      "Validated batch 120 batch loss 1.20693874\n",
      "Validated batch 121 batch loss 1.22312045\n",
      "Validated batch 122 batch loss 1.2504499\n",
      "Validated batch 123 batch loss 1.16181064\n",
      "Validated batch 124 batch loss 1.12102294\n",
      "Validated batch 125 batch loss 1.3271997\n",
      "Validated batch 126 batch loss 1.1537708\n",
      "Validated batch 127 batch loss 1.1199739\n",
      "Validated batch 128 batch loss 1.2054677\n",
      "Validated batch 129 batch loss 1.31531262\n",
      "Validated batch 130 batch loss 1.31391394\n",
      "Validated batch 131 batch loss 1.40621424\n",
      "Validated batch 132 batch loss 1.19427705\n",
      "Validated batch 133 batch loss 1.36303759\n",
      "Validated batch 134 batch loss 1.1945951\n",
      "Validated batch 135 batch loss 1.27662885\n",
      "Validated batch 136 batch loss 1.30265677\n",
      "Validated batch 137 batch loss 0.976856709\n",
      "Validated batch 138 batch loss 1.13588202\n",
      "Validated batch 139 batch loss 1.17274296\n",
      "Validated batch 140 batch loss 1.1385839\n",
      "Validated batch 141 batch loss 1.25416803\n",
      "Validated batch 142 batch loss 1.15262008\n",
      "Validated batch 143 batch loss 1.24042737\n",
      "Validated batch 144 batch loss 1.3732326\n",
      "Validated batch 145 batch loss 1.0441612\n",
      "Validated batch 146 batch loss 1.17151737\n",
      "Validated batch 147 batch loss 1.17226613\n",
      "Validated batch 148 batch loss 1.22960198\n",
      "Validated batch 149 batch loss 1.26508152\n",
      "Validated batch 150 batch loss 1.14078069\n",
      "Validated batch 151 batch loss 0.909574866\n",
      "Validated batch 152 batch loss 1.19698334\n",
      "Validated batch 153 batch loss 1.09632409\n",
      "Validated batch 154 batch loss 1.14692283\n",
      "Validated batch 155 batch loss 1.21112442\n",
      "Validated batch 156 batch loss 1.0446744\n",
      "Validated batch 157 batch loss 1.19564176\n",
      "Validated batch 158 batch loss 1.29880345\n",
      "Validated batch 159 batch loss 1.23057759\n",
      "Validated batch 160 batch loss 1.17907381\n",
      "Validated batch 161 batch loss 1.11462152\n",
      "Validated batch 162 batch loss 1.15610468\n",
      "Validated batch 163 batch loss 1.24228919\n",
      "Validated batch 164 batch loss 1.22379875\n",
      "Validated batch 165 batch loss 1.08377039\n",
      "Validated batch 166 batch loss 1.17595291\n",
      "Validated batch 167 batch loss 1.2616204\n",
      "Validated batch 168 batch loss 1.09312248\n",
      "Validated batch 169 batch loss 1.11537373\n",
      "Validated batch 170 batch loss 1.11393368\n",
      "Validated batch 171 batch loss 1.1845305\n",
      "Validated batch 172 batch loss 1.04493117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 173 batch loss 1.21560538\n",
      "Validated batch 174 batch loss 1.07821667\n",
      "Validated batch 175 batch loss 1.24179506\n",
      "Validated batch 176 batch loss 1.28851032\n",
      "Validated batch 177 batch loss 1.32793891\n",
      "Validated batch 178 batch loss 1.13405704\n",
      "Validated batch 179 batch loss 1.31382596\n",
      "Validated batch 180 batch loss 1.07013333\n",
      "Validated batch 181 batch loss 1.09418845\n",
      "Validated batch 182 batch loss 1.21974277\n",
      "Validated batch 183 batch loss 1.16348398\n",
      "Validated batch 184 batch loss 1.28438354\n",
      "Validated batch 185 batch loss 1.36238027\n",
      "Epoch 5 val loss 1.208081603050232\n",
      "Model /aiffel/aiffel/mpii/models/model-epoch-5-loss-1.2081.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43160bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-epoch-5-loss-1.2081.h5')\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348eee75",
   "metadata": {},
   "source": [
    "## 03. 예측 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e7fff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41836ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcffc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bbe3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45888329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b9018ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfdf6a",
   "metadata": {},
   "source": [
    "## 04. 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1ca7d",
   "metadata": {},
   "source": [
    "- 모델의 가중치를 가져오는 과정에서 오류가 많이 났었다. 학습시간이 오래걸리는데 오류가 나면 다시 복구하는데 많은 시간이 소모되었다.\n",
    "- SBL 모델은 어떻게 해야하는지 감이 안잡힌다. 참고할만한 github도 부족했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68adaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
